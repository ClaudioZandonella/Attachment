%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%%
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% ----------------------------------------------------------------------
%%
\documentclass[man, floatsintext]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}

% KabelExtra packages
\usepackage{booktabs} %
\usepackage{longtable} %
\usepackage{array} %
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float} %
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\usepackage{amsmath}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{../Biblio-attachment.bib}

%----    LaTeX Settings    ----%
% \usepackage{setspace} % linee spacing
\newcommand{\codespacing}{\linespread{1}}
\newcommand{\doublespacing}{\linespread{1.655}}

\setcounter{secnumdepth}{3} % Show section number
%----

\title{Evaluating Informative Hypotheses With Equality and Inequality Constraint: The Bayes Factor Encompassing Prior Approach}
\shorttitle{The Bayes Factor Encompassing Prior Approach}

\author{Claudio Zandonella Callegher, Tatiana Marci, Pietro De Carli, Gianmarco Altoè}
\affiliation{UNIPD}

\leftheader{loook}

\abstract{\lipsum[1]}

\keywords{keyword-1, keyword-2}

\authornote{
   \addORCIDlink{Claudio Zandonella Callegher}{0000-0000-0000-0000}

  Correspondence concerning this article should be addressed to Claudio Zandonella Callegher, Department of Developmental Psychology and Socialisation, University of Padua, Via Venezia, 8 - 35131 Padova, Italy.  E-mail: claudiozandonella@gmail.com}

<<settings, echo=FALSE, message=FALSE>>=
library(kableExtra)
library(ggplot2)

# Chunks settings
knitr::opts_chunk$set(echo = FALSE,
                      # Plot settings
                      dev = "tikz", dev.args=list(pointsize=12),fig.align='center',
                      fig.height=3, fig.width=5, fig.pos = "!ht",

                      # Code output width
                      tidy=TRUE, tidy.opts = list(width.cutoff = 80),
                      # comment = NA, prompt = TRUE

                      # Cache options
                      cache = FALSE, autodep=TRUE)

# Console output width
options(width = 80)

# Chunk theme
thm=knit_theme$get("bclear")
knitr::knit_theme$set(thm)
knitr::opts_chunk$set(background = c(.98, .98, 1))

# Option KableExtra
# options(knitr.kable.NA = '')

## ggplot settings
theme_set(theme_classic()+
            theme(text = element_text(size=12)))



devtools::load_all("../../")
source("../Utils_report.R")

#----    load drake    ----

drake_load_all()
@

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

One of the principal aspects of empirical research is the evaluation of research or theoretical hypothesis. When conducting a study, researcher usually have expectations based on hypotheses or theoretical perspectives they want to evaluate according to observed data.

For example researcher could be interested in evaluating whether the treatment group scores higher than the control group.

they want to evaluate them according to / if they are supported by the data; collecting evidence.

In psychology the most often use tool (the dominant statistical approach) to evaluate research hypotheses (if an hypothesis is supported by the data), is the Null hypothesis Significance Test (NHST). Debate in the literature about the utility of NHST (p-value) in hypothesis testing. this approach, however, present several limitation. First, data are evaluated against the catch all null-hypothesis that "nothing is going on" (Cohen, 1994; Royall, 1997, pp. 79–81). This is rarely the actual hypothesis of interest of the researcher who usually have some specific expectation about the phenomenon under investigation. Second, the NHST do not formalize the alternative hypothesis thus in the NHST is not possible to "accept" an hypothesis but only reject the Null-Hypothesi (the p-value do not quantify the evidence in favor of an hypothesis). This is inconvenient to the researcher as in case of not significant result is left in a state of indecision. Third limited type of hypothesis that can be evaluated, no include expectation in the form of parameter order constraints. [Expectation can not be formulated - you can use one-side test however, when there are more groups, more variables, or more constraints, null hypothesis testing is not the appropriate tool see Van de Schoot, Hoijtink et al. (2011) for a detailed discussion.\parencite{vandeschootIntroductionBayesianModel2011}]

[classical significance tests are only suitable for testing a null hypothesis against a single alternative, and unsuitable for testing multiple hypotheses with equality as well as order constraints (Silvapulle \& Sen, 2004). Second, traditional model comparison tools (e.g., the AIC, BIC, or CFI) are generally not suitable for evaluating models (or hypotheses) with order constraints on certain parameters (Mulder et al. 2009; Braeken, Mulder, \& Wood, 2015). \textcite{mulderSimpleBayesianTesting2019}]

[broad class of hypotheses]

In general, researchers are interested in comparing different hypothesis and understand which is the most supported by the data. Model comparison is a different approach that allows to compare different models. Models formalized according to researchers expectation or theoretical perspectives are compared against each other according to the data. In this way, it is possible to evaluate the relative plausibility of each model given the data and the set of model considered. Models can be compared using Information criteria (the most popular are the AIC and BIC) and the relative plausibility of each model can be computed. [Information criteria allow to evaluate the ability of the model to predict new data penalizing for model complexity.] Model comparison has several advantages and in particular it allows to directly compute the relative plausibility of each hypothesis. Model comparison, however, is still limited in the possibility to formalize hypothesis that include complex parameter order constraints.

An alternative approach to evaluate research hypotheses is based on the Bayes Factor. In the last 25 years, there has been an increasing interest / growth in popularity in the use of Bayes Factor and its adoption has been proposed (advocated) as the solution to the critical issues of the NHST. Although the Bayes Factor has its own limitations and  does not solve the fundamental issues of the misuse of statistical techniques \url{https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/}, it offers some clear advantages. In particular, the Bayes Factor allows to compare hypotheses obtaining a relative index of evidence, like in the model comparison approach, but it also allows to easily compare researchers' expectations. In fact, informative hypotheses can be formalized according to researchers expectation using equality and inequality constraints and than compared.

The evaluation of informative hypothesis based on the Bayes Factor has received increasing attention and several contribution can be found in the literature. \textcite{vandeschootIntroductionBayesianModel2011} presented a general introduction to informative hypotheses testing using the Bays Factor, whereas \textcite{hoijtinkInformativeHypothesesTheory2012} offered more detailed description on the development of this approach. Other studies, instead, considered the application of this method in specific models; for example: mixed effect models \parencite{katoBayesianApproachInequality2006}, evaluation of correlation coefficients \parencite{mulderBayesFactorsTesting2016} and multinomial models \parencite{heckMultinomialModelsLinear2019}. In addition, \textcite{guBayesianEvaluationInequality2014} proposed an approximate procedure to evaluate as long as  the structural parameters are unbounded.

Note, however, that all previously mentioned studies considered informative hypothesis only with inequality constraints. Equality constraints could not be defined together with inequality constraints in the same hypothesis but they had to be approximated as "about equality constraints" (i.e., a equality constraints of type $\theta_i = \theta_j$ is approximated as $|\theta_i - \theta_j| < \xi$ for a value of $\xi$ small enough {TODO more below?}).

It's only more recently that \textcite{guApproximatedAdjustedFractional2018} introduced an approximate procedure to evaluate both equality and inequality constraints using the Bayes Factor in the case of general statistical models (i.e., generalized linear mixed models and structural equation models). Subsequentley, this approach was exteended  by \textcite{mulderBayesFactorTesting2019} to the generalized multivariate probit models. Finally, \textcite{mulderSimpleBayesianTesting2019} proposed an accurate procedure (i.e., not based on an approximation) that allows to test informative hypothesis with equality and inequality constraints in linear regression models.

The development and implementation of this approach are of particular interest because, although with its limits (discussed latrer), it allows researchers to directly evaluate their expectations and hypotheses. Available literature, however, it is mainly based on articles published in specialize journals of mathematical psychology. The complexity of these articles makes it difficult for non-experts in this approach to clearly understand all the steps involved. On the other hand, articles offering a general introduction to the Bayes Factor do not provide enough details to allow readers to apply autonomously the method to their problems, but they usually relay on ad-hoc solutions implemented in some statistical software.

The aim of this paper is to offer a clear e detailed description of the entire procedure. In particular we refer to the approximated method proposed by \textcite{guApproximatedAdjustedFractional2018} as it is applicable to a wider range of conditions than the more accurate approach proposed by \textcite{mulderSimpleBayesianTesting2019}. The paper is organized as follow. First the method is introduced considering a toy example. This allow to provide a  detailed description of all steps and elements involved in the formalization of informative hypothesis and Bayes Factor computation. Subsequently an application of the method to real data is presented to offer the opportunity to discuss the typical issues encountered in real complex scenario.

[Code provided in the SM]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bays Factor for Informative Hypothesis Testing}

The evaluation of informative hypothesis with equality and/or inequality constraints involves several steps and elements. In this section, first we describe how to formulate informative hypothesis. Subsequently, we introduce the Bayes Factor considering the encompassing prior approach based on the approximated method proposed by \textcite{guApproximatedAdjustedFractional2018}.

[In order to facilitate the understanding, a toy example is considered.]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Formulation of Informative Hypothesis}\label{sec:informative-hypothesis}

Informative hypotheses can be defined according to researchers expectation, evidence form the literature or theoretical perspectives and they are formed by equality and/or inequality constraints on certain model parameters. These constraints are obtain as a linear combination of certain parameters and eventual constant values. For example, it is possible to state that two parameters are equal ($\theta_i = \theta_j$), one parameter is grater than another ($\theta_i > \theta_j$), the difference between two parameter is less than a given value ($\theta_i - \theta_j < .5$) or express other complex conditions ($2\times\theta_i - \theta_j < .1 \times \sigma$).

Thus, an informative hypothesis $H_i$ with equality and inequality constraints can be expressed in the form
\begin{equation}
H_i:\ \ R_E\theta = r_E \ ~ \ \& \ ~ \ R_I\theta > r_I,
\end{equation}
where $R_E$ is a matrix expressing the equality constraints and $r_E$ is a vector containing the constants of the equality constraints. Whereas, $R_I$ is a matrix expressing the inequality constraints and $r_E$ is a vector containing the constants of the inequality constraints. Finally, $\theta$ is a vector of the model parameters involved in the constraints.

As an example, consider a study evaluating the efficacy of a new psychological treatment that is supposed to improve a given cognitive ability. In the study, a control group receiving no treatment and another group receiving the traditional treatment were included as comparison. Moreover, imagine that the new psychological treatment was administrated in three different modalities to different groups: individually, in group, online. Researchers expect no differences between the three modalities but they assume that the new treatment to perform better than the traditional one that in turn is better than the control condition. We can express this hypothesis as
\begin{equation}
H_i:\ \ \theta_{control}< \theta_{traditional} < \theta_{individual} = \theta_{group} = \theta_{online},
\end{equation}
were $\theta_{control}$ is the average score of the control group, $\theta_{traditional}$ is the average score of the group receiving the traditional treatment, and $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$ are the average scores of the groups receiving the new treatment individually, in group, and on-line, respectively. The correspondig formulation of the hypothesis using matrix notation introduced before is
\begin{equation}
\begin{aligned}
H_i: &\\
& R_E\theta =
\begin{bmatrix}
0 & 0 & 1 & -1 & 0\\
0 & 0 & 0 & 1 & -1
\end{bmatrix}	\begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} = \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_E,\\~\\
& R_I\theta =
\begin{bmatrix}
-1 & 1 & 0 & 0 & 0\\
0 & -1 & 1 & 0 & 0
\end{bmatrix}	\begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} > \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_I.
\end{aligned}
\end{equation}
Note how each row of $R_E$ and $R_I$ matrices expresses an equality or an inequality constraint, respectively. For example, in the first row of $R_E$ we have $\theta_{individual} - \theta_{group} = 0$ (i.e., $\theta_{individual} = \theta_{group}$) and in the first row of $R_I$ we have $\theta_{traditional} - \theta_{control} > 0$ (i.e., $\theta_{traditional} > \theta_{control}$).

Now that we have understood how to define an informative hypothesis with equality and inequality constraints introducing an appropriate notation, lets see how to evaluate an informative hypothesis using the Bayes Factor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bayes Factor}

The Bayes Factor of hypothesis $H_1$ against a competing hypothesis $H_2$ is defined as the ration between the marginal likelihoods of the two hypothesis:
\begin{equation}
BF_{12} = \frac{Pr(Y|H_1)}{Pr(Y|H_2)} = \frac{\int l(Y|\theta_1, H_1) \pi(\theta_1| H_1)\,d\theta_1}{\int l(Y|\theta_2, H_2) \pi(\theta_2| H_2)\,d\theta_2},
\end{equation}
where $Y$ indicates the data, $\theta_i$ is the vector of parameters, $l(Y|\theta_i, , H_i)$ is the likelihood function, and $\pi(\theta_i| H_i)$ is the prior of the parameters under the hypothesis $H_i$. The marginal likelihood $Pr(Y|H_i)$ is a measure of the plausibility of the data under $H_i$.

Therefore, the Bayes Factor $BF_{12}$ quantifies the relative support of the data for the two competing hypotheses (and not the ratio between the probability of the two hypotheses). For values close to 1, the Bayes Factor indicates that $H_1$ and $H_2$ have similar support from the data. Larger values indicate evidence in favor of $H_1$, whereas values close to 0 indicate evidence in favor of $H_2$. The ratio between the posterior probabilities of the two hypotheses can be computed as
\begin{equation}
\frac{Pr(H_1|Y)}{Pr(H_2|Y)} = \overbrace{\frac{Pr(Y|H_1)}{Pr(Y|H_2)}}^{BF_{12}} \times \frac{Pr(H_1)}{Pr(H_2)},
\end{equation}
where $Pr(H_i)$ is th prior probability of $H_i$ and $Pr(H_i|Y)$ is the posterior probability of $H_i$.For a detailed description of the Bayes Factor considering also its interpretation and application in different context see \textcite{wagenmakersBayesianHypothesisTesting2010, mulderEditorsIntroductionSpecial2016, heckReviewApplicationsBayes2020}.

Note that to compute the marginal likelihood $Pr(Y|H_i)$ of an hypothesis $H_i$ it is necessary to integrate the product between the likelihood and the prior. These integrals, however, are usually difficult to compute [in particular in the case of hypothesis with order constraints]. In the case of hypothesis with equality and inequality constraints, however, it is possible to adopt the encompassing prior approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Encompassing Prior Approach}

The basic idea of the encompassing prior approach is to consider an informative hypothesis as a subset of the parameter space of an unconstrained model. Thus, to evaluate the plausibility of an hypothesis we can consider the proportion of parameters of the unconstrained model that satisfy the constraints. More specifically, given an informative hypothesis $H_i$ and an unconstrained model $H_u$ (or \textit{encompassing model}) that does not contains any constraints on the parameters, if the prior under $H_i$ is defined as a truncation of the proper prior under $H_u$ (\textit{encompassing prior}) according to the constraints, then the Bayes Factor between $H_i$ and $H_u$ can be written as
\begin{equation}\label{eq:BF-encompassing}
\begin{aligned}
BF_{iu} &= \frac{Pr(\text{Inequality Const|Equality Const, Data, } H_u)}{\pi(\text{Inequality Const|Equality Const, } H_u)} \times \frac{Pr(\text{Equality Const|Data, } H_u)}{\pi(\text{Equality Const}|H_u)} \\ ~\\
&= \frac{Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)}{\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)} \times \frac{Pr(R_E\theta =r_E| Y, H_u)}{\pi(R_E\theta =r_E| H_u)}.
\end{aligned}
\end{equation}

The first term is the ratio between the conditional posterior probability and conditional prior probability that the inequality constraints hold under the unconstrained model $H_u$ given the equality constraints. The second term is the ratio between marginal posterior density and the marginal prior density of the equality constraints under $H_u$, the well-known Savage–Dickey density ratio \parencite{dickeyWeightedLikelihoodRatio1971, wetzelsEncompassingPriorGeneralization2010}. In particular, the four elements can be interpreted as:
\begin{itemize}
  \item{The \textbf{conditional posterior probability} $Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)$ is a measure of the fit of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{conditional prior probability} $\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)$ is a measure of the complexity of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal posterior density} $Pr(R_E\theta =r_E| Y, H_u)$ is a measure of the fit of the equality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal prior density} $\pi(R_E\theta =r_E| H_u)$ is a measure of the complexity of the equality constraints of $H_i$ under $H_u$.}
\end{itemize}
Summarizing at the denominators we have measures of the \textbf{complexity} of the inequality and equality constraints of the informative hypothesis $H_i$; at the numerators, instead, we have measures of th \textbf{fit} of the data to the inequality and equality constraints of the informative hypothesis $H_i$. Thus, if the hypothesis $H_i$, although applying constraints to the parametric space (less complexity), is still able to provide a good description of the data, the Bayes Factor will favor $H_i$. On the contrary, if the support of by the data is poor, the Bayes Factor will favor the unconstrained model $H_u$.

The proof of the formulation of the Bayes Factor following the encompassing prior approach and the evaluation of its consistency (i.e., the probability of selecting the correct hypothesis goes to 1 for the sample size going to infinity) is provided in \textcite{guApproximatedAdjustedFractional2018, mulderBayesFactorTesting2019}. Note that slightly different notation is used here to enhance comprehension and underline that at the numerators we have values computed from the posterior of $H_u$, whereas at the denominators we have values computed from the prior of $H_u$.

To compute the Bayes Factor following the encompassing prior approach, only the prior and the posterior of the unconstrained model are required. In order to do that, we first need to define the encompassing prior for the unconstrained model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Definition of Encompassing Prior}

Prior specification is an important element as the resulting value of the Bayes Factor is affected by the prior choice. This aspects is particularly relevant in the case of equality constraints \parencite{lindleyStatisticalParadox1957, bartlettCommentLindleyStatistical1957}, whereas inequality constraints are not affected as long as the prior is symmetric and centered to the focal point of interest (this aspect will be further discussed in the next section).

In order to avoid arbitrary specification of the prior, different methods have been proposed in the literature. For example: Jeffreys-Zellner-Siow (JZS) objective priors do not require subjective specification \parencite{jeffreysTheoryProbability1961,zellnerPosteriorOddsRatios1980, bayarriExtendingConventionalPriors2007}; partial Bayes Factor \parencite{desantisMethodsDefaultRobust1999} uses part of the data to define the prior wheres the other part is used to compute the Bayes Factor; intrinsic Bayes Factor \parencite{bergerIntrinsicBayesFactor1996} and fractional Bayes Factor \parencite{ohaganFractionalBayesFactors1995} are a variation of the partial Bayes Factor where priors are defined according to the average of all possible minimal subsets of the data or to a given small fraction of the data, respectively.

Both, \textcite{guApproximatedAdjustedFractional2018, mulderSimpleBayesianTesting2019} based their approach on the fractional Bayes Factor. Starting from a non-informative prior, a minimal fraction of the data is used to obtain a posterior that is subsequently used as proper prior, we refer to it as \textit{fractional prior} (have you ever heard Lindley's \citeyear{lindleyBayesianStatisticsReview1972} famous quote \textit{“Today’s posterior is tomorrow’s prior”}?). The remaining part of the data is used to compute the Bayes Factor. The two approaches, however, have an important difference. \textcite{guApproximatedAdjustedFractional2018} approximated the obtained fractional prior (as well as the posterior, see Section~TODO) to a (multivariate) normal distribution. In fact, due to large-sample theory \parencite{gelmanBayesianDataAnalysis2013}, parameter posterior distribution can be approximated to a (multivariate) normal distribution. On the contrary, \textcite{mulderSimpleBayesianTesting2019} provided analytic solution in the case of linear regression models obtaining accurate quantification.

All methods discussed above suggested objective procedures to avoid arbitrary prior specification. Nevertheless, using subjective (reasonable) priors according to previous information or experts' indication is still a possible approach. Keep in mind, however, that prior specification (even if obtained through an \textit{objective procedure}) affects the Bayes Factor result. Thus, it is fundamental to conduct a prior sensitivity analysis to evaluate the influence of prior specification on the results \parencite{duBayesFactorOnesample2019}.

Going back to our psychological treatment example, we could use independent normal distributions to specify the prior of each parameter of interest (i.e. $\theta_{control}$, $\theta_{traditional}$, $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$) obtaining as resulting prior a multivariate normal distribution with mean vector $\mu_{\theta}$ and covariance matrix $\Sigma_{\theta}$. In this way we can still follow the approach proposed by \textcite{guApproximatedAdjustedFractional2018}, based on normal approximation, that will simplify the computation of the Bayes Factor. For example, suppose that, according to experts indications, a reasonable prior choice for all parameters of interest is a normal distribution with mean zero and standard deviation 2:  $\mathcal{N}(0,2)$. The resulting prior is a multivariate normal distribution have mean vector $\mu_{\theta}$ and covariance matrix $\Sigma_{\theta}$:
\begin{equation}
\begin{gathered}
\pi(\bm{\theta})\sim \mathcal{N}(\mu_{\theta}, \Sigma_{\theta})\\~\\
\text{where  }\ \bm{\theta} = \begin{bmatrix}
0 \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ \ \text{and  } \ \
\Sigma_{\theta} = \begin{bmatrix}
4 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0\\
0 & 0 & 4 & 0 & 0\\
0 & 0 & 0 & 4 & 0\\
0 & 0 & 0 & 0 & 4
\end{bmatrix}.
\end{gathered}
\end{equation}

At this point, several authors underline the importance of centering the prior distribution on the constraints points of interest. Let's further discuss this issue in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Adjusting Prior Mean}\label{sec:adj-prior}

When evaluating informative hypothesis with equality and inequality constraints using the Bayes Factor, the priors have to be centered on the focal points of interests  \parencite{zellnerPosteriorOddsRatios1980, jeffreysTheoryProbability1961, mulderPriorAdjustedDefault2014}. In the case of equality constraints, centering the prior allows to consider values close to the point of interest more likely a priori than values far away. This should be in line with researchers' expectation, otherwise one could question why testing that value.

In the case of inequality constraints, instead, this adjustment is done to guarantee that no constrain is favored a priori. Consider the example represented in Figure~\ref{fig:plot-prior-adj} where the hypothesis $\theta_i > k$ is evaluated against $\theta_i < k$. Remember that, when computing the Bayes Factor, prior probability that the constraint hold is used as a measure of complexity of the hypothesis. Thus, if the prior is not centered on the focal point of interest (i.e., $k$), the less complex hypothesis will be erroneously preferred a priori over the other. Only by centering the prior on the focal point the hypotheses will be equally likely a priori.

<<plot-prior-adj, cache=TRUE, fig.cap="Example of non centered prior considering the constraints $\\theta_i > k$ vs. $\\theta_i < k$">>=
plot_prior_adj()
@

Centering the prior to the focal point, however, can be difficult in the case of complex hypothesis where constraints are defined as linear combination of several parameters. To overcome this issue, \textcite{guApproximatedAdjustedFractional2018} proposed the following transformation of the parameters of interest:
\begin{equation} \label{eq:param-transf}
\begin{aligned}
\bm{\beta} = &R\bm{\theta}-r\\~\\
\text{with  } \bm{\beta} = \begin{bmatrix}\beta_E\\\beta_I\end{bmatrix},\ R =& \begin{bmatrix}R_E\\R_I\end{bmatrix} \text{  and  } r = \begin{bmatrix}r_E\\r_I\end{bmatrix},
\end{aligned}
\end{equation}
where $\bm{\theta}$ is the vector of original parameters, $R$ is the matrix expressing equality and inequality constraints, and $r$ is the vector with the constants of the equality and inequality constraints. Doing this, the informative hypothesis under evaluation becomes:
\begin{equation}
H_i:\ \ \beta_E = 0 \ ~ \ \& \ ~ \ \beta_I > 0.
\end{equation}

This parameter transformation has the advantage of simplifying the hypothesis expression without changing the original expectations. In fact, for example, evaluating $\theta_i > \theta_j$ is equivalent to evaluate $\beta_i = \theta_i - \theta_j > 0$. Thus, given the original prior $\pi(\bm{\theta})\sim\mathcal{N}(\mu_{\theta}, \Sigma_{\theta})$, the prior of the new parameter vector $\bm{\beta}$ is given by
\begin{equation}\label{eq:normal-transf}
\pi(\bm{\beta}) \sim \mathcal{N}(\mu_{\beta}, \Sigma_{\beta}) = \mathcal{N}(R\bm{\theta}-r, R\Sigma_{\theta}R^T).
\end{equation}

Note that this operation is nothing more than applying a linear transformation to the original multivariate normal distribution.\footnote{Given a multivariate normal distribution $Y\sim\mathcal{N}(\mu, \Sigma)$, the result of a linear transformation $AY + b$ is still a multivariate normal distribution with vector mean $\mu_{t}=AY+b$ and covariance matrix $\Sigma_{t} = A\Sigma A^T$.} In order to do that, however, it is necessary for the matrix $R$ to be \textit{full-row-rank} (i.e., all rows are linearly independent). If this is not the case, the obtained matrix $\Sigma_{\beta}$ will not be a proper covariance matrix. In Section~\ref{sec:rank-hypo} we will discuss a solution to overcome this limit.

Now to center the prior to the focal points we can simply set the mean vector to zero. Thus, the adjusted prior of $\bm{\beta}$ is
\begin{equation}
\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(\bm{0}, \Sigma_{\beta}) = \mathcal{N}(\bm{0}, R\Sigma_{\theta}R^T).
\end{equation}

Considering the psychological treatment example, the obtained adjusted prior is $\mathcal{N}(\bm{0}, \Sigma_{\beta})$ where:
\begin{equation}
\bm{0} =
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ ~ \ \text{  and  }\ ~ \
\Sigma_{\beta} = R\Sigma_{\theta}R^T = \begin{bmatrix}
8 & -4 & 0 & 4 \\
-4 & 8 & 0 & 0 \\
0 & 0 & 8 & -4 \\
4 & 0 & -4 & 8
\end{bmatrix}.
\end{equation}

So far we have defined the prior for the parameter vector $\bm{\theta}$ of the encompassing model. Moreover, we have obtained the adjusted prior for the new transformed parameter vector $\bm{\beta}$ that will allow us to properly evaluate the equality and inequality constraints. At this point we need to compute the posterior of the encompassing model parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Posterior Encompassing Model}

Posterior distribution of the encompassing model parameters can be obtained thorough numerical approximation using Markov Chain Monte Carlo (MCMC) sampling algorithms, such as Metropolis–Hastings algorithm \parencite{hastingsMonteCarloSampling1970} or Gibbs sampling \parencite{gemanStochasticRelaxationGibbs1984}. Bayesian statistical inference methods are implemented in all major statistical software. In R statistical software \parencite{rcoreteamLanguageEnvironmentStatistical2021}, for example, the popular \texttt{brms} package \parencite{burknerBrmsPackageBayesian2017, burknerAdvancedBayesianMultilevel2018a}, which is based on STAN \parencite{standevelopmentteamRStanInterfaceStan2020}, allows to easily conduct Bayesian inference.

Following \textcite{guApproximatedAdjustedFractional2018} approach, once we obtained the model posterior, we can approximate it to a (multivariate) normal distribution. Thus, the resulting posterior distribution is
\begin{equation}
Pr(\bm{\theta}|Y) \sim \mathcal{N}(\hat{\bm{\theta}}, \hat{\Sigma}_{\theta})
\end{equation}
where posterior mean $\hat{\bm{\theta}}$ and posterior covariance $\hat{\Sigma}_{\theta}$ can be computed directly from the posterior draws. Next, we can obtain the posterior with respect to the vector parameters $\bm{\beta}$ applying the same transformation used for the prior distribution,
\begin{equation}
Pr(\bm{\beta}|Y) \sim \mathcal{N}(\hat{\bm{\beta}}, \hat{\Sigma}_{\beta}) = \mathcal{N}(R\hat{\bm{\theta}}-r, R\hat{\Sigma}_{\theta}R^T).
\end{equation}

At this point, we have both the prior and the posterior distributions of the parameters of interests of the encompassing model. Before to proceed, however, let's underline few important aspects.

When defining the prior, we adjusted the prior mean centering it over the constraint focal points. This change would slightly influence the posterior result as well. However, as underlined by \textcite{guApproximatedAdjustedFractional2018}, small prior changes will result in negligible changes on the posterior for large samples due to large-sample theory. For this reason in their approach, the authors do not adjust the posterior but only the prior. Therefore, when computing the posterior we can simply consider the prior defined at the beginning $\pi(\bm_{\theta})\sim \mathcal{N}(\mu_{\theta}, \Sigma_{\theta})$ without worrying about adjusting it.

In addition, note that in \textcite{guApproximatedAdjustedFractional2018} approach posterior mean $\hat{\bm{\theta}}$ and posterior covariance $\hat{\Sigma_{\theta}}$ are not computed from the posterior draws but they are obtained using the maximum likelihood estimate and the inverse of the Fisher information matrix, respectively. This has the advantage of being faster (posterior draws are not required) but it may be not possible for some complex model. [TODO add reference? change something, cioè non si rispetta molto l'inferenza bayesiana]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computing the Bayes Factor}\label{sec:compute-bf}

In the previous section we obtained the adjusted prior and the posterior of the vector of transformed parameters $\bm{\beta}$, respectively
\begin{equation}
\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(0, \Sigma_{\beta}) \ \ \text{and} \ \
Pr(\bm{\beta}|Y) \sim \mathcal{N}(\hat{\bm{\beta}}, \hat{\Sigma}_{\beta}).
\end{equation}

Given the parameter transformation from $\bm{\theta}$ to $\bm{\beta}$, we can rewrite the Formula~\ref{eq:BF-encompassing} of the Bayes Factor between $H_i$ and $H_u$ as follow,
\begin{equation}
BF_{iu} = \frac{Pr(\beta_I > 0 | \beta_E = 0, Y, H_u)}{\pi_{adj}(\beta_I > 0 | \beta_E =0, H_u)} \times \frac{Pr(\beta_E =0| Y, H_u)}{\pi_{adj}(\beta_E =0| H_u)},
\end{equation}
where $\beta_I$ and $\beta_E$ were obtained in Formula~\ref{eq:param-transf} and represent the inequality and equality constraints, respectively.

Now, thanks to (multivariate) normal distribution approximation, we can easily compute the required conditional probabilities and marginal densities required to calculate the Bayes Factor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Marginal Density}

Marginal distribution of a subset of variables of a multivariate normal distribution is obtained simply discarding the variables to marginalize out. For example, given the adjusted prior of the psychological treatment example, $\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(\bm{0}, \Sigma_{\beta})$ where
\begin{equation}
\bm{0} =
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ ~ \ \text{  and  }\ ~ \
\Sigma_{\beta} = \begin{bmatrix}
8 & -4 & 0 & 4 \\
-4 & 8 & 0 & 0 \\
0 & 0 & 8 & -4 \\
4 & 0 & -4 & 8
\end{bmatrix},
\end{equation}
the marginal distribution of the equality constraints $\bm{\beta}_E$ is
\begin{equation}
\begin{gathered}
\pi_{adj}(\bm{\beta}_E) \sim \mathcal{N}(\mu_{\beta_E}, \Sigma_{\beta_E}),\\~\\
\text{where  }\ \mu_{\beta_E} = \begin{bmatrix}
0 \\ 0
\end{bmatrix} \ \ \text{and  } \ \
\Sigma_{\beta_E} = \begin{bmatrix}
8 & -4 \\
-4 & 8
\end{bmatrix}.
\end{gathered}
\end{equation}

At this point computing the density at $\beta_E = 0$ is elementary. In R, this can be done using the  \texttt{dmvnorm()} function from the \texttt{mvtnorm} package \parencite{genzMvtnormMultivariateNormal2021}. To compute the \textbf{marginal prior density} $\pi_{adj}(\beta_E =0| H_u)$ of the psychological treatment example, we can use the following code:
\codespacing
<<marginal-density, echo = TRUE, cache=TRUE>>=
# Prior info
mu_prior <- c(0, 0, 0, 0)
Sigma_prior <- matrix(c( 8,-4, 0, 4,
                        -4, 8, 0, 0,
                         0, 0, 8,-4,
                         4, 0,-4, 8), ncol = 4, byrow = TRUE)

# Marginal prior density at beta_1 = 0 and beta_2 = 0
mvtnorm::dmvnorm(x = c(0, 0),
                 mean = mu_prior[1:2],
                 sigma = Sigma_prior[1:2, 1:2])

@
\doublespacing

Analogously, \textbf{marginal posterior density} $\pi_{adj}(\beta_E =0| Y, H_u)$ can be computed considering this time the estimated posterior mean vector $\bm{\hat{\beta}}$ and covariance matrix $\hat{\Sigma}_{\beta}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional Probability}

To compute the conditional probability of the inequality constraints given the equality constraints in a multivariate normal distribution, we can use the \texttt{pcmvnorm()} function from the \texttt{condMVNorm} R-package \parencite{varadhanCondMVNormConditionalMultivariate2020}. Considering the psychological treatment example, to calculate the \textbf{conditional prior probability} $\pi_{adj}(\beta_I > 0 | \beta_E =0, H_u)$ we can run the following code:
\codespacing
<<conditional-prob, echo = TRUE, cache=TRUE>>=
# Conditional prior probability that beta_3 > 0 and beta_4 > 0
# given beta_1 = 0 and beta_2 = 0
condMVNorm::pcmvnorm(
    lower = c(0, 0), upper = c(Inf, Inf), # inequality constraints
    mean = mu_prior, sigma = Sigma_prior,
    dependent.ind = 3:4,                  # inequality variables
    given.ind = 1:2, X.given = c(0, 0))   # equality variables and constraints

@
\doublespacing

Analogously, \textbf{conditional posterior probability} $\pi_{adj}(\beta_I > 0 | \beta_E =0, Y, H_u)$ can be computed considering this time the estimated posterior mean vector $\bm{\hat{\beta}}$ and covariance matrix $\hat{\Sigma}_{\beta}$.

At this point we have all the elements required and we can easily compute the Bayes Factor $BF_{iu}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Advanced Elements}

In the previous section we have described in details all the steps and elements required to evaluate informative hypotheses using the Bayes Factor with encompassing prior approach. Before moving to an applied example, however, we consider some more advanced aspects that are important to take into account when applying this method. In particular, we discuss parameters standardization, range constraints, hypothesis matrix rank, comparable hypotheses, and bounded parameter space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Parameters Standardization}\label{sec:param-standard}
When evaluating informative it may be necessary to standardize the parameters of interest. As described by \textcite{guApproximatedAdjustedFractional2018}, parameters have to be standardized when comparing regression coefficient of continuous variables that are measured on different scales. In these cases, in fact, evaluating if $\theta_i>\theta_j$ would be affected by the scale of the two variables. Standardizing the parameters will allow to correctly compare the two parameters. On the contrary, standardizing the parameters is not necessary if they are compared to constants (i.e., $\theta > 3$) or if they represent groups means, such as the case of categorical variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Range Constraints}
It is possible to express range constraints such as $0<\theta_i<1$ in which the parameter is constrained between two values. These constraints can be expressed as the combination of two constraints. For example, the previous hypothesis is equivalent to $\beta_1 = \theta_i>0$ and $\beta_2 = 1 - \theta_i > 0$. However, adjusting the prior mean in the case of range constraints would require to center the prior on the middle of the range space \parencite{mulderBIEMSFortran902012}. Thus, in the previous example we would set $\mu_{\beta_1} = \mu_{\beta_2} = .5$. For a detailed description of prior specification for range constraints consider Appendix A in \textcite{guApproximatedAdjustedFractional2018}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hypothesis Matrix Rank}\label{sec:rank-hypo}
In Equation~\ref{eq:normal-transf}, we presented how to obtain the distribution of the transformed parameters $\bm{\beta}$ from the distribution of the original parameters $\bm{\theta}$. This is simply a linear transformation, but, to obtain a proper covariance matrix $\Sigma_{\beta} = R\Sigma_{\theta}R^T$, the hypothesis matrix $R$ has to be \textit{full-row-rank} (i.e., all rows are linearly independent).

To overcome this limit, we can use the same solution adopted by \textcite{mulderBayesFactorsTesting2016, mulderSimpleBayesianTesting2019}. A matrix $R^*$ is defined selecting the maximum number of linearly independent rows of $R$ and it is used to compute the covariance matrix $\Sigma_{\beta} = R^*\Sigma_{\theta}R^{*T}$. This allows us to obtain the distribution of the parameters $\bm{\beta^*}$ that are linearly independent. The remaining constraints can be expressed as linear combinations of the parameters $\bm{\beta^*}$. Finally, to evaluate the probability that all constraints holds we can draw a large sample from the distribution of $\bm{\beta^*}$ and compute the proportion of draws satisfying all the constraints.

Let's see an example to clarify the procedure. Suppose we have the following hypothesis $H_i: 0 = \theta_1 < \{\theta_2, \theta_3\} < \theta_4$. Thus, the corresponding hypothesis matrix is
\begin{equation}
\begin{aligned}
R &= \begin{bmatrix}
 1 & 0 & 0 & 0\\
-1 & 1 & 0 & 0\\
-1 & 0 & 1 & 0\\
 0 &-1 & 0 & 1\\
 0 & 0 &-1 & 1\\
\end{bmatrix} \ ~ \ \text{  with  }
\end{aligned}
\qquad
\begin{aligned}
\beta_1 &= \theta_1\\
\beta_2 &= \theta_2 - \theta_1\\
\beta_3 &= \theta_3 - \theta_1\\
\beta_4 &= \theta_4 - \theta_2\\
\beta_5 &= \theta_4 - \theta_3\\
\end{aligned}
\end{equation}

Note that only the first four rows ($\beta_1,\beta_2,\beta_3,\beta_4$) are linearly independent. The fifth row, instead, can be obtained as a linear combination: $\beta_5 = \beta_4 - \beta_3 + \beta_2$. A this point, we can define $R^*$ as the first four rows of $R$ and use it to obtain the distribution of $\bm{\beta^*}$ given the original distribution of $\bm{\theta}$. To compute the probability of the inequality constraints, we need to draw a large sample from the multivariate normal distribution conditional to the equality constraints. To do that wee can use the \texttt{rcmvnorm()} function from the R-package \texttt{condMVNorm}.
\codespacing
<<example-rank, echo = TRUE, cache=TRUE>>=
# theta distribution
theta_mu <- c(0,0,0,0)
theta_sigma <- diag(4)*2

# Hypothesis matrix
R <- matrix(c(1, 0, 0, 0,
             -1, 1, 0, 0,
             -1, 0, 1, 0,
              0,-1, 0, 1,
              0, 0,-1, 1), ncol = 4, byrow = TRUE)
r <- c(0, 0, 0, 0, 0)

# beta distribution
beta_mu <- R[1:4,] %*% theta_mu - r[1:4]
beta_sigma <- R[1:4, ] %*% theta_sigma %*% t(R[1:4, ])

# Obtain draws
set.seed(2021)
obs <- condMVNorm::rcmvnorm(
  1e4, mean = beta_mu, sigma = beta_sigma,
  dependent.ind = 2:4,        # inequality variables
  given.ind = 1, X.given = 0) # equality variable and constraint

# Define linearly dependent constraints
colnames(obs) <- c("beta_2", "beta_3", "beta_4")
obs <- cbind(obs,
             "beta_5" = obs[, "beta_4"] - obs[, "beta_3"] + obs[, "beta_2"])

# Evaluate on each draw if all constraints hold
test <- rowSums(obs > 0) == 4

# Proportion
mean(test)
@
\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Comparable Hypotheses}
To compute the Bayes Factor between different informative hypotheses, we can simply consider the ratio of the Bayes Factor of each hypotheses against the encompassing model:
\begin{equation}
BF_{ij} = \frac{BF_{iu}}{BF_{ju}}
\end{equation}
In order to do that, however, it is necessary for the informative hypotheses considered to be comparable \parencite{guApproximatedAdjustedFractional2018}. That is, the intersection of the constrained region of each hypothesis must be non-empty. Thus, a set of informative hypotheses $\{H_1,\ldots, H_i\}$ is comparable if exist at least one solution to
\begin{equation}
\begin{bmatrix}
R_{1E}\\
R_{1I}
\end{bmatrix} \theta = \begin{bmatrix}
r_{1E}\\
r_{1I}
\end{bmatrix}, \ldots,\begin{bmatrix}
R_{iE}\\
R_{iI}
\end{bmatrix} \theta = \begin{bmatrix}
r_{iE}\\
r_{iI}
\end{bmatrix}
\end{equation}
This will allow to select a common solution of $\bm{\theta}$  that can be used as the adjusted prior mean of the encompassing model. Note that to solve these equations, also inequality constraints are considered as equality constraints.

For example, $H_1: \theta_1 > \theta_2$ and $H_2: 0 = \theta_1 < \theta_2$ are comparable hypotheses because exists the common solution $\theta_1 = \theta_2 = 0$. On the country, $H_3: \theta_1 < \theta_2 +3 $ and $H_4: \theta_1 = \theta_2$ are not comparable hypotheses because there is no a common solution to $\theta_1 = \theta_2 +3 $ and $\theta_1 = \theta_2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Bounded Parameter Space}

The described approach using (multivariate) normal approximation can be straightforward applied in the case in which parameters of interest are unbounded. In case of bounded parameters (i.e., variances, correlations, or probabilities), however, appropriate prior distributions are required to take into account the parameters bounded nature.

Note that, if only inequality constraints are considered, the prior distribution is only used to measure the complexity of the inequality constrained hypotheses. In this case, the prior probability that a inequality constraint holds will be insensitive to the exact distributional form as long as the distribution is symmetrical and centered on the focal point. For example, the probability that a correlation coefficient $\rho_i$ is greater than zero is the same if we consider a  uniform distribution between -1 and 1 (taking into account parameter bounded nature) or if we consider a normal distribution centered on zero (without taking into account parameter bounded nature; see Figure~\ref{fig:plot-bounded-par}).

<<plot-bounded-par, cache=TRUE, fig.cap="The probability that $\\rho_i>0$ is the same under the uniform prior and the normal prior">>=
plot_bounded_par()
@

Thus, as suggested by \textcite{guBayesianEvaluationInequality2014}, as long as the complexity of the constrained hypotheses is the same regardless of a (multivariate) normal distribution or an appropriate bonded prior distribution being used, it is still possible to follow the described approach. This is true for hypotheses that belong to an equivalent set \parencite[for an appropriate definition of equivalent set see][]{guBayesianEvaluationInequality2014, hoijtinkInformativeHypothesesTheory2012}.

In the case of equality and inequality constraints, however, normal approximation is possible, based on the same rationale as before, only to compute the conditional prior probability. Instead, to estimate the marginal prior density, alternative solutions have to be adopted using appropriate prior distribution that takes into account the parameters bounded nature. See \textcite{mulderBayesFactorTesting2019} for an example of hypotheses evaluating correlation coefficients in generalized multivariate probit model.

Note that these issues are relevant only when considering the prior distribution. In the case of the posterior distribution, instead, we can safely approximate it to a (multivariate) normal distribution thanks to the large-sample theory. Moreover, prior specification is relevant only for the parameters of interest (i.e., the parameters involved in the constraints) as nuisance parameters are integrated out in the Bayes Factor. Thus, the results will be insensitive to the actual choice of the prior for the nuisance parameters as long as it is vague enough.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypotheses Testing in the Attachement Theory}

In this section, we propose a real case study example evaluating different informative hypotheses within the attachment theory. This will allow us to face the common issues found in real research applications. First, we provide required background information regarding the attachment theory and study aims and characteristics. Subsequently, all the steps involved in the evaluation of informative hypotheses using the Bayes Factor with encompassing prior approach are described. Finally, the results are briefly discussed considering the prior sensitivity analysis.

All analyses were conducted using the R statistical software \parencite[v\Sexpr{paste(version$major,version$minor, sep = ".")};][]{rcoreteamLanguageEnvironmentStatistical2021}. All materials, data [?], and analysis code are available at TODO. The analyses report with further details is available online at TODO.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background Information}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The Attachamnt Theory}

The attachment theory originates from the pioneering work of \textcite{bowlbyAttachmentLoss1969} and \textcite{ainsworthAttachmentExplorationSeparation1970}. The main tenet of attachment theory is that the relationships with the caregivers that children develop in the early stages of their life (i.e., \textit{attachment bound}) will affect children social and emotional development \parencite{cassidyHandbookAttachmentTheory2016}. The attachment bound concerns particular aspects of a relationship that are related to emotional regulation, proximity and protection. Four main attachment styles have been recognized in the literature according to the different behavioral patterns and internal presentations:
\begin{itemize}
  \item{\textbf{Secure Attachment}} - children who are securely attached display optimal emotional regulation and they consider the caregiver as a secure base.
  \item{\textbf{Anxious Attachment}} - anxious children manifest high level of anxiety in stressful situations and their relationships with the caregivers is ambivalent displaying anger or helplessness.
  \item{\textbf{Avoidant Attachment}} - avoidant children mask distress in stressful situations displaying little emotions and their relationships with the caregivers is characterized by little involvement.
  \item{\textbf{Fearful Attachment}} - fearful children lack an adequate emotional regulation in stressful situation displaying disorganized behaviors.
\end{itemize}

The attachment theory is one of the main and most supported theory in psychology. In the literature, however, there is still an open debate on the relative role of mother and father attachment on children social-emotional development. Four different main theoretical perspectives have been identified \parencite{brethertonFathersAttachmentTheory2010}:
\begin{itemize}
  \item{\textbf{Monotropy Theory}} - only the principal attachment figure (usually the mother) has an impact on children development.
  \item{\textbf{Hierarchy Theory}} - the principal attachment figure (usually the mother) has a greater impact on children development than the subsidiary attachment figure (usually the father).
  \item{\textbf{Independence Theory}} - all attachment figures are equally important but they they affect differently the children development.
  \item{\textbf{Integration Theory}} - to understand the impact on children development is necessary to consider attachment relationships taken together.
\end{itemize}
Contrasting results have been reported by studies investigating which is the "\textit{correct}" theory. No study, however, have tried to properly evaluate the different theoretical by directly compare the different hypotheses.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Present Study}

The aim of the present study is to directly compare the four different theoretical perspectives regarding the role of father and mother attachment, using the Bayes Factor with the encompassing prior approach.

In the analysis, $n =\Sexpr{nrow(data_cluster)}$ Italian children (\Sexpr{perc_females()}\% Females) between 8 and 12 years old (\textit{middle childhood}, third to sixth school grade) are included. Attachment towards the mother and towards the father was measured separately using the The brief Experiences in Close Relationships Scale (ECR; [TODO add references]) completed by the children. Subsequently, a cluster analyses were conducted separately for thee mother and the father to identify the 4 attachment styles. The results are reported  in Table~\ref{tab:table-cluster}.

\codespacing
<<table-cluster, cache=TRUE>>=
get_table_cluster()
@
\doublespacing

Children social-emotional development was measured using the Strength \& Difficulties Questionnaire (SDQ; [TODO add references]) completed by the teachers and separate scores for externalizing and internalizing problems were obtained as sum of the questionnaire items. In the analysis, however, only externalizing problems  are considered as teachers are expected to be better at reporting externalizing problems than internalizing problems [TODO spiegare il perchè?]. The distribution of externalizing problems ($M = \Sexpr{round(mean(data_cluster$externalizing_sum), 2)}; SD =  \Sexpr{format(round(sd(data_cluster$externalizing_sum, 2)), nsmall = 2)}$) is presented in Figure~\ref{fig:plot-externalizing-dist}.
<<plot-externalizing-dist, cache=TRUE, fig.cap="Distribution of externalizing problems ($n_{subj} = 847$)">>=
plot_externalizing_dist()
@
More information about the sample, descriptive statistics, cluster analysis, and analysis of internalizing problems can be found in the Supplemental Material available online [TODO].

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluting Hypotheses with Bayes Factor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Formalization of Informative Hypotheses}

Each theoretical perspective was formalized into a different informative hypothesis taking into account its own theoretical tenets, main evidences in the literature, and authors' clinical experience in the field.

The following notation is used to formalize the hypothesis. $M$ and $F$ are used to indicate the attachment towards the mother and the father, respectively. The specific attachment style is specified in the subscript where $S$ indicates secure attachment, $Ax$ anxious attachment, $Av$ avoidant attachment, and $F$ fearful attachment. For example $F_{Av}$ represents children with avoidant attachment towards the father.

Note that when we do not expect interaction between mother and father attachment, we can consider the role of the two parents separately. Whereas, if interaction is expected, it is necessary to take into account the unique combination of mother and father attachment. Thus, for example, we use $M_SF_{Ax}$ to indicate children with secure attachment towards the mother and anxious attachment towards the father. Moreover, $*$ subscript is used to indicate any attachment style and a set of subscripts is used to indicate "\textit{one among}". For example, $M_SF_{Ax;Av}$ represents a children with secure attachment towards the mother and anxious or avoidant attachment towards the father.

To correctly interpret the following Figures, note that the order is important, whereas the actual values are only indicative.

\paragraph{Null Hypothesis} This is a reference hypothesis where mother and father attachment are expected to have no effect. The hypothesis is represented in Figure~\ref{fig:plot-null-hypothesis}:
\begin{equation}
\begin{gathered}
M_* = 0,\\
F_* = 0.
\end{gathered}
\end{equation}

<<plot-null-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\textbf{Null Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "null")
@

\paragraph{Monotropy Hypothesis} Father attachment is expected to have no effect, whereas considering mother attachment we expect the following order: secure children with the lowest level of problems, anxious and avoidant children with similar levels of problems, fearful children with the highest levels of problems. The hypothesis is represented in Figure~\ref{fig:plot-monotropy-hypothesis}:
\begin{equation}
\begin{gathered}
M_S < M_{Ax} = M_{Av} < M_F,\\
F_* = 0.
\end{gathered}
\end{equation}

<<plot-monotropy-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Monotropy Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "monotropy")
@

\paragraph{Hierarchy Hypothesis} Father attachment is expected to have influence in the same way as mother attachment but with a minor role. The hypothesis is represented in Figure~\ref{fig:plot-hierarchy-hypothesis}:
\begin{equation}
\begin{gathered}
M_S < M_{Ax} = M_{Av} < M_F,\\
F_S < F_{Ax} = F_{Av} < F_F,\\
F_{Ax}< M_{Ax};\ ~ \ F_{Av}< M_{Av};\ ~ \ F_F< M_F.
\end{gathered}
\end{equation}

<<plot-hierarchy-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Hierarchy Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "hierarchy")
@

\paragraph{Independence Hypothesis} Mother and father attachment are expected to affect children outcomes differently. In this case, we considered avoidant attachemnt towards the father as a condition of higher risk. The hypothesis is represented in Figure~\ref{fig:plot-independence-hypothesis}:
\begin{equation}
\begin{gathered}
M_S < M_{Ax} = M_{Av} < M_F,\\
F_S < F_{Ax} < F_{Av} < F_F.\\
\end{gathered}
\end{equation}

<<plot-independence-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Independence Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "independence")
@

\paragraph{Integration Hypothesis}  Mother and father attachment are expected to interact. In this case, we consider secure attachment as a protective factor and fearful attachment as a risk condition. We do not specify the conditions $M_SF_F$ and $M_FF_S$ as their frequency is very low (\Sexpr{perc_rare_condition()}\% of the sample). The hypothesis is represented in Figure~\ref{fig:plot-integration-hypothesis}:
\begin{equation}
\begin{gathered}
M_SF_S< \{M_SF_{Ax;Av} = M_{Ax;Av}F_S\} < M_{Ax;Av}F_{Ax;Av} <  \{M_FF_{Ax;Av} = M_{Ax;Av}F_F\} <  M_FF_F,\\
\text{with}\\
M_{Ax}F_{Ax} = M_{Ax}F_{Av} = M_{Av}F_{Ax} = M_{Av}F_{Av}.
\end{gathered}
\end{equation}

<<plot-integration-hypothesis,fig.width=4, fig.high = 3, cache=TRUE, fig.cap="\\textbf{Integration Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "integration")
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Definition of the Encompassing Model}

A Zero-Inflated Negative Binomial (ZIBM) mixed effect model is defined to take into account the characteristics of the dependent variable and its distribution (see Figure~\ref{fig:plot-externalizing-dist} and see Supplemental Material for more details regarding the analysis of zero inflation [TODO]
):
\begin{equation}
y_{ij} \sim ZINB(p_{ij}, \mu_{ij}, \phi),
\end{equation}
where $p_i$ is the probability of an observation $y_{ij}$ being an extra zero (i.e., a zero not coming from the Negative Binomial distribution) and $1-p_i$ indicates the probability of a given observation $y_{ij}$ being gnerated form a Negative Binomial distribution with mean $\mu_{ij}$ and variance $\sigma_{ij}^2 = \mu_{ij} + \frac{\mu_{ij}^2}{\phi}$. Moreover, we define
\begin{equation}
\begin{gathered}
p_{ij} = \text{logit}^{-1}(X_i^T\beta_p+ Z_j^Tu_p),\\
\mu_{ij} = \text{exp}(X_i^T\beta_{\mu}+ Z_j^Tu_{\mu}).
\end{gathered}
\end{equation}

That is, both $\bm{p}$ and $\bm{\mu}$ are modeled separately according to fixed and random effects.
In particular, we considered the children classroom ID as random effect in both cases to account for teachers' different ability to evaluate children problems. While, regarding fixed effects, only the role of gender for $\bm{p}$ was considered, whereas for $\bm{\mu}$  the interaction between mother and father attachment was included together with gender. In the R formula syntax,
\codespacing
<<formula-syntax, eval = FALSE, echo=TRUE>>=
# Regression on p
p ~ gender + (1|ID_class)

# Regression on mu
mu ~ gender + mother * father + (1|ID_class)
@
\doublespacing

The parameters of interest (i.e., those related to mother and father attachment interaction) are unbounded. Thus, we can simply specified a normal distribution with mean 0 and standard deviation of 3, $\mathcal{N}(0,3)$, as reasonable prior. This prior is intended to be non informative but without being excessively diffuse. The influence of prior specification is subsequently evaluated in a prior sensitivity analysis. Regarding the other nuisance parameters (i.e., intercepts, gender effects, random effects and shapes parameters) \texttt{brms} default priors were maintained (see Supplemental Material for further details [TODO]). The encompassing model was estimated using 6 independent chains with 10,000 iterations (warm-up 2,000).

Finally, note that in this case we do not need do standardize our parameters as they represents groups means differences (see Section~\ref{sec:param-standard}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hypothesis Matrices}

Before computing the hypothesis matrix for each informative hypothesis, it is important to consider the contrasts coding and the resulting parametrization of the encompassing model. For mother and father attachment, default treatment contrasts were used \parencite{schadHowCapitalizePriori2019} considering secure attachment as reference category. Therefore, model intercept represents children with secure attachment towards both parents ($M_SF_S$) and we have parameters indicating the main effects of mother and father attachment and other parameters for the interaction effect.

Now, given the informative hypotheses and the parametrization of the encompassing model, we can now obtain the respective hypotheses matrices. In order to do that, first, we need the model matrix with all the conditions of interest. Note that we hove to considered only condition relevant to the constraints (i.e., those related to mother and father attachment) ignoring other nuisance conditions (i..e, gender).

Subsequently, we can derive the required equality and inequality constraints. In particular, with hypotheses that do not expect interaction between mother and father attachment (i.e., monotropy, hierarchy, and independence hypotheses), all interaction terms are set equal to zero and main effects are obtain considered the reference level of the other parent (i.e., $M_{Ax}F_S$ is the main effect of anxious attachment towards the mother).  As an example, consider the following code:
\codespacing
<<echo = TRUE, cache=TRUE, eval = FALSE>>=
# Define relevant conditions
attachment <- c("S", "Ax", "Av", "F")
new_data <- expand.grid(
  mother = factor(attachment, levels = attachment),
  father = factor(attachment, levels = attachment)
)

# Get model matrix
mm <- model.matrix(~ mother * father, data = new_data)[,-1] # remove intercept
rownames(mm) <- paste0("M_",new_data$mother, "_F_", new_data$father)

# Get constraints main effect
# M_Ax = M_Av  --->  M_Ax_F_S - M_Av_F_S = 0
# F_F > F_Av   --->  M_S_F_F - M_S_F_Av > 0
rbind(mm["M_Ax_F_S", ] - mm["M_Av_F_S", ],
      mm["M_S_F_F", ] - mm["M_S_F_Av", ])

# Get constraints interaction
# M_F_F_Ax = M_Ax_F_F  --->  M_F_F_Ax - M_Ax_F_F = 0
# M_Ax_F_Av > M_S_F_Ax  --->  M_Ax_F_Av - M_S_F_Ax > 0
rbind(mm["M_F_F_Ax", ] - mm["M_Ax_F_F", ],
      mm["M_Ax_F_Av", ] - mm["M_S_F_Ax", ])
@
\doublespacing
In this way, we can easily specify all constraints included in each hypothesis and obtain the respective hypothesis matrix ($R$) and vector with the constraints constants ($r$). In all our hypotheses, however, no constrain include constant values. Thus, $r$ will be always a vector of zeros and it is ignored. Moreover, also the intercept is ignored because there is no constraints that includes $M_SF_S$ alone (e.g., $M_SF_S>k$) but is always compared with another groups (e.g., $M_SF_S< M_{Ax}F_S$). Thus, the inclusion of the intercept is redundant and we ignored it.

The full hypothesis matrices specification is available at [TODO].

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Computing the Bayes Factor}

So far we defined the hypotheses matrices, specified the encompassing prior, and obtained the model posterior distribution. In order to compute the Bayes Factor, we need the adjusted prior and the posterior of the transformed parameters vector $\bm{\beta}$ (i.e., the parameters that identify the constraints) for each hypothesis.

\begin{itemize}
\item{\textbf{Adjusted Prior $\bm{\beta}$.}} As reported in Section~\ref{sec:adj-prior}, adjusted prior is required to properly evaluate the constraints. Applying the Equation~\ref{eq:normal-transf}, we obtained the prior for the transformed parameter and we set the mean vector to zero.
\item{\textbf{Posterior $\bm{\beta}$.}} Applying the same transformation used for the prior we obtained the posterior distribution of the transformed parameters vector $\bm{\beta}$.
\end{itemize}

As discussed in Section~\ref{sec:rank-hypo}, Equation~\ref{eq:normal-transf} requires the hypothesis matrix $R$ to be \textit{full-row-rank} (i.e., all constraints are linearly independent). This is not the case for the hierarchy hypothesis, thus we followed the solution presented in Section~\ref{sec:rank-hypo}. Detailed information are available at [TODO].

Now, we have all the elements required to compute the Bayes Factor for each hypothesis as described in Section~\ref{sec:compute-bf}. Moreover, assuming that each hypothesis is equally likely a priori, we can calculate the posterior probability of each hypothesis as \parencite{hoijtinkInformativeHypothesesTheory2012, guBayesianEvaluationInequality2014},
\begin{equation}
\text{Posterior Probability} H_i = \frac{BF_{iu}}{\sum_iBF_{iu}}.
\end{equation}

Results are reported in Table~\ref{tab:table-bf-results}.
\codespacing
<<table-bf-results, cache=TRUE>>=
get_table_bf(bf_result = BF_weights_ext,
             path_img = "figure/")
@
\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results and Prior Sensitivity Analysis}

The results clearly indicate that the Monotropy Theory is the most supported by the data among those considered. Remember, however, that prior specification affects the Bayes Factor results. It is recommended, therefore, to evaluate the results obtained using different prior settings as well. In particular, we considered as possible priors for the parameters of interest:
\begin{itemize}
  \item{$\mathcal{N}(0,.5)$} - unreasonable tight prior
  \item{$\mathcal{N}(0,1)$} - tighter prior
  \item{$\mathcal{N}(0,3)$} - original prior
  \item{$\mathcal{N}(0,5)$} - more diffuse prior
  \item{$\mathcal{N}(0,10)$} - unreasonably diffuse prior
\end{itemize}

The results of the prior sensitivity analysis are reported in Table~\ref{tab:table-sens-analysis}.
\codespacing
<<table-sens-analysis, cache=TRUE>>=
get_table_sens_analysis(summary_sensitivity = summary_sensitivity_ext)
@
\doublespacing
Overall results are consistent as they always indicate Monotropy Theory as the most supported by the data. However, we can observe two distinct patterns. As the prior gets more diffuse, the order of magnitude of the Bayes Factor comparing each hypothesis with the encompassing model increases. Moreover, the Null Theory increase its probability with more diffuse prior, whereas Hierarchy, Independence and Integration Theory increase their probability with tighter priors.

To interpret this patterns, remember that order constraints are insensitive to the distribution specification as long as the distribution is symmetric and centered on the focal point. On the contrary, equality constraints are highly affected by prior definition. As the prior gets more diffuse, the value of the density at zero decreases and, even for posterior distributions centered far from zero, the densities ratio at zero will favor the posterior. Thus, as the prior gets more diffuse, the BF will favor more and more hypotheses with equality constraints, whereas for tighter prior the BF will strongly penalize hypotheses with equality constraints if these are not correct (see Figure~\ref{fig:plot-sensitivity-prior}).

All defined hypothesis include equality constraints. Thus, for more diffuse prior we observe that the order of magnitude of the Bayes Factor comparing each hypothesis with the encompassing model increases. Moreover, the hypothesis with an higher number of equality constraints (e.g., Null Hypothesis) will be favored over hypothesis with a smaller number of equality constraints (e.g., Hierarchy, Independence and Integration Theory).

<<plot-sensitivity-prior, cache = TRUE, fig.cap="Evaluating densities at 0 for different prior settings and a selection of parameters posteriors">>=
get_plot_sensitivity()
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Limits}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Old Material}

[Problem of nested models?]
p-value do

statistical approach have been developed to evaluate hypothesis and by far one of the most applied tool is the Null Significance hypothesis

When conducting a study, researchers usually have expectetions about the phenomeno under investigation. They want to verify some kind of hypothesis.

translation of theoretical hypothesis into statistical models
these hypothesis can be moreover less conscious, verify if an effect exist but if interrogated in mor e depth evaluate
Informative hypothesis with equality or order constraints.

"Informative hypotheses are increasingly being used in psychological sciences because they adequately capture researchers’ theories and expectations; One of the objectives of psychological studies is to test hypotheses that represent scientific expectations. The main tool available for this purpose is null hypothesis significance testing where the goal is to falsify a null hypothesis of ‘no effect’. On the other hand, psychologists may expect; These expectations cannot be formulated by the traditional null hypothesis. Instead, such expectations can be translated to so-called informative hypotheses which assume a specific structure of the model parameters (Hoijtink, 2012)" \textcite{guApproximatedAdjustedFractional2018}

Traditional null hypothesis, statistical approaches has evolved to evaluate specific hypothesis,

"Many empirical researchers seek to evaluate and test hypothe- ses by comparing theoretical predictions to observed data. The dominant statistical vehicle for this activity is null hypothesis sig- nificance testing using p values. Despite their popularity, the liter- ature contains an intense and ongoing debate about the usefulness of p values for testing scientific expectations (e.g., Berger \& Sellke, 1987; Cohen, 1994; Edwards, Lindman, \& Savage, 1963; Hubbard \& Armstrong, 2006; Wagenmakers, 2007; Wainer, 1999, among many others).
One important critique of p values is that they cannot be used to quantify evidence in favor of the null hypothesis; a p value can only be used to falsify that null hypothesis; he p value does not allow one to discriminate absence of evidence (i.e., uninformative data) from evidence of absence (i.e., data supporting the null hypothesis; p values tend to overestimate the evidence against the null hypothesis; A final critique we mention here is that p values are limited regarding the types of hypotheses that can be tested. ; p values are of limited use for testing hypotheses with order constraints on the parameters of interest (Braeken, Mulder, \& Wood, 2015);
Bayes factors are computed by assessing the relative predictive adequacy of the hypotheses under consideration, as provided by the so-called marginal likelihood ; The goal of a null hypothesis significance test (NHST) on the other hand is to determine whether there is enough evidence in the data to reject the null hypothesis, while controlling the probability of incorrectly rejecting the null; In a Bayes factor test the outcome is the relative evidence in the data for H0 against H1, which lies on a continuous; the outcome of a NHST, as advocated by Neyman and Pearson, is a dichotomous decision; BF relative measure which balances between the plausibility of the null hypothesis and the alternative hypothesis where the prior under the alternative formalizes the anticipated effect if the null is not true.
" \parencite{mulderEditorsIntroductionSpecial2016}

"A core element of science is that data are used to argue for or against hypotheses or theories. ;these statistics are designed to make decisions, such as rejecting a hypothesis, rather than providing for a measure of ev- idence. ; In this paper, we explore a statistical notion that does allow for the desired interpretation as a measure of evidence: the Bayes factor (Good, 1979, 1985; Jeffreys, 1961; Kass \& Raftery, 1995).; appealing method for assessing the impact of data on the evaluation of hypotheses. Bayes factors present a useful and meaningful measure of evidence." \parencite{moreyPhilosophyBayesFactors2016}


different approaches model comparison: formalize models according to hypotheses or theoretical perspectives

- what are informative hypothesis
  - equal; grater; smaller; Inequality equality constraints
  - comes from theoretical prespective, previous studies, review
  - "An informative hypothesis consists of equality and/or inequality constraints among the parameters of interest in a statistical model." \textcite{guApproximatedAdjustedFractional2018}
  - "Informative hypotheses covers a much broader range of scientific expectations than the class of standard null hypotheses; In addition, by testing competing informative hypotheses directly against each other a researcher obtains a direct answer as to which scientific theory is most supported by the data." \textcite{guApproximatedAdjustedFractional2018}
  - evaluation of informative hypothesis frequentist vs bayesian  \textcite{guApproximatedAdjustedFractional2018}

- methods available
  - NHST - multiple testing (post hoc analyses)
  - Bayesian model comparison - information criteria
    - "in contrast to Bayesian model selection (BF) both the AIC and BIC are as yet unable to deal with hypotheses specified using inequality constraints." \parencite{vandeschootIntroductionBayesianModel2011}
  - BF encompassing prior
    - Gu, Mulder, Dekovic, and Hoijtink (2014) have shown how to evaluate inequality constrained hypothesis in general statistical models
    - the usefulness of the Bayes factor for testing hypotheses in psychological research has been highlighted in various studies in a special issue on the topic (Mulder \& Wagenmakers, 2016)
    - \textcite{guApproximatedAdjustedFractional2018} "This paper proposes an approximation of a fractional Bayes factor to extend its applicability to testing informative hypotheses for more general models. These models can be generalized linear (mixed) models (McCullogh \& Searle, 2001); Due to large-sample theory (Gelman, Carlin, Stern, \& Rubin, 2004, pp. 101–107), the posterior distribution of the parameters in each model can be approximated by a (multivariate) normal distribution. This paper also approximates the implicit fractional prior with a (multivariate) normal distribution as a general methodology to ensure a fast computation of the (adjusted) fractional Bayesian factor. "

"Of course, these hypotheses can be evaluated using classical null hypothesis testing, or one-sided hypothesis testing. However, when there are more groups, more variables, or more constraints, null hypothesis testing is not the appropriate tool see Van de Schoot, Hoijtink et al. (2011) for a detailed discussion. Moreover, p-values are fundamentally incompa- tible with measures of evidence."\parencite{vandeschootIntroductionBayesianModel2011}

"inequality constraints can provide a direct evaluation of the hypothesis of interest, in contrast to other, heuristic methods such as testing the equality of all three dosage condition parameters and then carrying out additional, post hoc analyses to determine directional differences." \parencite{heckMultinomialModelsLinear2019}

- formalization hypotheses:
  - declare expectation strong theories
  - "(a) translating a verbal theory into predicted patterns, (b) deriving algebraic implications of axioms or formal theories, and (c) brute force enumeration of all of the predictions made by the deterministic theory, typically under a set of theory-specific assumptions" \parencite{heckMultinomialModelsLinear2019}

- definition of informative hypothesis
  - \textcite{guApproximatedAdjustedFractional2018}

- Components Bayesian model selection\parencite{vandeschootIntroductionBayesianModel2011}:
  - Prior - Admissible parameter space
  - Likelihood
  - Marginal likelihood
  - Model fit / model complexity (size) "In sum, the marginal likelihood rewards a hypothesis with the correct (in)equality constraints. This is because the average likelihood value is higher when many small likelihood values are not taken into account. The smaller the parameter space, the less complex a model becomes. Therefore, the methodology combines model fit and model size of a hypothesis."
  - BF "Recall that Bayes factors provide a direct quantification of the support in the data for the constraints imposed on the means. With support we mean: the trade-off between model size and model fit."

A key difficulty in analyzing inequality-constrained models and theories is that it can quickly become difficult to characterize the resulting restricted parameter space (e.g., Davis- Stober, 2012; Fishburn, 1992) \parencite{heckMultinomialModelsLinear2019}

- previoous work
  - \textcite{vandeschootIntroductionBayesianModel2011} - simple general introduction Bayesian model selection for evaluating informative hypotheses. Easily overcomes problem of equality with region of equality. No code or clear step by step.
  - \textcite{heckMultinomialModelsLinear2019} - Multinomial Models with Linear Inequality Constraints. Complex mathematical psychology
  - \textcite{guApproximatedAdjustedFractional2018} - adjusted fractional BF to avoid prior specification; general approach for different model and eequality and inequalitty constraints.
  - \textcite{hoijtinkInformativeHypothesesTheory2012} - general book on testing informative hypothesis
  - \textcite{hoijtinkTutorialTestingHypotheses2019} - introduction-tutorial to Bayes factor, very descriptive no formulas, the part of informative hypothesis is restricted to ANOVA using bain package.
  -\textcite{mulderEditorsIntroductionSpecial2016} - introduction to issue about BF.
  \textcite{moreyPhilosophyBayesFactors2016} - phylosophycal fundation of the BF.
  -\textcite{katoBayesianApproachInequality2006} - inequality constraints in mixed effect model
  - not god for wide audience, complex notation athough valuable makes complex to understand.



- extra
  - combination with threshoolds $\theta_1 - \theta_2 < k$
  - different approach with range constraints $|\theta_1 - \theta_2| < k$
  - "the parameters of interest may need to be standardized in some situations. The need for standardization depends on the statistical model and informative hypothesis under evaluation.;  it is undesirable to standardize the parameters h if they represent means." \textcite{guApproximatedAdjustedFractional2018}
  - issue of consistency
  - range constrain issue mean prior see Gu 2018
  - multiverse analysis


\printbibliography
\end{document}

%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% This work is "maintained" (as per LPPL maintenance status) by
%% Daniel A. Weiss.
%%
%% This work consists of the file  apa7.dtx
%% and the derived files           apa7.ins,
%%                                 apa7.cls,
%%                                 apa7.pdf,
%%                                 README,
%%                                 APA7american.txt,
%%                                 APA7british.txt,
%%                                 APA7dutch.txt,
%%                                 APA7english.txt,
%%                                 APA7german.txt,
%%                                 APA7ngerman.txt,
%%                                 APA7greek.txt,
%%                                 APA7czech.txt,
%%                                 APA7turkish.txt,
%%                                 APA7endfloat.cfg,
%%                                 Figure1.pdf,
%%                                 shortsample.tex,
%%                                 longsample.tex, and
%%                                 bibliography.bib.
%%
%%
%% End of file `./samples/longsample.tex'.

<<biber>>=
system(paste("biber", sub("\\.Rnw$", "", current_input())))
@

