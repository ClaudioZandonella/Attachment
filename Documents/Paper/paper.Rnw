%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%%
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% ----------------------------------------------------------------------
%%
\documentclass[man, floatsintext]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}

% KabelExtra packages
\usepackage{booktabs} %
\usepackage{longtable} %
\usepackage{array} %
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float} %
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\usepackage{amsmath}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{../Biblio-attachment.bib}

%----    LaTeX Settings    ----%
% \usepackage{setspace} % linee spacing
\newcommand{\codespacing}{\linespread{1}}
\newcommand{\doublespacing}{\linespread{1.655}}

\setcounter{secnumdepth}{3} % Show section number
%----

\title{Evaluating Informative Hypotheses with Equality and Inequality Constraints:\\ A Tutorial Using the Bayes Factor via the Encompassing Prior Approach}
\shorttitle{The Bayes Factor via the Encompassing Prior Approach}

\author{Claudio Zandonella Callegher$^1$, Tatiana Marci$^1$, Pietro De Carli$^2$, Gianmarco Altoè$^1$}
\affiliation{$^1$UNIPD\\$^2$UNIMI}

\leftheader{loook}

\abstract{When conducting a study, researchers usually have expectations based on hypotheses or theoretical perspectives they want to evaluate. Equality and inequality constraints on the model parameters are used to formalize researchers' expectations or theoretical perspectives into the so-called informative hypotheses. However, traditional statistical approaches, such as the Null Hypothesis Significance Testing (NHST) or the model comparison using information criteria (e.g., AIC and BIC), are unsuitable for testing complex informative hypotheses. An alternative approach is to use the Bayes factor. In particular, the Bayes factor based on the encompassing prior approach allows researchers to easily evaluate complex informative hypotheses in a wide range of statistical models (e.g., generalized linear). This paper provides a detailed introduction to the Bayes factor with encompassing prior. First, all steps and elements involved in the formalization of  informative hypotheses and the computation of the Bayes factor with encompassing prior are described. Next, we apply this method to a real case scenario, considering the attachment theory. Specifically, we analyzed the relative influence of maternal and paternal attachment on children's social-emotional development by comparing the various theoretical perspectives debated in the literature.



\vspace{2em}
\begin{center}
\textbf{Translational Abstract}
\end{center}
\vspace{.5em}

The evaluation of hypotheses is one of the main goals of empirical research. In fact, when conducting a study, researchers usually want to evaluate the plausibility of specific hypotheses according to the collected data. These hypotheses can be formalized in accordance with researchers' expectations or theoretical perspectives into the so-called informative hypotheses. However, traditional statistical approaches, such as the Null Hypothesis Significance Testing (NHST) or the model comparison using information criteria (e.g., AIC and BIC), are not suitable for evaluating the plausibility of complex informative hypotheses. Alternatively, the Bayes factor via the encompassing prior approach allows researchers to easily evaluate the plausibility of complex informative hypotheses in a wide range of statistical models. This paper provides a detailed introduction to the Bayes factor with encompassing prior. First, we describe all steps and elements involved in the formalization of  informative hypotheses and the computation of the Bayes factor. Next, we apply this method to a real case scenario, considering the attachment theory. Specifically, we analyzed the relative influence of maternal and paternal attachment on children's social-emotional development by comparing the various theoretical perspectives debated in the literature. This approach is of particular interest in applied research, as it allows researchers to formalize hypotheses with great flexibility and to directly evaluate which hypothesis is the most supported by the data.
}

\keywords{Bayes Factor, Encompassing Prior, Informative Hypothesis, Equality and Inequality Constraints, Attachment Theory}

\authornote{
\vspace{-1cm}
\begin{flushleft}\footnotesize
\addORCIDlink{Claudio Zandonella Callegher}{0000-0001-7721-6318}\\
\addORCIDlink{Tatiana Marci}{0000-0002-2813-0312}\\
\addORCIDlink{Pietro De Carli}{0000-0001-6538-2175}\\
\addORCIDlink{Gianmarco Altoè}{0000-0003-1154-9528}

Data used for the illustrative application are part of a larger project on the socioemotional development of school-age children, which was approved by the Ethics Committee of the School of Psychology at the University of Padova (protocol \#[1838-2016]). All materials and data are available at [TODO].

Correspondence concerning this article should be addressed to Claudio Zandonella Callegher, Department of Developmental Psychology and Socialisation, University of Padua, Via Venezia, 8 - 35131 Padova, Italy.  E-mail: claudiozandonella@gmail.com
\end{flushleft}
}

<<settings, echo=FALSE, message=FALSE>>=
library(kableExtra)
library(ggplot2)

# Chunks settings
knitr::opts_chunk$set(echo = FALSE,
                      # Plot settings
                      dev = "tikz", dev.args=list(pointsize=12),fig.align='center',
                      fig.height=3, fig.width=5, fig.pos = "!ht",

                      # Code output width
                      tidy=TRUE, tidy.opts = list(width.cutoff = 80),
                      # comment = NA, prompt = TRUE

                      # Cache options
                      cache = FALSE, autodep=TRUE)

# Console output width
options(width = 80)

# Chunk theme
thm=knit_theme$get("bclear")
knitr::knit_theme$set(thm)
knitr::opts_chunk$set(background = c(.98, .98, 1))

# Option KableExtra
# options(knitr.kable.NA = '')

## ggplot settings
theme_set(theme_classic()+
            theme(text = element_text(size=12)))

devtools::load_all("../../")
source("../Utils_report.R")

#----    load drake    ----

drake_load_paper()
@

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

When conducting a study, researchers usually have expectations based on hypotheses or theoretical perspectives they want to evaluate according to the observed data. In fact, the evaluation of research and theoretical hypotheses is one of the principal goals of empirical research.

The dominant statistical approach to evaluate research hypotheses in psychology is the Null Hypothesis Significance Testing (NHST). In the literature, however, the NHST's utility and validity are largely debated \parencite{wassersteinMovingWorld052019}. This approach presents several limitations. First, the NHST places a narrow focus on statistical testing rather than on the formalization of hypotheses, leading researchers to evaluate data against the catch-all null hypothesis that nothing is going on rather than testing their specific expectation (and the alternative hypothesis is rarely formalized). Second, the \textit{p}-value does not quantify the evidence in favor of one hypothesis; therefore, it is not possible to \textit{“accept”} a hypothesis but only to \textit{“reject”} it. This situation has some drawbacks because nonsignificant results leave researchers in a state of indecision. Third, the NHST does not allow researchers to test multiple hypotheses at the same time. With the NHST, the null hypothesis is tested only against a single alternative. Fourth, the NHST is unsuitable for testing broad classes of hypotheses with equality and inequality constraints \parencite{mulderSimpleBayesianTesting2019}. Researchers can evaluate expectations using one-side tests; however, when more groups or more variables are involved, it is not possible to evaluate complex parameter constraints that reflect researchers' expectations \parencite{vandeschootIntroductionBayesianModel2011}.

Model comparison is a different approach that allows researchers to compare multiple hypotheses and identify the most supported by the data \parencite{mcelreathStatisticalRethinkingBayesian2020}. Researchers first formalize hypotheses as statistical models according to their expectations or theoretical perspectives. Therefore, it is possible to determine which is the most supported model among those considered according to the data. To do so, a popular approach is to use information criteria, such as the AIC or BIC criteria, that estimate a model's ability to predict new data \parencite{wagenmakersAICModelSelection2004,akaike1973a, schwarzEstimatingDimensionModel1978}. Model comparison has several advantages; in particular, it allows researchers to directly compute each model's relative plausibility given the data and the set of models considered. However, model comparison using information criteria is not appropriate to evaluate hypotheses that include complex parameter constraints reflecting researchers' expectations. In fact, information criteria evaluate models' complexity according to the number of parameters, but they do not take into account possible order constraints on the parameters.

An alternative criterium to evaluate research hypotheses in a model comparison is the Bayes factor. During the last 25 years, interest in the Bayes factor has increased, and its use has been proposed as the solution to the NHST's critical issues \parencite{heckReviewApplicationsBayes2020, mulderEditorsIntroductionSpecial2016}. Although it has its own limitations and it does not solve the fundamental issues of the misuse of statistical techniques \parencite{schadWorkflowTechniquesRobust2021, gelmanBayesianDataAnalysis2013}, the Bayes factor offers some clear advantages. In particular, it allows us to compare hypotheses obtaining a relative index of evidence, like the information criteria, but it also allows us to easily compare complex research hypotheses. In fact, researchers can formalize so-called \textit{“informative hypotheses”}, hypotheses containing information about the ordering of the model parameters,  according to their expectations or theoretical perspectives, using equality and inequality constraints \parencite{vandeschootIntroductionBayesianModel2011}. Researchers can then compare these hypotheses against each other, using the Bayes factor.

The evaluation of informative hypotheses is of particular interest to researchers, as it allows them to assess specific complex expectations and theoretical perspectives directly. In the literature, a particular approach allowing researchers to easily compute the Bayes factor with complex informative hypotheses has received increasing attention: \textit{“the Bayes factor with encompassing prior”}. \textcite{vandeschootIntroductionBayesianModel2011} presented a general introduction to informative hypothesis testing using the Bayes factor with the encompassing prior approach whereas \textcite{hoijtinkInformativeHypothesesTheory2012} offered a more detailed description of this method's development. In other studies, however, researchers considered the application of this approach with specific statistical models, such as mixed-effect models \parencite{katoBayesianApproachInequality2006}, evaluation of correlation coefficients \parencite{mulderBayesFactorsTesting2016}, and multinomial models \parencite{heckMultinomialModelsLinear2019}. In addition, \textcite{guBayesianEvaluationInequality2014} proposed a general approximate procedure to evaluate inequality constraints in a wide range of statistical models.

However, in all the studies mentioned above, researchers considered only informative hypotheses with inequality constraints. To evaluate equality and inequality constraints in the same hypothesis, researchers had to approximate equality constraints to “\textit{about equality constraints}” (i.e., a equality constraints of type $\theta_i = \theta_j$ is approximated to $|\theta_i - \theta_j| < \xi$ for a value of $\xi$ small enough).

Only recently, \textcite{guApproximatedAdjustedFractional2018} introduced an approximate procedure to evaluate equality and inequality constraints, using the Bayes factor with the encompassing prior approach in a wide range of statistical models (i.e., generalized linear mixed models and structural equation models). Subsequently, \textcite{mulderBayesFactorTesting2019} extended this approach to generalized multivariate probit models, and, finally, \textcite{mulderSimpleBayesianTesting2019} proposed an accurate procedure (i.e., not based on an approximation) that allows for testing of informative hypotheses with equality and inequality constraints in linear regression models.

The development and implementation of this approach are of particular interest because although it has limits, it allows researchers to directly evaluate their expectations and hypotheses. The available literature, however, is rather technical. These articles' complexity makes it difficult for researchers not familiar with this approach to clearly understand all the steps involved. On the other hand, articles offering a general introduction to the Bayes factor do not provide enough details to allow readers to autonomously apply this method to their problems; instead, they usually rely on ad hoc solutions implemented in some statistical software. For example, \textcite{hoijtinkTutorialTestingHypotheses2019a} offered an excellent introduction to the Bayes factor for the evaluation of informative hypotheses using the R package \texttt{bain}. However, it is limited to the context of analysis-of-variance models, and it is not possible to extend its use to other cases.

The aim of this paper, therefore, is to offer a clear and detailed description of the Bayes factor with the encompassing prior approach to allow other researchers to apply this approach in their studies. In particular, we refer to the approximated method \textcite{guApproximatedAdjustedFractional2018} proposed, as it applies to a wider range of conditions than the more accurate approach \textcite{mulderSimpleBayesianTesting2019} introduced. In particular, researchers can use the proposed method in the case of generalized linear mixed models and structural equation models, two very general families of statistical models. Moreover, by providing an exhaustive description of all of this approach's steps, researchers do not have to rely on already available solutions; instead, they can implement this approach according to their specific needs. The paper is organized as follows. First, we introduce the method, providing a detailed description of all steps and elements involved in the formalization of informative hypotheses and Bayes factor computation. We then present an application of the method to real data to discuss the typical issues encountered in real complex scenarios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bayes Factor for Informative Hypothesis Testing}

The evaluation of informative hypotheses with equality and/or inequality constraints involves several steps and elements. In this section, first, we describe how to formulate informative hypotheses. We then introduce the Bayes factor, considering the encompassing prior approach based on the approximated method \textcite{guApproximatedAdjustedFractional2018} proposed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Formulation of Informative Hypotheses}\label{sec:informative-hypotheses}

Researchers can define informative hypotheses according to their expectation, evidence from the literature, or theoretical perspectives, and they form them with equality and/or inequality constraints on certain model parameters. These constraints are obtained as a linear combination of certain parameters and eventual constant values. For example, it is possible to state that two parameters are equal ($\theta_i = \theta_j$) or that one parameter is greater than another ($\theta_i > \theta_j$) or to express other complex conditions, such as the difference between two parameter is less than a given value ($\theta_i - \theta_j < .5$; $2\times\theta_i - \theta_j < .1 \times \sigma$).

Therefore, an informative hypothesis, $H_i$, with equality and inequality constraints can be expressed in the form
\begin{equation}
H_i:\ \ R_E\bm{\theta} = r_E \ ~ \ \& \ ~ \ R_I\bm{\theta} > r_I,
\end{equation}
where $R_E$ is a matrix expressing the equality constraints and $r_E$ is a vector containing the equality constraints' constant values. $R_I$ is a matrix expressing the inequality constraints, and $r_E$ is a vector containing the inequality constraints' constant values. Finally, $\bm{\theta}$ is a vector with the model parameters involved in the constraints.

As an example, consider a study evaluating the efficacy of a new psychological treatment that is supposed to improve a given cognitive ability. In the study, a control group receiving no treatment and another group receiving the traditional treatment were included as a comparison. Moreover, imagine that the new psychological treatment was administered in three different modalities to different groups: individually, in-group, and online. Researchers expect no differences between the three modalities, but they hypothesize that the new treatment will perform better than the traditional one, and this result, in turn, will be better than the no-treatment control condition. We can express this hypothesis as
\begin{equation}
H_i:\ \ \theta_{control}< \theta_{traditional} < \theta_{individual} = \theta_{group} = \theta_{online},
\end{equation}
were $\theta_{control}$ is the parameter estimating the average score of the no-treatment control group, $\theta_{traditional}$ is the parameter estimating the average score of the group receiving the traditional treatment, and $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$ are the parameters estimating the average scores of the groups receiving the new treatment individually, in-group, and online, respectively. The corresponding formulation of the hypothesis using the matrix notation previously introduced is
\begin{equation}
\begin{aligned}
H_i: &\\
& R_E\bm{\theta} =
\begin{bmatrix}
0 & 0 & 1 & -1 & 0\\
0 & 0 & 0 & 1 & -1
\end{bmatrix}        \begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} = \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_E,\\~\\
& R_I\bm{\theta} =
\begin{bmatrix}
-1 & 1 & 0 & 0 & 0\\
0 & -1 & 1 & 0 & 0
\end{bmatrix}        \begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} > \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_I.
\end{aligned}
\end{equation}
Note how each row of $R_E$ and $R_I$ matrices expresses an equality or an inequality constraint, respectively. For example, in the first row of $R_E$, we have $\theta_{individual} - \theta_{group} = 0$ (i.e., $\theta_{individual} = \theta_{group}$) and in the first row of $R_I$ we have $\theta_{traditional} - \theta_{control} > 0$ (i.e., $\theta_{traditional} > \theta_{control}$).

Now that we have explained how to define an informative hypothesis with equality and inequality constraints introducing an appropriate notation, let's see how to evaluate an informative hypothesis using the Bayes factor with the encompassing prior approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bayes Factor}

The Bayes factor of hypothesis $H_1$ against a competing hypothesis, $H_2$, is defined as the ratio between the marginal likelihoods of the two hypotheses:
\begin{equation}
BF_{12} = \frac{Pr(Y|H_1)}{Pr(Y|H_2)} = \frac{\int l(Y|\theta_1, H_1) \pi(\theta_1| H_1)\,d\theta_1}{\int l(Y|\theta_2, H_2) \pi(\theta_2| H_2)\,d\theta_2},
\end{equation}
where $Y$ indicates the data, $\theta_i$ is the vector of parameters under the hypothesis $H_i$, $l(Y|\theta_i, , H_i)$ is the likelihood function under the hypothesis $H_i$, and $\pi(\theta_i| H_i)$ is the prior of the parameters under the hypothesis $H_i$. The marginal likelihood $Pr(Y|H_i)$ can be interpreted as a measure of the data's plausibility under $H_i$.

Therefore, the Bayes factor, $BF_{12}$, quantifies the relative support of the data for the two competing hypotheses (and not the ratio between the probability of the two hypotheses). For values close to 1, the Bayes factor indicates that $H_1$ and $H_2$ have similar support from the data. Larger values indicate evidence in favor of $H_1$ whereas values close to 0 indicate evidence in favor of $H_2$. The ratio between these two hypotheses' posterior probabilities can be computed as
\begin{equation}
\frac{Pr(H_1|Y)}{Pr(H_2|Y)} = \overbrace{\frac{Pr(Y|H_1)}{Pr(Y|H_2)}}^{BF_{12}} \times \frac{Pr(H_1)}{Pr(H_2)},
\end{equation}
where $Pr(H_i)$ is the prior probability of $H_i$ and $Pr(H_i|Y)$ is the posterior probability of $H_i$. For a detailed description of the Bayes factor, considering also its interpretation and application in various contexts see \textcite{wagenmakersBayesianHypothesisTesting2010, mulderEditorsIntroductionSpecial2016, heckReviewApplicationsBayes2020}.

Note that to compute the marginal likelihood $Pr(Y|H_i)$ of a hypothesis, $H_i$, it is necessary to integrate the product between the likelihood and the prior. These integrals, however, are usually difficult to compute (in particular when hypotheses include order constraints). In the case of hypotheses with equality and inequality constraints, however, it is possible to simplify the computation of the Bayes factor using the encompassing prior approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Encompassing Prior Approach}

The basic idea of the encompassing prior approach is to consider an informative hypothesis as a subset of an unconstrained model's parameter space. Therefore, to evaluate a hypothesis's plausibility, we can consider the proportion of parameter space of the unconstrained model that satisfies the constraints. More specifically, given an informative hypothesis, $H_i$, and an unconstrained model, $H_u$, (or \textit{encompassing model}) that does not contain any constraints on the parameters, if the prior under $H_i$ is defined as a truncation of the proper prior under $H_u$ (\textit{encompassing prior}) according to the constraints, then the Bayes factor between $H_i$ and $H_u$ can be written as
\begin{equation}\label{eq:BF-encompassing}
\begin{aligned}
BF_{iu} &= \frac{Pr(\text{Inequality Const|Equality Const, Data, } H_u)}{\pi(\text{Inequality Const|Equality Const, } H_u)} \times \frac{Pr(\text{Equality Const|Data, } H_u)}{\pi(\text{Equality Const}|H_u)} \\ ~\\
&= \frac{Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)}{\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)} \times \frac{Pr(R_E\theta =r_E| Y, H_u)}{\pi(R_E\theta =r_E| H_u)}.
\end{aligned}
\end{equation}

The first term is the ratio between the conditional posterior probability and the conditional prior probability that the inequality constraints hold under the unconstrained model, $H_u$, given the equality constraints. The second term is the ratio between marginal posterior density and the marginal prior density of the equality constraints under $H_u$ \parencite[the well-known Savage–Dickey density ratio; ][]{dickeyWeightedLikelihoodRatio1971, wetzelsEncompassingPriorGeneralization2010}. In particular, the four elements can be interpreted as:
\begin{itemize}
  \item{The \textbf{conditional posterior probability}, $Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)$, is a measure of the fit of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{conditional prior probability}, $\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)$, is a measure of the complexity of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal posterior density}, $Pr(R_E\theta =r_E| Y, H_u)$, is a measure of the fit of the equality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal prior density}, $\pi(R_E\theta =r_E| H_u)$, is a measure of the complexity of the equality constraints of $H_i$ under $H_u$.}
\end{itemize}
Summarizing at the denominators, we have measures of the \textbf{complexity} of the inequality and equality constraints of the informative hypothesis $H_i$; at the numerators, instead, we have measures of the \textbf{fit} of the data to the inequality and equality constraints of the informative hypothesis $H_i$. Therefore, if the hypothesis $H_i$, although applying constraints to the parametric space (less complexity), can still provide an accurate description of the data, the Bayes factor will favor $H_i$. On the contrary, if the support of the data is poor, the Bayes factor will favor the unconstrained model, $H_u$.

The proof of the formulation of the Bayes factor with the encompassing prior approach and the evaluation of its consistency (i.e., the probability of selecting the correct hypothesis goes to 1 for the sample size going to infinity) is provided in \textcite{guApproximatedAdjustedFractional2018, mulderBayesFactorTesting2019}. Note that slightly different notation is used here to enhance comprehension and emphasize that at the numerators, we have values computed from the posterior of $H_u$ whereas at the denominators, we have values computed from the prior of $H_u$.

To compute the Bayes factor with the encompassing prior approach, only the prior and the posterior of the unconstrained model are required. To obtain them, first, we need to define the encompassing prior of the unconstrained model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Definition of the Encompassing Prior}

Prior specification is an important element as the resulting Bayes factor value is affected by the prior choice. This is particularly relevant in the case of equality constraints \parencite{lindleyStatisticalParadox1957, bartlettCommentLindleyStatistical1957}, whereas inequality constraints are not affected as long as the prior is symmetric and centered on the focal point of interest. We will discuss this feature further in Section~\ref{sec:adj-prior}.

To avoid arbitrary prior specification, researchers have proposed various methods. For example: Jeffreys-Zellner-Siow objective priors do not require subjective specification \parencite{jeffreysTheoryProbability1961,zellnerPosteriorOddsRatios1980, bayarriExtendingConventionalPriors2007}; partial Bayes factor \parencite{desantisMethodsDefaultRobust1999} defines the prior according to part of the data whereas the remaining part is used to compute the Bayes factor; intrinsic Bayes factor \parencite{bergerIntrinsicBayesFactor1996} and fractional Bayes factor \parencite{ohaganFractionalBayesFactors1995} are variations on the partial Bayes factor where priors are defined according to the average of all possible minimal subsets of the data or to a given small fraction of the data, respectively.

\textcite{guApproximatedAdjustedFractional2018, mulderSimpleBayesianTesting2019} based their approaches on the fractional Bayes factor. Starting from a noninformative prior, a minimal fraction of the data is used to obtain a posterior that is subsequently used as proper prior, which we refer to as \textit{fractional prior} Consider  Lindley's (\citeyear{lindleyBayesianStatisticsReview1972}) famous quote, \textit{“Today’s posterior is tomorrow’s prior”}. The remaining part of the data is used to compute the Bayes factor. These two approaches, however, have an important difference. \textcite{guApproximatedAdjustedFractional2018} approximated the obtained fractional prior (as well as the posterior, see Section~\ref{sec:posterior-encompassing}) to a (multivariate) normal distribution. In fact, according to large-sample theory \parencite{gelmanBayesianDataAnalysis2013}, parameter posterior distribution can be approximated to a (multivariate) normal distribution. On the contrary, \textcite{mulderSimpleBayesianTesting2019} provided an analytic solution in the case of linear regression models, obtaining an accurate quantification of the distribution.

All methods discussed above suggest objective procedures to avoid arbitrary prior specification. Nevertheless, using subjective (reasonable) priors according to previous information or experts' indications is still a possible approach. However, prior specification (even if obtained through an \textit{objective procedure}) affects the Bayes factor results. Therefore, it is necessary to conduct a prior sensitivity analysis to evaluate the influence of prior specification on the results \parencite{schadWorkflowTechniquesRobust2021, duBayesFactorOnesample2019}.

Going back to our psychological treatment example, we could use independent normal distributions to specify the prior of each parameter of interest (i.e. $\theta_{control}$, $\theta_{traditional}$, $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$), obtaining as resulting prior a multivariate normal distribution with mean vector $\mu_{\theta}$ and covariance matrix $\Sigma_{\theta}$. In this way, we can still follow the approach \textcite{guApproximatedAdjustedFractional2018} proposed based on normal approximation, which will simplify the computation of the Bayes factor. For example, suppose that, according to experts' indications, a reasonable prior choice for all parameters of interest is a normal distribution with a mean of 0 and standard deviation of 2:  $\mathcal{N}(0,2)$. The resulting prior is a multivariate normal distribution with mean vector $\mu_{\theta}$ and covariance matrix $\Sigma_{\theta}$:
\begin{equation}
\begin{gathered}
\pi(\bm{\theta})\sim \mathcal{N}(\mu_{\theta}, \Sigma_{\theta})\\~\\
\text{where  }\ \bm{\theta} = \begin{bmatrix}
0 \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ \ \text{and  } \ \
\Sigma_{\theta} = \begin{bmatrix}
4 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0\\
0 & 0 & 4 & 0 & 0\\
0 & 0 & 0 & 4 & 0\\
0 & 0 & 0 & 0 & 4
\end{bmatrix}.
\end{gathered}
\end{equation}

At this point, several authors emphasize the importance of centering the prior distribution on the constraints' points of interest. Let's further discuss this issue in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Adjusting Prior Mean}\label{sec:adj-prior}

When researchers evaluates informative hypotheses with equality and inequality constraints using the Bayes factor, they must center the priors on the focal points of interest  \parencite{zellnerPosteriorOddsRatios1980, jeffreysTheoryProbability1961, mulderPriorAdjustedDefault2014}. In the case of equality constraints, centering the prior allows one to consider values close to the point of interest more likely a priori than distant values. This adjustment should be in line with researchers' expectations; otherwise, one could question why he or she is testing that value.

In the case of inequality constraints, instead, this adjustment is done to guarantee that no constraint is favored a priori. Consider the example represented in Figure~\ref{fig:plot-prior-adj}, where the hypothesis $\theta_i > k$ is evaluated against $\theta_i < k$. Remember that, when computing the Bayes factor, the prior probability that the constraint holds is used as a measure of the hypothesis's complexity. Therefore, if the prior is not centered on the focal point of interest (i.e., $k$), the less complex hypothesis (i.e., $\theta_i > k$ ) will be erroneously preferred a priori over the other (i.e., $\theta_i < k$ ). The hypotheses will be equally likely a priori only when the prior is centered on the focal point.

<<plot-prior-adj, cache=TRUE, fig.cap="Example of non centered prior considering the constraints $\\theta_i > k$ vs. $\\theta_i < k$.">>=
plot_prior_adj()
@

Centering the prior to the focal point, however, can be difficult in the case of complex hypotheses in which constraints are defined as a linear combination of several parameters. To overcome this issue, \textcite{guApproximatedAdjustedFractional2018} proposed the following transformation of the parameters of interest:
\begin{equation} \label{eq:param-transf}
\begin{aligned}
\bm{\beta} = &R\bm{\theta}-r\\~\\
\text{with  } \bm{\beta} = \begin{bmatrix}\beta_E\\\beta_I\end{bmatrix},\ R =& \begin{bmatrix}R_E\\R_I\end{bmatrix} \text{  and  } r = \begin{bmatrix}r_E\\r_I\end{bmatrix},
\end{aligned}
\end{equation}
where $\bm{\theta}$ is the vector of original parameters, $R$ is the matrix expressing equality and inequality constraints, and $r$ is the vector with the constants of the equality and inequality constraints. The informative hypothesis under evaluation then becomes:
\begin{equation}
H_i:\ \ \beta_E = 0 \ ~ \ \& \ ~ \ \beta_I > 0.
\end{equation}

This parameter transformation has the advantage of simplifying the hypothesis expression without changing the original expectations. In fact, for example, evaluating $\theta_i > \theta_j$ is equivalent to evaluating $\beta_i = \theta_i - \theta_j > 0$. Therefore, given the original prior $\pi(\bm{\theta})\sim\mathcal{N}(\mu_{\theta}, \Sigma_{\theta})$, the prior of the new parameter vector $\bm{\beta}$ is given by
\begin{equation}\label{eq:normal-transf}
\pi(\bm{\beta}) \sim \mathcal{N}(\mu_{\beta}, \Sigma_{\beta}) = \mathcal{N}(R\bm{\theta}-r, R\Sigma_{\theta}R^T).
\end{equation}

Note that this operation is nothing more than applying a linear transformation to the original multivariate normal distribution.\footnote{Given a multivariate normal distribution $Y\sim\mathcal{N}(\mu, \Sigma)$, the result of a linear transformation $AY + b$ is still a multivariate normal distribution with vector mean $\mu_{t}=AY+b$ and covariance matrix $\Sigma_{t} = A\Sigma A^T$.} To do that, however, the matrix $R$ must be \textit{full-row-rank} (i.e., all rows are linearly independent). If this is not the case, the obtained matrix $\Sigma_{\beta}$ will not be a proper covariance matrix. In Section~\ref{sec:rank-hypo}, we will discuss a solution to overcome this limit.

Now, to center the prior of $\bm{\beta}$ to the focal points, we can simply set the mean vector to 0. Therefore, the adjusted prior of $\bm{\beta}$ is
\begin{equation}
\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(\bm{0}, \Sigma_{\beta}) = \mathcal{N}(\bm{0}, R\Sigma_{\theta}R^T).
\end{equation}

Considering the psychological treatment example, the obtained adjusted prior is $\mathcal{N}(\bm{0}, \Sigma_{\beta})$, where:
\begin{equation}
\bm{0} =
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ ~ \ \text{  and  }\ ~ \
\Sigma_{\beta} = R\Sigma_{\theta}R^T = \begin{bmatrix}
8 & -4 & 0 & 4 \\
-4 & 8 & 0 & 0 \\
0 & 0 & 8 & -4 \\
4 & 0 & -4 & 8
\end{bmatrix}.
\end{equation}

So far, we have defined the prior for the parameter vector $\bm{\theta}$ of the encompassing model. Moreover, we have obtained the adjusted prior for the new transformed parameter vector $\bm{\beta}$, allowing us to properly evaluate the equality and inequality constraints. At this point, we need to compute the posterior of the encompassing model parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Posterior Encompassing Model}\label{sec:posterior-encompassing}

Posterior distribution of the encompassing model parameters can be obtained through numerical approximation using Markov Chain Monte Carlo (MCMC) sampling algorithms, such as the Metropolis–Hastings algorithm \parencite{hastingsMonteCarloSampling1970} or Gibbs sampling \parencite{gemanStochasticRelaxationGibbs1984}. Bayesian statistical inference methods are implemented in all major statistical software. In R statistical software \parencite{rcoreteamLanguageEnvironmentStatistical2021}, for example, the popular \texttt{brms} package \parencite{burknerBrmsPackageBayesian2017, burknerAdvancedBayesianMultilevel2018a}, which is based on STAN \parencite{standevelopmentteamRStanInterfaceStan2020}, allows researchers to easily conduct Bayesian inference.

Following the approach established by \textcite{guApproximatedAdjustedFractional2018}, once we obtain the model posterior, we can approximate it to a (multivariate) normal distribution. Therefore, the resulting posterior distribution is
\begin{equation}
Pr(\bm{\theta}|Y) \sim \mathcal{N}(\hat{\bm{\theta}}, \hat{\Sigma}_{\theta}),
\end{equation}
where posterior mean $\hat{\bm{\theta}}$ and posterior covariance $\hat{\Sigma}_{\theta}$ can be computed directly from the posterior draws. Next, we can obtain the posterior with respect to the vector parameters $\bm{\beta}$ applying the same transformation used for the prior distribution:
\begin{equation}
Pr(\bm{\beta}|Y) \sim \mathcal{N}(\hat{\bm{\beta}}, \hat{\Sigma}_{\beta}) = \mathcal{N}(R\hat{\bm{\theta}}-r, R\hat{\Sigma}_{\theta}R^T).
\end{equation}

At this point, we have both prior and posterior distributions of the parameters of interests of the encompassing model. Before proceeding, however, let's emphasize some important aspects.

When defining the prior, we adjusted the prior mean by centering it over the constraint's focal points. This change would slightly influence the resulting posterior, as well. However, as \textcite{guApproximatedAdjustedFractional2018} pointed out, small prior changes will result in negligible changes in the posterior for large samples, according to large-sample theory. Therefore, the authors do not adjust the posterior in their approach but only the prior. Therefore, when computing the posterior, we can simply consider the prior $\pi(\bm{\theta})\sim \mathcal{N}(\mu_{\theta}, \Sigma_{\theta})$ defined at the beginning without worrying about adjusting it.

In addition, note that in the original approach \textcite{guApproximatedAdjustedFractional2018} proposed, the posterior mean $\hat{\bm{\theta}}$ and the posterior covariance $\hat{\Sigma_{\theta}}$ are not obtained from the posterior draws but are computed directly from the sample data using the maximum likelihood estimate and the inverse of the Fisher information matrix, respectively. This method has the advantage of being faster (posterior draws are not required), but it may be not possible for some complex models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computing the Bayes Factor}\label{sec:compute-bf}

In the previous section, we obtained the adjusted prior and the posterior of the vector of transformed parameters $\bm{\beta}$, respectively
\begin{equation}
\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(0, \Sigma_{\beta}) \ \ \text{and} \ \
Pr(\bm{\beta}|Y) \sim \mathcal{N}(\hat{\bm{\beta}}, \hat{\Sigma}_{\beta}).
\end{equation}

Given the parameter transformation from $\bm{\theta}$ to $\bm{\beta}$, we can rewrite Formula~\ref{eq:BF-encompassing} of the Bayes factor between $H_i$ and $H_u$ as follows:
\begin{equation}
BF_{iu} = \frac{Pr(\beta_I > 0 | \beta_E = 0, Y, H_u)}{\pi_{adj}(\beta_I > 0 | \beta_E =0, H_u)} \times \frac{Pr(\beta_E =0| Y, H_u)}{\pi_{adj}(\beta_E =0| H_u)},
\end{equation}
where $\beta_I$ and $\beta_E$ are defined in Formula~\ref{eq:param-transf} and represent the inequality and equality constraints, respectively.

Now, thanks to (multivariate) normal distribution approximation, we can easily compute the required conditional probabilities and marginal densities required to calculate the Bayes factor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Marginal Density}

The marginal distribution of a subset of variables of a multivariate normal distribution is obtained by simply discarding the variables to marginalize out. For example, given the adjusted prior of the psychological treatment example, $\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(\bm{0}, \Sigma_{\beta})$, where
\begin{equation}
\bm{0} =
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ ~ \ \text{  and  }\ ~ \
\Sigma_{\beta} = \begin{bmatrix}
8 & -4 & 0 & 4 \\
-4 & 8 & 0 & 0 \\
0 & 0 & 8 & -4 \\
4 & 0 & -4 & 8
\end{bmatrix},
\end{equation}
the marginal distribution of the equality constraints $\bm{\beta}_E$ is
\begin{equation}
\begin{gathered}
\pi_{adj}(\bm{\beta}_E) \sim \mathcal{N}(\mu_{\beta_E}, \Sigma_{\beta_E}),\\~\\
\text{where  }\ \mu_{\beta_E} = \begin{bmatrix}
0 \\ 0
\end{bmatrix} \ \ \text{and  } \ \
\Sigma_{\beta_E} = \begin{bmatrix}
8 & -4 \\
-4 & 8
\end{bmatrix}.
\end{gathered}
\end{equation}

At this point, computing the density at $\beta_E = 0$ is elementary. In R, this can be done using the  \texttt{dmvnorm()} function from the \texttt{mvtnorm} package \parencite{genzMvtnormMultivariateNormal2021}. To compute the \textbf{marginal prior density}, $\pi_{adj}(\beta_E =0| H_u)$, of the psychological treatment example, we can use the following code:
\codespacing
<<marginal-density, echo = TRUE, cache=TRUE>>=
# Prior info
mu_prior <- c(0, 0, 0, 0)
Sigma_prior <- matrix(c( 8,-4, 0, 4,
                        -4, 8, 0, 0,
                         0, 0, 8,-4,
                         4, 0,-4, 8), ncol = 4, byrow = TRUE)

# Marginal prior density at beta_1 = 0 and beta_2 = 0
mvtnorm::dmvnorm(x = c(0, 0),
                 mean = mu_prior[1:2],
                 sigma = Sigma_prior[1:2, 1:2])

@
\doublespacing

Similarly, it is possible to compute the \textbf{marginal posterior density}, $\pi_{adj}(\beta_E =0| Y, H_u)$, considering this time the estimated posterior mean vector, $\bm{\hat{\beta}}$, and the estimated  posterior covariance matrix, $\hat{\Sigma}_{\beta}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional Probability}

To compute the conditional probability of the inequality constraints given the equality constraints in a multivariate normal distribution, we can use the \texttt{pcmvnorm()} function from the \texttt{condMVNorm} R-package \parencite{varadhanCondMVNormConditionalMultivariate2020}. Considering the psychological treatment example, to calculate the \textbf{conditional prior probability}, $\pi_{adj}(\beta_I > 0 | \beta_E =0, H_u)$, we can run the following code:
\codespacing
<<conditional-prob, echo = TRUE, cache=TRUE>>=
# Conditional prior probability that beta_3 > 0 and beta_4 > 0
# given beta_1 = 0 and beta_2 = 0
condMVNorm::pcmvnorm(
    lower = c(0, 0), upper = c(Inf, Inf), # inequality constraints
    mean = mu_prior, sigma = Sigma_prior,
    dependent.ind = 3:4,                  # inequality variables
    given.ind = 1:2, X.given = c(0, 0))   # equality variables and constraints

@
\doublespacing

Analogously, it is possible to compute the \textbf{conditional posterior probability}, $\pi_{adj}(\beta_I > 0 | \beta_E =0, Y, H_u)$, considering this time the estimated posterior mean vector, $\bm{\hat{\beta}}$, and the estimated posterior covariance matrix, $\hat{\Sigma}_{\beta}$.

At this point, we have all the elements required, and we can easily compute the Bayes factor, $BF_{iu}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Advanced Elements}

In the previous sections, we have described in detail all the steps and elements required to evaluate informative hypotheses using the Bayes factor with the encompassing prior approach, following the approximated method \textcite{guApproximatedAdjustedFractional2018} proposed. As already highlighted, however, compared to the original approach \textcite{guApproximatedAdjustedFractional2018} proposed, we took a slightly different path in the prior definition and posterior approximation. Instead of using the fractional prior method, we defined subjective (reasonable) priors, and we relied on posterior draws instead of maximum likelihood estimates to approximate the posterior. We made these choices to follow the usual Bayesian workflow in data analysis \parencite{gelmanBayesianWorkflow2020, schadPrincipledBayesianWorkflow2021}.

Before moving to an applied example, however, we consider some more advanced aspects that are important to take into account when applying this method. In particular, we discuss issues related to parameters' standardization, range constraints, hypothesis matrix rank, comparable hypotheses, and bounded parameter space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Parameters Standardization}\label{sec:param-standard}

When evaluating informative hypotheses, it may be necessary to standardize the parameters of interest. As \textcite{guApproximatedAdjustedFractional2018} showed, parameters have to be standardized when comparing regression coefficients of continuous variables that are measured on different scales - in these cases, in fact, determining whether $\theta_i>\theta_j$ is affected by the scale of the two variables. Standardizing the parameters allows us to correctly compare the two parameters. On the contrary, standardizing the parameters is not necessary if they are compared to constants (i.e., $\theta > 3$) or if they represent group means, such as in the case of categorical variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Range Constraints}

It is possible to express range constraints, such as $0<\theta_i<1$, in which the parameter is constrained between two values. These constraints can be expressed as the combination of two constraints. For example, the previous hypothesis is equivalent to $\beta_1 = \theta_i>0$ and $\beta_2 = 1 - \theta_i > 0$. However, adjusting the prior mean in the case of range constraints would require us to center the prior on the middle of the range space \parencite{mulderBIEMSFortran902012}. Therefore, in the previous example, we would set $\mu_{\beta_1} = \mu_{\beta_2} = .5$. For a detailed description of prior specification in the case of range constraints, see Appendix A in \textcite{guApproximatedAdjustedFractional2018}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hypothesis Matrix Rank}\label{sec:rank-hypo}

In Equation~\ref{eq:normal-transf}, we presented how to obtain the distribution of the transformed parameters $\bm{\beta}$ from the distribution of the original parameters $\bm{\theta}$. This is simply a linear transformation, but to obtain a proper covariance matrix $\Sigma_{\beta} = R\Sigma_{\theta}R^T$, the hypothesis matrix $R$ has to be \textit{full-row-rank} (i.e., all rows linearly independent).

To overcome this limitation, we can use the same solution adopted by \textcite{mulderBayesFactorsTesting2016, mulderSimpleBayesianTesting2019}. A matrix $R^*$ is defined by selecting the maximum number of linearly independent rows of $R$ and used to compute the covariance matrix $\Sigma_{\beta} = R^*\Sigma_{\theta}R^{*T}$. This allows us to obtain the distribution of the parameters $\bm{\beta^*}$ that are linearly independent. The remaining constraints can be expressed as linear combinations of the parameters $\bm{\beta^*}$. Finally, to evaluate the probability that all constraints hold, we can draw a large sample from the distribution of $\bm{\beta^*}$ and compute the proportion of draws satisfying all the constraints.

The following is an example to clarify this procedure. Suppose the following hypothesis: $H_i: 0 = \theta_1 < \{\theta_2, \theta_3\} < \theta_4$. Thus, the corresponding hypothesis matrix is
\begin{equation}
\begin{aligned}
R &= \begin{bmatrix}
 1 & 0 & 0 & 0\\
-1 & 1 & 0 & 0\\
-1 & 0 & 1 & 0\\
 0 &-1 & 0 & 1\\
 0 & 0 &-1 & 1\\
\end{bmatrix} \ ~ \ \text{  with  }
\end{aligned}
\qquad
\begin{aligned}
\beta_1 &= \theta_1\\
\beta_2 &= \theta_2 - \theta_1\\
\beta_3 &= \theta_3 - \theta_1\\
\beta_4 &= \theta_4 - \theta_2\\
\beta_5 &= \theta_4 - \theta_3\\
\end{aligned}
\end{equation}

Note that only the first four rows ($\beta_1,\beta_2,\beta_3,\beta_4$) are linearly independent. The fifth row can be obtained as a linear combination: $\beta_5 = \beta_4 - \beta_3 + \beta_2$. At this point, we can define $R^*$ as the first four rows of $R$ and use it to obtain the distribution of $\bm{\beta^*}$ given the original distribution of $\bm{\theta}$. To compute the probability of the inequality constraints, we need to draw a large sample from the multivariate normal distribution conditional to the equality constraints. To do that, we can use the \texttt{rcmvnorm()} function from the R-package \texttt{condMVNorm}.
\codespacing
<<example-rank, echo = TRUE, cache=TRUE>>=
# theta distribution
theta_mu <- c(0,0,0,0)
theta_sigma <- diag(4)*2

# Hypothesis matrix
R <- matrix(c(1, 0, 0, 0,
             -1, 1, 0, 0,
             -1, 0, 1, 0,
              0,-1, 0, 1,
              0, 0,-1, 1), ncol = 4, byrow = TRUE)
r <- c(0, 0, 0, 0, 0)

# beta distribution
beta_mu <- R[1:4,] %*% theta_mu - r[1:4]
beta_sigma <- R[1:4, ] %*% theta_sigma %*% t(R[1:4, ])

# Obtain draws
set.seed(2021)
obs <- condMVNorm::rcmvnorm(
  1e4, mean = beta_mu, sigma = beta_sigma,
  dependent.ind = 2:4,        # inequality variables
  given.ind = 1, X.given = 0) # equality variable and constraint

# Define linearly dependent constraints
colnames(obs) <- c("beta_2", "beta_3", "beta_4")
obs <- cbind(obs,
             "beta_5" = obs[, "beta_4"] - obs[, "beta_3"] + obs[, "beta_2"])

# Evaluate on each draw if all constraints hold
test <- rowSums(obs > 0) == 4

# Proportion
mean(test)
@
\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Comparable Hypotheses}

To compute the Bayes factor between two different informative hypotheses, we can simply consider the ratio of the Bayes factor of each hypothesis against the encompassing model:
\begin{equation}
BF_{ij} = \frac{BF_{iu}}{BF_{ju}}
\end{equation}
To do that, however, it is necessary for the informative hypotheses considered to be comparable \parencite{guApproximatedAdjustedFractional2018}. That is, the intersection of the constrained region of each hypothesis must be nonempty. Thus, a set of informative hypotheses $\{H_1,\ldots, H_i\}$ is comparable if exist at least one solution to
\begin{equation}
\begin{bmatrix}
R_{1E}\\
R_{1I}
\end{bmatrix} \theta = \begin{bmatrix}
r_{1E}\\
r_{1I}
\end{bmatrix}, \ldots,\begin{bmatrix}
R_{iE}\\
R_{iI}
\end{bmatrix} \theta = \begin{bmatrix}
r_{iE}\\
r_{iI}
\end{bmatrix}.
\end{equation}

This will allow us to select a common solution of $\bm{\theta}$  that can be used as the adjusted prior mean of the encompassing model. Note that to solve these equations, inequality constraints are also considered equality constraints.

For example, $H_1: \theta_1 > \theta_2$ and $H_2: 0 = \theta_1 < \theta_2$ are comparable hypotheses because a common solution exists ($\theta_1 = \theta_2 = 0$). On the contrary, $H_3: \theta_1 < \theta_2 +3 $ and $H_4: \theta_1 = \theta_2$ are not comparable hypotheses because there is no common solution to $\theta_1 = \theta_2 +3 $ and $\theta_1 = \theta_2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Bounded Parameter Space}

The described approach using (multivariate-) normal approximation can be straightforwardly applied when parameters of interest are unbounded. However, in the case of bounded parameters (i.e., variances, correlations, or probabilities), appropriate prior distributions are required to take into account the parameters' bounded nature.

Note that if only inequality constraints are considered, the prior distribution is only used to measure the complexity of the inequality-constrained hypotheses. In this case, the prior probability that an inequality constraint holds will be insensitive to the exact distributional form, as long as the distribution is symmetrical and centered on the focal point. For example, the probability that a correlation coefficient $\rho_i$ is greater than zero is the same if we consider a  uniform distribution between -1 and 1 (taking into account parameter's bounded nature) or if we consider a normal distribution centered on zero (without taking into account parameter's bounded nature; see Figure~\ref{fig:plot-bounded-par}).

<<plot-bounded-par, cache=TRUE, fig.cap="The probability that $\\rho_i>0$ is the same under the uniform prior and the normal prior.">>=
plot_bounded_par()
@

Thus, as suggested by \textcite{guBayesianEvaluationInequality2014}, as long as the complexity of the constrained hypotheses is the same regardless of using (multivariate-) normal distribution or an appropriate bonded prior distribution, it is still possible to follow the described approach. This is true for hypotheses that belong to an equivalent set \parencite[for a rigorous definition of equivalent sets, see][]{guBayesianEvaluationInequality2014, hoijtinkInformativeHypothesesTheory2012}.

In the case of equality and inequality constraints, however, normal approximation is possible. This is done based on the same rationale as before and is only used to compute the conditional prior probability. To estimate the marginal prior density, alternative solutions have to be adopted using appropriate prior distribution to take the bounded nature of the parameters into account. See \textcite{mulderBayesFactorTesting2019} for an example of hypotheses evaluating correlation coefficients in generalized multivariate probit models.

These issues are relevant only when considering the prior distribution. In the case of the posterior distribution, instead, we can safely approximate it to a (multivariate) normal distribution thanks to large-sample theory. Moreover, prior specification is relevant only for the parameters of interest (i.e., the parameters involved in the constraints), as nuisance parameters are integrated out in the Bayes factor. Thus, the results will be insensitive to the actual choice of the prior for the nuisance parameters as long as it is vague enough.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{A Case Study: Hypotheses Testing in Attachment Theory}

In this section, we propose a real case study evaluating different informative hypotheses within attachment theory. This will allow us to face the common issues found in real research applications. First, we provide the required background information regarding attachment theory and the study aims and characteristics. Subsequently, we describe all the steps involved in the evaluation of informative hypotheses using the Bayes factor with the encompassing prior approach. Finally, we briefly discuss the obtained results, considering the prior sensitivity analysis and limitations of this approach.

All analyses were conducted using R statistical software \parencite[v\Sexpr{paste(version$major,version$minor, sep = ".")};][]{rcoreteamLanguageEnvironmentStatistical2021}. All materials, data, and analysis code are available at \url{https://github.com/ClaudioZandonella/Attachment}. Supplemental Material with further details is also available online at \url{https://claudiozandonella.github.io/Attachment/}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background Information}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Attachment Theory}

Attachment theory originates from the pioneering work of \textcite{bowlbyAttachmentLoss1969} and Ainsworth (\citeyear{ainsworthAttachmentExplorationSeparation1970}). They postulated that children in stressful situations actively seek proximity of the caregiver through attachment behaviors (e.g., crying; moving toward the caregiver) in order to fulfil the evolutionary goal of protection from dangers. The main tenet of attachment theory is that the relationships with the caregivers that children develop in the early stages of their lives (i.e., \textit{attachment bonds}) will affect children’s social-emotional future development \parencite{cassidyHandbookAttachmentTheory2016}. Besides behavior, people construct mental representations, or working models, of the self and significant others based on interpersonal experiences. Describing individual differences in self-reported attachment mental representations, empirical research suggested a model based on two dimensional traits \parencite{mikulincerAttachmentAdulthoodStructure2007}: attachment avoidance (characterized by discomfort with closeness, excessive self-reliance, and difficulty in relying on others for obtaining security) and attachment anxiety (characterized by low self-worth and fear of abandonment and rejection). This perspective, initially developed with adult participants, has been extended to the study of self-reported attachment representations during childhood as well as during middle childhood \parencite{kernsAttachmentMiddleChildhood2005, marciBriefExperiencesClose2019}. The two dimensions of attachment mental representations can be combined to provide four different attachment styles:

\begin{itemize}
  \item{\textbf{Secure Attachment}}: Children who are securely attached display optimal emotional regulation and consider their caregiver a secure base.
  \item{\textbf{Anxious Attachment}}: Anxious children manifest high levels of anxiety in stressful situations; their relationships with caregivers are ambivalent, displaying anger or helplessness.
  \item{\textbf{Avoidant Attachment}}: Avoidant children mask distress in stressful situations, displaying few emotions; their relationships with caregivers are characterized by little involvement.
  \item{\textbf{Fearful Attachment}}: Fearful children lack adequate emotional regulation in stressful situations, with the risk of displaying disorganized behaviors.
\end{itemize}

Attachment theory is one of the most supported theories in psychology \parencite{cassidyHandbookAttachmentTheory2016}. In the literature, however, there is still an open debate on the relative influence of mother and father attachment on children's social-emotional development. Four different main theoretical perspectives have been identified \parencite{brethertonFathersAttachmentTheory2010}:
\begin{itemize}
  \item{\textbf{Monotropy Theory}}: Only the principal attachment figure (usually the mother) has an impact on children's development.
  \item{\textbf{Hierarchy Theory}}: The principal attachment figure (usually the mother) has a greater impact on children's development than the subsidiary attachment figure (usually the father).
  \item{\textbf{Independence Theory}}: All attachment figures are equally important, but they affect the children's development differently.
  \item{\textbf{Integration Theory}}: To understand their impact on children's development, it is necessary to consider all attachment relationships together.
\end{itemize}
Contrasting results have been reported by studies investigating which is the “\textit{correct}” theory. No study, however, has tried to properly evaluate the different theoretical perspectives by directly comparing the different hypotheses.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Present Study}

We aimed to directly compare the four different theoretical perspectives regarding the influence of father and mother attachment using the Bayes factor with the encompassing prior approach.

Out of our initial population, $\Sexpr{nrow(data_cluster)}$ Italian children (\Sexpr{perc_females()}\% females) between 8 and 12 years old (\textit{middle childhood}, third to sixth grade) were included in the final analysis. Attachment toward the mother and the father were measured separately using the brief Experiences in Close Relationships Scale - Revised Child Version \parencite[ECR-RC;][]{Brenning2014ThePQ, marciBriefExperiencesClose2019} completed by the children. Subsequently, we performed two separate cluster analyses for mother and father scores. Both analyses supported the existence of the four attachment profiles (see above). The results of the classification are reported in Table~\ref{tab:table-cluster}.

\codespacing
<<table-cluster, cache=TRUE>>=
get_table_cluster()
@
\doublespacing

We measured children's social-emotional development using the Strengths and Difficulties Questionnaire \parencite[SDQ;][]{goodmanWhenUseBroader2010} completed by teachers. Separate scores for externalizing and internalizing problems were obtained as sums of the questionnaire items. For the pourpose of this work, we considered only externalizing problems.  The distribution of externalizing problems ($M = \Sexpr{my_round(mean(data_cluster$externalizing_sum), 2)};\ SD =  \Sexpr{my_round(sd(data_cluster$externalizing_sum), 2)};\ \text{Median} = \Sexpr{my_round(median(data_cluster$externalizing_sum), 1)}$) is presented in Figure~\ref{fig:plot-externalizing-dist}.
<<plot-externalizing-dist, cache=TRUE, fig.cap="Distribution of externalizing problems ($n_{subj} = 847$).">>=
plot_externalizing_dist()
@

Externalizing problems according to attachment style are reported in Table~\ref{tab:table-cluster-ext}. More information about the sample, descriptive statistics, cluster analysis, and analysis of internalizing problems can be found in the Supplemental Material available online \url{https://claudiozandonella.github.io/Attachment/}. All materials and data are available at [TODO].

\codespacing
<<table-cluster-ext, cache=TRUE>>=
get_table_cluster_ext()
@
\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluating Hypotheses with Bayes Factor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Formalization of Informative Hypotheses}

Each theoretical perspective was formalized into a different informative hypothesis taking into account its own theoretical tenets, main evidence in the literature, and our clinical experience in the field.

We used the following notation to formalize the hypotheses. $M$ and $F$ are used to indicate attachment toward the mother and the father, respectively. Specific attachment styles are specified using subscript: $S$ indicates secure attachment, $Ax$ anxious attachment, $Av$ avoidant attachment, and $F$ fearful attachment. For example, $F_{Av}$ represents children with avoidant attachment toward their fathers.

Note that when we do not expect interaction between mother and father attachment, we can consider the roles of the two parents separately. Whereas, if an interaction is expected, it is necessary to take into account the unique combination of mother and father attachment. Thus, for example, we use $M_SF_{Ax}$ to indicate children with secure attachment toward their mothers and anxious attachment toward their fathers. Moreover, $*$ subscript is used to indicate any attachment style and a set of subscripts is used to indicate “\textit{one among}”. For example, $M_SF_{Ax;Av}$ represents children with secure attachment toward their mothers and anxious or avoidant attachment toward their fathers.

To correctly interpret the following Figures, note that the order is important, whereas the actual values are only indicative.

\paragraph{Null Hypothesis} This is a reference hypothesis in which mother attachment and father attachment are expected to have no effect and only gender differences are taken into account (see Section~\ref{sec:def-encompassing-model} for the actual model definition). The hypothesis is represented in Figure~\ref{fig:plot-null-hypothesis}:
\begin{equation}
\begin{gathered}
M_* = 0,\\
F_* = 0.
\end{gathered}
\end{equation}

<<plot-null-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Null Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "null")
@

\paragraph{Monotropy Hypothesis} Father attachment is expected to have no effect, whereas considering mother attachment, we expect the following order: secure children should have the lowest level of problems, anxious and avoidant children should exhibit similar levels of problems, and fearful children should exhibit the highest levels of problems. This hypothesis is represented in Figure~\ref{fig:plot-monotropy-hypothesis}:
\begin{equation}
\begin{gathered}
M_S < M_{Ax} = M_{Av} < M_F,\\
F_* = 0.
\end{gathered}
\end{equation}

<<plot-monotropy-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Monotropy Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "monotropy")
@

\paragraph{Hierarchy Hypothesis} Father attachment is expected to follow the same pattern as mother attachment, but with lesser influence. This hypothesis is represented in Figure~\ref{fig:plot-hierarchy-hypothesis}:
\begin{equation}
\begin{gathered}
M_S < M_{Ax} = M_{Av} < M_F,\\
F_S < F_{Ax} = F_{Av} < F_F,\\
F_{Ax}< M_{Ax};\ ~ \ F_{Av}< M_{Av};\ ~ \ F_F< M_F.
\end{gathered}
\end{equation}

<<plot-hierarchy-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Hierarchy Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "hierarchy")
@

\paragraph{Independence Hypothesis} Mother and father attachment are expected to affect children's outcomes differently. In this case, we consider avoidant attachment toward the father as a condition of greater risk. This hypothesis is represented in Figure~\ref{fig:plot-independence-hypothesis}:
\begin{equation}
\begin{gathered}
M_S < M_{Ax} = M_{Av} < M_F,\\
F_S < F_{Ax} < F_{Av} < F_F.\\
\end{gathered}
\end{equation}

<<plot-independence-hypothesis,fig.width=7, fig.high = 4, out.width="100%", cache=TRUE, fig.cap="\\textbf{Independence Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "independence")
@

\paragraph{Integration Hypothesis}  Mother and father attachment are expected to interact. In this case, we consider secure attachment as a protective factor and fearful attachment as a risk condition. We do not specify the conditions $M_SF_F$ and $M_FF_S$, as their frequency is very low (\Sexpr{perc_rare_condition()}\% of the sample). The hypothesis is represented in Figure~\ref{fig:plot-integration-hypothesis}:
\begin{equation}
\begin{gathered}
M_SF_S< \{M_SF_{Ax;Av} = M_{Ax;Av}F_S\} < M_{Ax;Av}F_{Ax;Av} <  \{M_FF_{Ax;Av} = M_{Ax;Av}F_F\} <  M_FF_F,\\
\text{with}\\
M_{Ax}F_{Ax} = M_{Ax}F_{Av} = M_{Av}F_{Ax} = M_{Av}F_{Av}.
\end{gathered}
\end{equation}

<<plot-integration-hypothesis,fig.width=4, fig.high = 3, cache=TRUE, fig.cap="\\textbf{Integration Hypothesis.} Expected externalizing problems according to mother and father attachment.">>=
plot_hypothesis(hypothesis = "integration")
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Definition of the Encompassing Model} \label{sec:def-encompassing-model}

We define a zero-inflated negative binomial (ZINB) mixed-effect model to take into account the characteristics of the dependent variable and its distribution (see Figure~\ref{fig:plot-externalizing-dist} and see Supplemental Material for more details regarding the zero-inflation analysis \url{https://claudiozandonella.github.io/Attachment/}):
\begin{equation}
y_{ij} \sim ZINB(p_{ij}, \mu_{ij}, \phi),
\end{equation}
where $p_{ij}$ is the probability of an observation $y_{ij}$ being an extra zero (i.e., a zero not coming from the negative binomial distribution) and $1-p_{ij}$ indicates the probability of a given observation $y_{ij}$ being generated form a negative binomial distribution with mean $\mu_{ij}$ and variance $\sigma_{ij}^2 = \mu_{ij} + \frac{\mu_{ij}^2}{\phi}$. Moreover, we define
\begin{equation}
\begin{gathered}
p_{ij} = \text{logit}^{-1}(X_i^T\beta_p+ Z_j^Tu_p),\\
\mu_{ij} = \text{exp}(X_i^T\beta_{\mu}+ Z_j^Tu_{\mu}).
\end{gathered}
\end{equation}

That is, both $\bm{p}$ and $\bm{\mu}$ are modeled separately according to fixed and random effects.
In particular, we consider the children's classroom IDs as a random effect in both cases to account for teachers' different ability to evaluate children's problems. Regarding fixed effects, only the role of gender is considered for $\bm{p}$; for $\bm{\mu}$, the interaction between mother and father attachment are included together with gender. In R formula syntax, we have
\codespacing
<<formula-syntax, eval = FALSE, echo=TRUE>>=
# Regression on p
p ~ gender + (1|ID_class)

# Regression on mu
mu ~ gender + mother * father + (1|ID_class)
@
\doublespacing

The parameters of interest (i.e., those related to mother and father attachment interaction) and the parameter related to gender differences are unbounded. Thus, we can simply specify a normal distribution with a mean of 0 and standard deviation of 3, $\mathcal{N}(0,3)$, as reasonable prior. This prior is intended to be noninformative but without being excessively diffuse.\footnote{Considering 1 as intercept (note that $exp(1)$ is approximately the sample mean value), values included within 1 standard deviation, $exp(1 \pm 1\times SD)$, range between 0 and 55. Although externalizing problems are bounded between 0 and 20, prior predicted values are still reasonable, as they cover all possible values without including excessively large values. More diffuse priors would result in values with a higher order of magnitude, and tighter priors would exclude plausible values (see Supplemental Material for further details \url{https://claudiozandonella.github.io/Attachment/})} We subsequently evaluated the influence of prior specification in a prior sensitivity analysis. Regarding the other nuisance parameters (i.e., intercepts, random effects, and shape parameters) \texttt{brms}' default priors are maintained (see Supplemental Material for further details \url{https://claudiozandonella.github.io/Attachment/}). The encompassing model was estimated using six independent chains with 10,000 iterations (warm-up 2,000).

Finally, note that in this case, we do not need to standardize our parameters, as they represent mean group differences (see Section~\ref{sec:param-standard}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hypothesis Matrices}

Before computing the hypothesis matrix for each informative hypothesis, it is important to consider the contrasts' coding and the resulting parametrization of the encompassing model. We used default treatments contrasts for mother and father attachment \parencite{schadHowCapitalizePriori2019}, considering secure attachment as the reference category. Therefore, model intercept represents children with secure attachment toward both parents ($M_SF_S$) and we have parameters indicating the main effects of mother and father attachment and other parameters for the interaction effect.

Given the informative hypotheses and the parametrization of the encompassing model, we can obtain the respective hypotheses' matrices. To do that, we first need the model matrix with all the conditions of interest. Note that we only have to consider  conditions relevant to the constraints (i.e., those related to mother and father attachment), ignoring other nuisance conditions (i.e., gender and classroom ID).

Subsequently, we can derive the required equality and inequality constraints. In particular, with hypotheses that do not expect interaction between mother and father attachment (i.e., monotropy, hierarchy, and independence hypotheses), all interaction terms are set equal to zero, and main effects are obtain considering the reference level of the other parent (i.e., $M_{Ax}F_S$ is the main effect of anxious attachment toward the mother).  As an example, consider the following code:
\codespacing
<<echo = TRUE, cache=TRUE, eval = FALSE>>=
# Define relevant conditions
attachment <- c("S", "Ax", "Av", "F")
new_data <- expand.grid(
  mother = factor(attachment, levels = attachment),
  father = factor(attachment, levels = attachment)
)

# Get model matrix
mm <- model.matrix(~ mother * father, data = new_data)[,-1] # remove intercept
rownames(mm) <- paste0("M_",new_data$mother, "_F_", new_data$father)

# Get constraints main effect
# M_Ax = M_Av  --->  M_Ax_F_S - M_Av_F_S = 0
# F_F > F_Av   --->  M_S_F_F - M_S_F_Av > 0
rbind(mm["M_Ax_F_S", ] - mm["M_Av_F_S", ],
      mm["M_S_F_F", ] - mm["M_S_F_Av", ])

# Get constraints interaction
# M_F_F_Ax = M_Ax_F_F  --->  M_F_F_Ax - M_Ax_F_F = 0
# M_Ax_F_Av > M_S_F_Ax  --->  M_Ax_F_Av - M_S_F_Ax > 0
rbind(mm["M_F_F_Ax", ] - mm["M_Ax_F_F", ],
      mm["M_Ax_F_Av", ] - mm["M_S_F_Ax", ])
@
\doublespacing
In this way, we can easily specify all the constraints obtaining the respective hypothesis matrix ($R$) and the vector with the constraints' constant values ($r$) for each hypothesis. In all our hypotheses, however, no constraints include constant values. Thus, $r$ is always a vector of zeros and it can be ignored. Moreover, we can also ignore the intercept because no constraints include $M_SF_S$ alone (e.g., $M_SF_S>k$); but it is always compared with other groups (e.g., $M_SF_S< M_{Ax}F_S$). Thus, the inclusion of the intercept is redundant and we can ignore it.

Full hypothesis matrix specification for each hypothesis is available in the Supplemental Material \url{https://claudiozandonella.github.io/Attachment/}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Computing the Bayes Factor}

So far, we defined the hypotheses' matrices, specified the encompassing prior, and obtained the model's posterior distribution. To compute the Bayes factor, we now need the adjusted prior and the posterior of the transformed parameters vector $\bm{\beta}$ (i.e., the parameters that identify the constraints) for each hypothesis.

\begin{itemize}
\item{\textbf{Adjusted Prior $\bm{\beta}$.}} As reported in Section~\ref{sec:adj-prior}, the adjusted prior is required to properly evaluate the constraints. Applying Equation~\ref{eq:normal-transf}, we obtain the prior for the transformed parameter and then set the mean vector to zero.
\item{\textbf{Posterior $\bm{\beta}$.}} The same transformation used for the prior can be applied, this time considering the estimated posterior mean $\hat{\bm{\theta}}$ and the estimated posterior covariance $\hat{\Sigma}_{\theta}$, to obtain the posterior distribution of the transformed parameters vector $\bm{\beta}$.
\end{itemize}

As discussed in Section~\ref{sec:rank-hypo}, Equation~\ref{eq:normal-transf} requires hypothesis matrix $R$ to be \textit{full-row-rank} (i.e., all constraints must be linearly independent). However, this is not the case for the hierarchy hypothesis. Thus, we followed the solution presented in Section~\ref{sec:rank-hypo}. Detailed information is available in the Supplemental Material \url{https://claudiozandonella.github.io/Attachment/}.

Now, we have all the elements required to compute the Bayes factor for each hypothesis as described in Section~\ref{sec:compute-bf}. Moreover, assuming that each hypothesis is equally likely a priori, we can calculate the posterior probability of each hypothesis as \parencite{hoijtinkInformativeHypothesesTheory2012, guBayesianEvaluationInequality2014}:
\begin{equation}
\text{Posterior Probability} H_i = \frac{BF_{iu}}{\sum_iBF_{iu}}.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results and Prior Sensitivity Analysis}

Bayes factor and posterior probability of each hypothesis are reported in Table~\ref{tab:table-bf-results}. Results clearly indicate that among the considered hypotheses, the Monotropy Hypothesis is the most supported by the data.

\codespacing
<<table-bf-results, cache=TRUE>>=
get_table_bf(bf_result = BF_weights_ext,
             path_img = "figure/")
@
\doublespacing

Remember, however, that prior specification affects the Bayes factor results. It is therefore recommended, to evaluate the results we would obtain using different prior settings as well. In particular, we consider as possible priors for the parameters of interest:
\begin{itemize}
  \item{$\mathcal{N}(0,.5)$}: Unreasonable tight prior
  \item{$\mathcal{N}(0,1)$}: Tighter prior
  \item{$\mathcal{N}(0,3)$}: Original prior
  \item{$\mathcal{N}(0,5)$}: More diffuse prior
  \item{$\mathcal{N}(0,10)$}: Unreasonably diffuse prior
\end{itemize}

The results of the prior sensitivity analysis are reported in Table~\ref{tab:table-sens-prior-analysis}.
\codespacing
<<table-sens-prior-analysis, cache=TRUE>>=
get_table_sens_analysis(summary_sensitivity = summary_sensitivity_ext)
@
\doublespacing
Overall results consistently indicate the Monotropy Hypothesis as the most supported by the data. However, we can observe two distinct patterns. As the prior becomes more diffuse, the order of magnitude of the Bayes factor comparing each hypothesis with the encompassing model increases. Moreover, the probability of the Null Hypothesis increases with more diffuse priors, whereas the probabilities of the Hierarchy, Independence and Integration Hypothesis increases with tighter priors.

To interpret these patterns, remember that order constraints are insensitive to the distribution specification as long as the distribution is symmetric and centered on the constraint focal point. On the contrary, equality constraints are highly affected by the prior definition. As the prior gets more diffuse, the density value at zero decreases as well and, even for posterior distributions centered far from zero, the densities ratio at zero will favor the posterior. Thus, as the prior becomes more diffuse, the Bayes factor will increasingly favor the hypotheses with equality constraints. For tighter priors, the Bayes factor will strongly penalize hypotheses with equality constraints if these are not correct (see Figure~\ref{fig:plot-sensitivity-prior}).

All the defined hypotheses include equality constraints. Thus, for more diffuse priors we observe that the order of magnitude of the Bayes factor comparing each hypothesis with the encompassing model increases. Moreover, the hypothesis with the highest number of equality constraints (e.g., Null Hypothesis) will be favored over hypotheses with a smaller number of equality constraints (e.g., Hierarchy, Independence and Integration Hypothesis).

<<plot-sensitivity-prior, cache=TRUE, fig.cap="Evaluating densities at 0 for different prior settings and a selection of parameters posteriors.">>=
get_plot_sensitivity()
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Limitations}

When interpreting the results, it is important to take into account the limitations of the Bayes factor. First of all, the selected hypothesis is relative to the data and the set of hypotheses considered. That means that we should not interpret the selected hypothesis as the only “\textit{correct}” one. Perhaps we did not consider some important aspects, and new hypotheses may reveal themselves to be actually much better than the previous ones. For example, regarding the attachment results, taking into account the interaction between children's gender and attachment may be fundamental (is it possible that attachment toward the father plays a different role between girls and boys?). This limitation, however, becomes an advantage as it forces the researchers to focus on the definition of the hypotheses: to obtain good answers, first we need to ask the right questions.

Moreover, considering a unique single winning hypothesis may be limiting, as different hypotheses could help explain different aspects of the process under investigation. For example, mother and father attachment may play different roles at different ages. Thus, the selected hypothesis may vary according to children's age or other conditions. This should warn against interpreting the results in terms of winning or losing, focusing uniquely on the selected hypothesis and discarding other hypotheses from further investigations.

Another important limitation we observed is the sensitivity of the results to the prior specification. \textcite{gelmanBayesianDataAnalysis2013} do not recommend using the Bayes factor, given its sensitivity to model definition arbitrary choices. Evaluating the results under different conditions and transparently discussing the reasons behind arbitrary choices would help to interpret results more consciously \parencite{schadWorkflowTechniquesRobust2021}.

Finally, another limitation related to the use of the Bayes factor with the encompassing prior approach is that we do not obtain the actual estimates of the parameters posterior. The only information we get is the selected hypothesis, but we have no other information regarding the actual parameters' values. Therefore, for example, we cannot assess the actual effect sizes. To overcome this limitation, we have to obtain via Bayesian inference the actual posterior of the model parameters given the prior, formalized according to parameters' constraints, and the observed data \parencite[for an introduction to Bayesian estimation][]{kruschkeBayesianNewStatistics2018}. However, posterior estimation under inequality constraints requires the use of appropriate advanced computational algorithms \parencite[see][]{ghosalBayesianInferenceGeneralized2022, katoBayesianApproachInequality2006}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

In this paper, we presented a detailed description and an applied example of the Bayes factor with the encompassing prior approach. As discussed in the introduction, this approach has several advantages over traditional NHST: it is possible to evaluate complex hypotheses with equality and inequality constraints, multiple hypotheses can be tested simultaneously, and Bayes factor and posterior probabilities allow quantifying the relative evidence from the data in favor of the hypotheses.

This approach is of particular interest to the researchers, because informative hypotheses allow them to formalize their expectations with great flexibility. In this paper, we considered only the presence/absence of an effect (i.e., $\theta = 0$) or the expected effects order (i.e., $\theta_i > \theta_j$). However, in case of clear indications about the actual effects, researchers can specify the expected range of values for the effects of interest (i.e., $a<\theta_i<b$). This allows even a greater level of precision in the formalization of informative hypotheses.

As emphasized by \textcite[p.229]{guApproximatedAdjustedFractional2018}, “This class of informative hypotheses covers a much broader range of scientific expectations than the class of standard null hypotheses. In addition, by testing competing informative hypotheses directly against each other a researcher obtains a direct answer as to which scientific theory is most supported by the data”.

The available literature, however, is often technical and difficult to follow for non-experts in this approach. This paper filled this gap by providing a detailed description of all steps and elements involved in the computation of the Bayes factor with the encompassing prior approach. This will enhance researchers' awareness regarding all pros and cons of this method, helping them avoid applying statistical techniques as black boxes.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography
\end{document}

%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% This work is "maintained" (as per LPPL maintenance status) by
%% Daniel A. Weiss.
%%
%% This work consists of the file  apa7.dtx
%% and the derived files           apa7.ins,
%%                                 apa7.cls,
%%                                 apa7.pdf,
%%                                 README,
%%                                 APA7american.txt,
%%                                 APA7british.txt,
%%                                 APA7dutch.txt,
%%                                 APA7english.txt,
%%                                 APA7german.txt,
%%                                 APA7ngerman.txt,
%%                                 APA7greek.txt,
%%                                 APA7czech.txt,
%%                                 APA7turkish.txt,
%%                                 APA7endfloat.cfg,
%%                                 Figure1.pdf,
%%                                 shortsample.tex,
%%                                 longsample.tex, and
%%                                 bibliography.bib.
%%
%%
%% End of file `./samples/longsample.tex'.

<<biber>>=
system(paste("biber", sub("\\.Rnw$", "", current_input())))
@
