%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%%
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% ----------------------------------------------------------------------
%%
\documentclass[man, floatsintext]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}

% KabelExtra packages
\usepackage{booktabs} %
\usepackage{longtable} %
\usepackage{array} %
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float} %
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\usepackage{amsmath}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{../Biblio-attachment.bib}

%----    LaTeX Settings    ----%
% \usepackage{setspace} % linee spacing
\newcommand{\codespacing}{\linespread{1}}
\newcommand{\doublespacing}{\linespread{1.655}}

\setcounter{secnumdepth}{4} % Show section number
%----

\title{Evaluating Informative Hypotheses With Equality and Inequality Constraint: The Bayes Factor Encompassing Prior Approach}
\shorttitle{Evaluating Informative Hypotheses With Equality and Inequality Constraint: The Bayes Factor Encompassing Prior Approach}

\author{Claudio Zandonella Callegher, Tatiana Marci, Pietro De Carli, Gianmarco Altoè}
\affiliation{UNIPD}

\leftheader{loook}

\abstract{\lipsum[1]}

\keywords{keyword-1, keyword-2}

\authornote{
   \addORCIDlink{Claudio Zandonella Callegher}{0000-0000-0000-0000}

  Correspondence concerning this article should be addressed to Claudio Zandonella Callegher, Department of Developmental Psychology and Socialisation, University of Padua, Via Venezia, 8 - 35131 Padova, Italy.  E-mail: claudiozandonella@gmail.com}

<<settings, echo=FALSE, message=FALSE>>=
library(kableExtra)
library(ggplot2)

# Chunks settings
knitr::opts_chunk$set(echo = FALSE,
                      # Plot settings
                      dev = "tikz", dev.args=list(pointsize=12),fig.align='center',
                      fig.height=3, fig.width=5, fig.pos = "h",

                      # Code output width
                      tidy=TRUE, tidy.opts = list(width.cutoff = 87),
                      # comment = NA, prompt = TRUE

                      # Cache options
                      cache = FALSE, autodep=TRUE)

# Console output width
options(width = 90)

# Chunk theme
thm=knit_theme$get("bclear")
knitr::knit_theme$set(thm)
knitr::opts_chunk$set(background = c(.98, .98, 1))

# Option KableExtra
# options(knitr.kable.NA = '')

## ggplot settings
theme_set(theme_classic()+
            theme(text = element_text(size=12)))



devtools::load_all("../../")
source("../Utils_report.R")

#----    load drake    ----

drake_load_all()
@

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

One of the principal aspects of empirical research is the evaluation of research or theoretical hypothesis. When conducting a study, researcher usually have expectations based on hypotheses or theoretical perspectives they want to evaluate according to observed data.

For example researcher could be interested in evaluating whether the treatment group scores higher than the control group.

they want to evaluate them according to / if they are supported by the data; collecting evidence.

In psychology the most often use tool (the dominant statistical approach) to evaluate research hypotheses (if an hypothesis is supported by the data), is the Null hypothesis Significance Test (NHST). Debate in the literature about the utility of NHST (p-value) in hypothesis testing. this approach, however, present several limitation. First, data are evaluated against the catch all null-hypothesis that "nothing is going on" (Cohen, 1994; Royall, 1997, pp. 79–81). This is rarely the actual hypothesis of interest of the researcher who usually have some specific expectation about the phenomenon under investigation. Second, the NHST do not formalize the alternative hypothesis thus in the NHST is not possible to "accept" an hypothesis but only reject the Null-Hypothesi (the p-value do not quantify the evidence in favor of an hypothesis). This is inconvenient to the researcher as in case of not significant result is left in a state of indecision. Third limited type of hypothesis that can be evaluated, no include expectation in the form of parameter order constraints. [Expectation can not be formulated - you can use one-side test however, when there are more groups, more variables, or more constraints, null hypothesis testing is not the appropriate tool see Van de Schoot, Hoijtink et al. (2011) for a detailed discussion.\parencite{vandeschootIntroductionBayesianModel2011}]

[classical significance tests are only suitable for testing a null hypothesis against a single alternative, and unsuitable for testing multiple hypotheses with equality as well as order constraints (Silvapulle \& Sen, 2004). Second, traditional model comparison tools (e.g., the AIC, BIC, or CFI) are generally not suitable for evaluating models (or hypotheses) with order constraints on certain parameters (Mulder et al. 2009; Braeken, Mulder, \& Wood, 2015). \textcite{mulderSimpleBayesianTesting2019}]

[broad class of hypotheses]

In general, researchers are interested in comparing different hypothesis and understand which is the most supported by the data. Model comparison is a different approach that allows to compare different models. Models formalized according to researchers expectation or theoretical perspectives are compared against each other according to the data. In this way, it is possible to evaluate the relative plausibility of each model given the data and the set of model considered. Models can be compared using Information criteria (the most popular are the AIC and BIC) and the relative plausibility of each model can be computed. [Information criteria allow to evaluate the ability of the model to predict new data penalizing for model complexity.] Model comparison has several advantages and in particular it allows to directly compute the relative plausibility of each hypothesis. Model comparison, however, is still limited in the possibility to formalize hypothesis that include complex parameter order constraints.

An alternative approach to evaluate research hypotheses is based on the Bayes Factor. In the last 25 years, there has been an increasing interest / growth in popularity in the use of Bayes Factor and its adoption has been proposed (advocated) as the solution to the critical issues of the NHST. Although the Bayes Factor has its own limitations and  does not solve the fundamental issues of the misuse of statistical techniques \url{https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/}, it offers some clear advantages. In particular, the Bayes Factor allows to compare hypotheses obtaining a relative index of evidence, like in the model comparison approach, but it also allows to easily compare researchers' expectations. In fact, informative hypotheses can be formalized according to researchers expectation using equality and inequality constraints and than compared.

The evaluation of informative hypothesis based on the Bayes Factor has received increasing attention and several contribution can be found in the literature. \textcite{vandeschootIntroductionBayesianModel2011} presented a general introduction to informative hypotheses testing using the Bays Factor, whereas \textcite{hoijtinkInformativeHypothesesTheory2012} offered more detailed description on the development of this approach. Other studies, instead, considered the application of this method in specific models; for example: mixed effect models \parencite{katoBayesianApproachInequality2006}, evaluation of correlation coefficients \parencite{mulderBayesFactorsTesting2016} and multinomial models \parencite{heckMultinomialModelsLinear2019}. In addition, \textcite{guBayesianEvaluationInequality2014} proposed an approximate procedure to evaluate as long as  the structural parameters are unbounded.

Note, however, that all previously mentioned studies considered informative hypothesis only with inequality constraints. Equality constraints could not be defined together with inequality constraints in the same hypothesis but they had to be approximated as "about equality constraints" (i.e., a equality constraints of type $\theta_i = \theta_j$ is approximated as $|\theta_i - \theta_j| < \xi$ for a value of $\xi$ small enough {TODO more below?}).

It's only more recently that \textcite{guApproximatedAdjustedFractional2018} introduced an approximate procedure to evaluate both equality and inequality constraints using the Bayes Factor in the case of general statistical models (i.e., generalized linear mixed models and structural equation models). Subsequentley, this approach was exteended  by \textcite{mulderBayesFactorTesting2019} to the generalized multivariate probit models. Finally, \textcite{mulderSimpleBayesianTesting2019} proposed an accurate procedure (i.e., not based on an approximation) that allows to test informative hypothesis with equality and inequality constraints in linear regression models.

The development and implementation of this approach are of particular interest because, although with its limits (discussed latrer), it allows researchers to directly evaluate their expectations and hypotheses. Available literature, however, it is mainly based on articles published in specialize journals of mathematical psychology. The complexity of these articles makes it difficult for non-experts in this approach to clearly understand all the steps involved. On the other hand, articles offering a general introduction to the Bayes Factor do not provide enough details to allow readers to apply autonomously the method to their problems, but they usually relay on ad-hoc solutions implemented in some statistical software.

The aim of this paper is to offer a clear e detailed description of the entire procedure. In particular we refer to the approximated method proposed by \textcite{guApproximatedAdjustedFractional2018} as it is applicable to a wider range of conditions than the more accurate approach proposed by \textcite{mulderSimpleBayesianTesting2019}. The paper is organized as follow. First the method is introduced considering a toy example. This allow to provide a  detailed description of all steps and elements involved in the formalization of informative hypothesis and Bayes Factor computation. Subsequently an application of the method to real data is presented to offer the opportunity to discuss the typical issues encountered in real complex scenario.

[Code provided in the SM]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Bays Factor for Informative Hypothesis Testing}

The evaluation of informative hypothesis with equality and/or inequality constraints involves several steps and elements. In this section, first we describe how to formulate informative hypothesis. Subsequently, we introduce the Bayes Factor considering the encompassing prior approach based on the approximated method proposed by \textcite{guApproximatedAdjustedFractional2018}.

[In order to facilitate the understanding, a toy example is considered.]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Formulation of Informative Hypothesis}\label{sec:informative-hypothesis}

Informative hypotheses can be defined according to researchers expectation, evidence form the literature or theoretical perspectives and they are formed by equality and/or inequality constraints on certain model parameters. These constraints are obtain as a linear combination of certain parameters and eventual constant values. For example, it is possible to state that two parameters are equal ($\theta_i = \theta_j$), one parameter is grater than another ($\theta_i > \theta_j$), the difference between two parameter is less than a given value ($\theta_i - \theta_j < .5$) or express other complex conditions ($2\times\theta_i - \theta_j < .1 \times \sigma$).

Thus, an informative hypothesis $H_i$ with equality and inequality constraints can be expressed in the form
\begin{equation}
H_i:\ \ R_E\theta = r_E \ ~ \ \& \ ~ \ R_I\theta > r_I,
\end{equation}
where $R_E$ is a matrix expressing the equality constraints and $r_E$ is a vector containing the constants of the equality constraints. Whereas, $R_I$ is a matrix expressing the inequality constraints and $r_E$ is a vector containing the constants of the inequality constraints. Finally, $\theta$ is a vector of the model parameters involved in the constraints.

As an example, consider a study evaluating the efficacy of a new psychological treatment that is supposed to improve a given cognitive ability. In the study, a control group receiving no treatment and another group receiving the traditional treatment were included as comparison. Moreover, imagine that the new psychological treatment was administrated in three different modalities to different groups: individually, in group, online. Researchers expect no differences between the three modalities but they assume that the new treatment to perform better than the traditional one that in turn is better than the control condition. We can express this hypothesis as
\begin{equation}
H_i:\ \ \theta_{control}< \theta_{traditional} < \theta_{individual} = \theta_{group} = \theta_{online},
\end{equation}
were $\theta_{control}$ is the average score of the control group, $\theta_{traditional}$ is the average score of the group receiving the traditional treatment, and $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$ are the average scores of the groups receiving the new treatment individually, in group, and on-line, respectively. The correspondig formulation of the hypothesis using matrix notation introduced before is
\begin{equation}
\begin{aligned}
H_i: &\\
& R_E\theta =
\begin{bmatrix}
0 & 0 & 1 & -1 & 0\\
0 & 0 & 0 & 1 & -1
\end{bmatrix}	\begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} = \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_E,\\~\\
& R_I\theta =
\begin{bmatrix}
-1 & 1 & 0 & 0 & 0\\
0 & -1 & 1 & 0 & 0
\end{bmatrix}	\begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} > \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_I.
\end{aligned}
\end{equation}
Note how each row of $R_E$ and $R_I$ matrices expresses an equality or an inequality constraint, respectively. For example, in the first row of $R_E$ we have $\theta_{individual} - \theta_{group} = 0$ (i.e., $\theta_{individual} = \theta_{group}$) and in the first row of $R_I$ we have $\theta_{traditional} - \theta_{control} > 0$ (i.e., $\theta_{traditional} > \theta_{control}$).

Now that we have understood how to define an informative hypothesis with equality and inequality constraints introducing an appropriate notation, lets see how to evaluate an informative hypothesis using the Bayes Factor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Bayes Factor}

The Bayes Factor of hypothesis $H_1$ against a competing hypothesis $H_2$ is defined as the ration between the marginal likelihoods of the two hypothesis:
\begin{equation}
BF_{12} = \frac{Pr(Y|H_1)}{Pr(Y|H_2)} = \frac{\int l(Y|\theta_1, H_1) \pi(\theta_1| H_1)\,d\theta_1}{\int l(Y|\theta_2, H_2) \pi(\theta_2| H_2)\,d\theta_2},
\end{equation}
where $Y$ indicates the data, $\theta_i$ is the vector of parameters, $l(Y|\theta_i, , H_i)$ is the likelihood function, and $\pi(\theta_i| H_i)$ is the prior of the parameters under the hypothesis $H_i$. The marginal likelihood $Pr(Y|H_i)$ is a measure of the plausibility of the data under $H_i$.

Therefore, the Bayes Factor $BF_{12}$ quantifies the relative support of the data for the two competing hypotheses (and not the ratio between the probability of the two hypotheses). For values close to 1, the Bayes Factor indicates that $H_1$ and $H_2$ have similar support from the data. Larger values indicate evidence in favor of $H_1$, whereas values close to 0 indicate evidence in favor of $H_2$. The ratio between the posterior probabilities of the two hypotheses can be computed as
\begin{equation}
\frac{Pr(H_1|Y)}{Pr(H_2|Y)} = \overbrace{\frac{Pr(Y|H_1)}{Pr(Y|H_2)}}^{BF_{12}} \times \frac{Pr(H_1)}{Pr(H_2)},
\end{equation}
where $Pr(H_i)$ is th prior probability of $H_i$ and $Pr(H_i|Y)$ is the posterior probability of $H_i$.For a detailed description of the Bayes Factor considering also its interpretation and application in different context see \textcite{wagenmakersBayesianHypothesisTesting2010, mulderEditorsIntroductionSpecial2016, heckReviewApplicationsBayes2020}.

Note that to compute the marginal likelihood $Pr(Y|H_i)$ of an hypothesis $H_i$ it is necessary to integrate the product between the likelihood and the prior. These integrals, however, are usually difficult to compute [in particular in the case of hypothesis with order constraints]. In the case of hypothesis with equality and inequality constraints, however, it is possible to adopt the encompassing prior approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Encompassing Prior Approach}

The basic idea of the encompassing prior approach is to consider an informative hypothesis as a subset of the parameter space of an unconstrained model. Thus, to evaluate the plausibility of an hypothesis we can consider the proportion of parameters of the unconstrained model that satisfy the constraints. More specifically, given an informative hypothesis $H_i$ and an unconstrained model $H_u$ (or \textit{encompassing model}) that does not contains any constraints on the parameters, if the prior under $H_i$ is defined as a truncation of the proper prior under $H_u$ (\textit{encompassing prior}) according to the constraints, then the Bayes Factor between $H_i$ and $H_u$ can be written as
\begin{equation}\label{eq:BF-encompassing}
\begin{aligned}
BF_{iu} &= \frac{Pr(\text{Inequality Const|Equality Const, Data, } H_u)}{\pi(\text{Inequality Const|Equality Const, } H_u)} \times \frac{Pr(\text{Equality Const|Data, } H_u)}{\pi(\text{Equality Const}|H_u)} \\ ~\\
&= \frac{Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)}{\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)} \times \frac{Pr(R_E\theta =r_E| Y, H_u)}{\pi(R_E\theta =r_E| H_u)}.
\end{aligned}
\end{equation}

The first term is the ratio between the conditional posterior probability and conditional prior probability that the inequality constraints hold under the unconstrained model $H_u$ given the equality constraints. The second term is the ratio between marginal posterior density and the marginal prior density of the equality constraints under $H_u$, the well-known Savage–Dickey density ratio \parencite{dickeyWeightedLikelihoodRatio1971, wetzelsEncompassingPriorGeneralization2010}. In particular, the four elements can be interpreted as:
\begin{itemize}
  \item{The \textbf{conditional posterior probability} $Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)$ is a measure of the fit of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{conditional prior probability} $\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)$ is a measure of the complexity of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal posterior density} $Pr(R_E\theta =r_E| Y, H_u)$ is a measure of the fit of the equality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal prior density} $\pi(R_E\theta =r_E| H_u)$ is a measure of the complexity of the equality constraints of $H_i$ under $H_u$.}
\end{itemize}
Summarizing at the denominators we have measures of the \textbf{complexity} of the inequality and equality constraints of the informative hypothesis $H_i$; at the numerators, instead, we have measures of th \textbf{fit} of the data to the inequality and equality constraints of the informative hypothesis $H_i$. Thus, if the hypothesis $H_i$, although applying constraints to the parametric space (less complexity), is still able to provide a good description of the data, the Bayes Factor will favor $H_i$. On the contrary, if the support of by the data is poor, the Bayes Factor will favor the unconstrained model $H_u$.

The proof of the formulation of the Bayes Factor following the encompassing prior approach and the evaluation of its consistency (i.e., the probability of selecting the correct hypothesis goes to 1 for the sample size going to infinity) is provided in \textcite{guApproximatedAdjustedFractional2018, mulderBayesFactorTesting2019}. Note that slightly different notation is used here to enhance comprehension and underline that at the numerators we have values computed from the posterior of $H_u$, whereas at the denominators we have values computed from the prior of $H_u$.

To compute the Bayes Factor following the encompassing prior approach, only the prior and the posterior of the unconstrained model are required. In order to do that, we first need to define the encompassing prior for the unconstrained model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Definition of Encompassing Prior}

Prior specification is an important element as the resulting value of the Bayes Factor is affected by the prior choice. This aspects is particularly relevant in the case of equality constraints \parencite{lindleyStatisticalParadox1957, bartlettCommentLindleyStatistical1957}, whereas inequality constraints are not affected as long as the prior is symmetric and centered to the focal point of interest (this aspect will be further discussed in the next section).

In order to avoid arbitrary specification of the prior, different methods have been proposed in the literature. For example: Jeffreys-Zellner-Siow (JZS) objective priors do not require subjective specification \parencite{jeffreysTheoryProbability1961,zellnerPosteriorOddsRatios1980, bayarriExtendingConventionalPriors2007}; partial Bayes Factor \parencite{desantisMethodsDefaultRobust1999} uses part of the data to define the prior wheres the other part is used to compute the Bayes Factor; intrinsic Bayes Factor \parencite{bergerIntrinsicBayesFactor1996} and fractional Bayes Factor \parencite{ohaganFractionalBayesFactors1995} are a variation of the partial Bayes Factor where priors are defined according to the average of all possible minimal subsets of the data or to a given small fraction of the data, respectively.

Both, \textcite{guApproximatedAdjustedFractional2018, mulderSimpleBayesianTesting2019} based their approach on the fractional Bayes Factor. Starting from a non-informative prior, a minimal fraction of the data is used to obtain a posterior that is subsequently used as proper prior, we refer to it as \textit{fractional prior} (have you ever heard Lindley's \citeyear{lindleyBayesianStatisticsReview1972} famous quote \textit{“Today’s posterior is tomorrow’s prior”}?). The remaining part of the data is used to compute the Bayes Factor. The two approaches, however, have an important difference. \textcite{guApproximatedAdjustedFractional2018} approximated the obtained fractional prior (as well as the posterior, see Section~TODO) to a (multivariate) normal distribution. In fact, due to large-sample theory \parencite{gelmanBayesianDataAnalysis2013}, parameter posterior distribution can be approximated to a (multivariate) normal distribution. On the contrary, \textcite{mulderSimpleBayesianTesting2019} provided analytic solution in the case of linear regression models obtaining accurate quantification.

All methods discussed above suggested objective procedures to avoid arbitrary prior specification. Nevertheless, using subjective (reasonable) priors according to previous information or experts' indication is still a possible approach. Keep in mind, however, that prior specification (even if obtained through an \textit{objective procedure}) affects the Bayes Factor result. Thus, it is fundamental to conduct a prior sensitivity analysis to evaluate the influence of prior specification on the results \parencite{duBayesFactorOnesample2019}.

Going back to our psychological treatment example, we could use independent normal distributions to specify the prior of each parameter of interest (i.e. $\theta_{control}$, $\theta_{traditional}$, $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$) obtaining as resulting prior a multivariate normal distribution with mean vector $\mu_{\theta}$ and covariance matrix $\Sigma_{\theta}$. In this way we can still follow the approach proposed by \textcite{guApproximatedAdjustedFractional2018}, based on normal approximation, that will simplify the computation of the Bayes Factor. For example, suppose that, according to experts indications, a reasonable prior choice for all parameters of interest is a normal distribution with mean zero and standard deviation 2:  $\mathcal{N}(0,2)$. The resulting prior is a multivariate normal distribution have mean vector $\mu_{\theta}$ and covariance matrix $\Sigma_{\theta}$:
\begin{equation}
\begin{gathered}
\pi(\bm{\theta})\sim \mathcal{N}(\mu_{\theta}, \Sigma_{\theta})\\~\\
\text{where  }\ \bm{\theta} = \begin{bmatrix}
0 \\ 0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ \ \text{and  } \ \
\Sigma_{\theta} = \begin{bmatrix}
4 & 0 & 0 & 0 & 0\\
0 & 4 & 0 & 0 & 0\\
0 & 0 & 4 & 0 & 0\\
0 & 0 & 0 & 4 & 0\\
0 & 0 & 0 & 0 & 4
\end{bmatrix}.
\end{gathered}
\end{equation}

At this point, several authors underline the importance of centering the prior distribution on the constraints points of interest. Let's further discuss this issue in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Adjusting Prior Mean}

When evaluating informative hypothesis with equality and inequality constraints using the Bayes Factor, the priors have to be centered on the focal points of interests  \parencite{zellnerPosteriorOddsRatios1980, jeffreysTheoryProbability1961, mulderPriorAdjustedDefault2014}. In the case of equality constraints, centering the prior allows to consider values close to the point of interest more likely a priori than values far away. This should be in line with researchers' expectation, otherwise one could question why testing that value.

In the case of inequality constraints, instead, this adjustment is done to guarantee that no constrain is favored a priori. Consider the example represented in Figure~\ref{fig:plot-prior-adj} where the hypothesis $\theta_i > k$ is evaluated against $\theta_i < k$. Remember that, when computing the Bayes Factor, prior probability that the constraint hold is used as a measure of complexity of the hypothesis. Thus, if the prior is not centered on the focal point of interest (i.e., $k$), the less complex hypothesis will be erroneously preferred a priori over the other. Only by centering the prior on the focal point the hypotheses will be equally likely a priori.

<<plot-prior-adj, fig.cap="Example of non centered prior considering the constraints $\\theta_i > k$ vs. $\\theta_i < k$">>=
plot_prior_adj()
@

Centering the prior to the focal point, however, can be difficult in the case of complex hypothesis where constraints are defined as linear combination of several parameters. To overcome this issue, \textcite{guApproximatedAdjustedFractional2018} proposed the following transformation of the parameters of interest:
\begin{equation} \label{eq:param-transf}
\begin{aligned}
\bm{\beta} = &R\bm{\theta}-r\\~\\
\text{with  } \bm{\beta} = \begin{bmatrix}\beta_E\\\beta_I\end{bmatrix},\ R =& \begin{bmatrix}R_E\\R_I\end{bmatrix} \text{  and  } r = \begin{bmatrix}r_E\\r_I\end{bmatrix},
\end{aligned}
\end{equation}
where $\bm{\theta}$ is the vector of original parameters, $R$ is the matrix expressing equality and inequality constraints, and $r$ is the vector with the constants of the equality and inequality constraints. Doing this, the informative hypothesis under evaluation becomes:
\begin{equation}
H_i:\ \ \beta_E = 0 \ ~ \ \& \ ~ \ \beta_I > 0.
\end{equation}

This parameter transformation has the advantage of simplifying the hypothesis expression without changing the original expectations. In fact, for example, evaluating $\theta_i > \theta_j$ is equivalent to evaluate $\beta_i = \theta_i - \theta_j > 0$. Thus, given the original prior $\pi(\bm{\theta})\sim\mathcal{N}(\mu_{\theta}, \Sigma_{\theta})$, the prior of the new parameter vector $\bm{\beta}$ is given by
\begin{equation}\label{eq:normal-transf}
\pi(\bm{\beta}) \sim \mathcal{N}(\mu_{\beta}, \Sigma_{\beta}) = \mathcal{N}(R\bm{\theta}-r, R\Sigma_{\theta}R^T).
\end{equation}

Note that this operation is nothing more than applying a linear transformation to the original multivariate normal distribution.\footnote{Given a multivariate normal distribution $Y\sim\mathcal{N}(\mu, \Sigma)$, the result of a linear transformation $AY + b$ is still a multivariate normal distribution with vector mean $\mu_{t}=AY+b$ and covariance matrix $\Sigma_{t} = A\Sigma A^T$.} In order to do that, however, it is necessary for the matrix $R$ to be \textit{full-row-rank} (i.e., all rows are linearly independent). If this is not the case, the obtained matrix $\Sigma_{\beta}$ will not be a proper covariance matrix. In Section~\ref{sec:rank-hypo} we will discuss a solution to overcome this limit.

Now to center the prior to the focal points we can simply set the mean vector to zero. Thus, the adjusted prior of $\bm{\beta}$ is
\begin{equation}
\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(\bm{0}, \Sigma_{\beta}) = \mathcal{N}(\bm{0}, R\Sigma_{\theta}R^T).
\end{equation}

Considering the psychological treatment example, the obtained adjusted prior is $\mathcal{N}(\bm{0}, \Sigma_{\beta})$ where:
\begin{equation}
\bm{0} =
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ ~ \ \text{  and  }\ ~ \
\Sigma_{\beta} = R\Sigma_{\theta}R^T = \begin{bmatrix}
8 & -4 & 0 & 4 \\
-4 & 8 & 0 & 0 \\
0 & 0 & 8 & -4 \\
4 & 0 & -4 & 8
\end{bmatrix}.
\end{equation}

So far we have defined the prior for the parameter vector $\bm{\theta}$ of the encompassing model. Moreover, we have obtained the adjusted prior for the new transformed parameter vector $\bm{\beta}$ that will allow us to properly evaluate the equality and inequality constraints. At this point we need to compute the posterior of the encompassing model parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Posterior Encompassing Model}

Posterior distribution of the encompassing model parameters can be obtained thorough numerical approximation using Markov Chain Monte Carlo (MCMC) sampling algorithms, such as Metropolis–Hastings algorithm \parencite{hastingsMonteCarloSampling1970} or Gibbs sampling \parencite{gemanStochasticRelaxationGibbs1984}. Bayesian statistical inference methods are implemented in all major statistical software. In R statistical software \parencite{rcoreteamLanguageEnvironmentStatistical2021}, for example, the popular \texttt{brms} package \parencite{burknerBrmsPackageBayesian2017, burknerAdvancedBayesianMultilevel2018a}, which is based on STAN \parencite{standevelopmentteamRStanInterfaceStan2020}, allows to easily conduct Bayesian inference.

Following \textcite{guApproximatedAdjustedFractional2018} approach, once we obtained the model posterior, we can approximate it to a (multivariate) normal distribution. Thus, the resulting posterior distribution is
\begin{equation}
Pr(\bm{\theta}|Y) \sim \mathcal{N}(\hat{\bm{\theta}}, \hat{\Sigma}_{\theta})
\end{equation}
where posterior mean $\hat{\bm{\theta}}$ and posterior covariance $\hat{\Sigma}_{\theta}$ can be computed directly from the posterior draws. Next, we can obtain the posterior with respect to the vector parameters $\bm{\beta}$ applying the same transformation used for the prior distribution,
\begin{equation}
Pr(\bm{\beta}|Y) \sim \mathcal{N}(\hat{\bm{\beta}}, \hat{\Sigma}_{\beta}) = \mathcal{N}(R\hat{\bm{\theta}}-r, R\hat{\Sigma}_{\theta}R^T).
\end{equation}

At this point, we have both the prior and the posterior distributions of the parameters of interests of the encompassing model. Before to proceed, however, let's underline few important aspects.

When defining the prior, we adjusted the prior mean centering it over the constraint focal points. This change would slightly influence the posterior result as well. However, as underlined by \textcite{guApproximatedAdjustedFractional2018}, small prior changes will result in negligible changes on the posterior for large samples due to large-sample theory. For this reason in their approach, the authors do not adjust the posterior but only the prior. Therefore, when computing the posterior we can simply consider the prior defined at the beginning $\pi(\bm_{\theta})\sim \mathcal{N}(\mu_{\theta}, \Sigma_{\theta})$ without worrying about adjusting it.

In addition, note that in \textcite{guApproximatedAdjustedFractional2018} approach posterior mean $\hat{\bm{\theta}}$ and posterior covariance $\hat{\Sigma_{\theta}}$ are not computed from the posterior draws but they are obtained using the maximum likelihood estimate and the inverse of the Fisher information matrix, respectively. This has the advantage of being faster (posterior draws are not required) but it may be not possible for some complex model. [TODO add reference? change something, cioè non si rispetta molto l'inferenza bayesiana]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Computing the Bayes Factor}

In the previous section we obtained the adjusted prior and the posterior of the vector of transformed parameters $\bm{\beta}$, respectively
\begin{equation}
\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(0, \Sigma_{\beta}) \ \ \text{and} \ \
Pr(\bm{\beta}|Y) \sim \mathcal{N}(\hat{\bm{\beta}}, \hat{\Sigma}_{\beta}).
\end{equation}

Given the parameter transformation from $\bm{\theta}$ to $\bm{\beta}$, we can rewrite the Formula~\ref{eq:BF-encompassing} of the Bayes Factor between $H_i$ and $H_u$ as follow,
\begin{equation}
BF_{iu} = \frac{Pr(\beta_I > 0 | \beta_E = 0, Y, H_u)}{\pi_{adj}(\beta_I > 0 | \beta_E =0, H_u)} \times \frac{Pr(\beta_E =0| Y, H_u)}{\pi_{adj}(\beta_E =0| H_u)},
\end{equation}
where $\beta_I$ and $\beta_E$ were obtained in Formula~\ref{eq:param-transf} and represent the inequality and equality constraints, respectively.

Now, thanks to (multivariate) normal distribution approximation, we can easily compute the required conditional probabilities and marginal densities required to calculate the Bayes Factor.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Marginal Density}

Marginal distribution of a subset of variables of a multivariate normal distribution is obtained simply discarding the variables to marginalize out. For example, given the adjusted prior of the psychological treatment example, $\pi_{adj}(\bm{\beta}) \sim \mathcal{N}(\bm{0}, \Sigma_{\beta})$ where
\begin{equation}
\bm{0} =
\begin{bmatrix}
0 \\ 0 \\ 0 \\ 0
\end{bmatrix} \ ~ \ \text{  and  }\ ~ \
\Sigma_{\beta} = \begin{bmatrix}
8 & -4 & 0 & 4 \\
-4 & 8 & 0 & 0 \\
0 & 0 & 8 & -4 \\
4 & 0 & -4 & 8
\end{bmatrix},
\end{equation}
the marginal distribution of the equality constraints $\bm{\beta}_E$ is
\begin{equation}
\begin{gathered}
\pi_{adj}(\bm{\beta}_E) \sim \mathcal{N}(\mu_{\beta_E}, \Sigma_{\beta_E}),\\~\\
\text{where  }\ \mu_{\beta_E} = \begin{bmatrix}
0 \\ 0
\end{bmatrix} \ \ \text{and  } \ \
\Sigma_{\beta_E} = \begin{bmatrix}
8 & -4 \\
-4 & 8
\end{bmatrix}.
\end{gathered}
\end{equation}

At this point computing the density at $\beta_E = 0$ is elementary. In R, this can be done using the  \texttt{dmvnorm()} function from the \texttt{mvtnorm} package \parencite{genzMvtnormMultivariateNormal2021}. To compute the \textbf{marginal prior density} $\pi_{adj}(\beta_E =0| H_u)$ of the psychological treatment example, we can use the following code:
\codespacing
<<marginal-density, echo = TRUE, cache=TRUE>>=
# Prior info
mu_prior <- c(0, 0, 0, 0)
Sigma_prior <- matrix(c( 8,-4, 0, 4,
                        -4, 8, 0, 0,
                         0, 0, 8,-4,
                         4, 0,-4, 8), ncol = 4, byrow = TRUE)

# Marginal prior density at beta_1 = 0 and beta_2 = 0
mvtnorm::dmvnorm(x = c(0, 0),
                 mean = mu_prior[1:2],
                 sigma = Sigma_prior[1:2, 1:2])

@
\doublespacing

Analogously, \textbf{marginal posterior density} $\pi_{adj}(\beta_E =0| Y, H_u)$ can be computed considering this time the estimated posterior mean vector $\bm{\hat{\beta}}$ and covariance matrix $\hat{\Sigma}_{\beta}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Conditional Probability}

To compute the conditional probability of the inequality constraints given the equality constraints in a multivariate normal distribution, we can use the \texttt{pcmvnorm()} function from the \texttt{condMVNorm} R-package \parencite{varadhanCondMVNormConditionalMultivariate2020}. Considering the psychological treatment example, to calculate the \textbf{conditional prior probability} $\pi_{adj}(\beta_I > 0 | \beta_E =0, H_u)$ we can run the following code:
\codespacing
<<conditional-prob, echo = TRUE, cache=TRUE>>=
# Conditional prior probability that beta_3 > 0 and beta_4 > 0
# given beta_1 = 0 and beta_2 = 0
condMVNorm::pcmvnorm(
    lower = c(0, 0), upper = c(Inf, Inf), # inequality constraints
    mean = mu_prior, sigma = Sigma_prior,
    dependent.ind = 3:4,                  # inequality variables
    given.ind = 1:2, X.given = c(0, 0))   # equality variables and constraints

@
\doublespacing

Analogously, \textbf{conditional posterior probability} $\pi_{adj}(\beta_I > 0 | \beta_E =0, Y, H_u)$ can be computed considering this time the estimated posterior mean vector $\bm{\hat{\beta}}$ and covariance matrix $\hat{\Sigma}_{\beta}$.

At this point we have all the elements required and we can easily compute the Bayes Factor $BF_{iu}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Advanced Elements}

In the previous section we have described in details all the steps and elements required to evaluate informative hypotheses using the Bayes Factor with encompassing prior approach. Before moving to an applied example, however, we consider some more advanced aspects that are important to take into account when applying this method. In particular, we discuss parameters standardization, range constraints, hypothesis matrix rank, comparable hypotheses, and bounded parameter space.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Parameters Standardization}
When evaluating informative it may be necessary to standardize the parameters of interest. As described by \textcite{guApproximatedAdjustedFractional2018}, parameters have to be standardized when comparing regression coefficient of continuous variables that are measured on different scales. In these cases, in fact, evaluating if $\theta_i>\theta_j$ would be affected by the scale of the two variables. Standardizing the parameters will allow to correctly compare the two parameters. On the contrary, standardizing the parameters is not necessary if they are compared to constants (i.e., $\theta > 3$) or if they represent groups means, such as the case of categorical variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Range Constraints}
It is possible to express range constraints such as $0<\theta_i<1$ in which the parameter is constrained between two values. These constraints can be expressed as the combination of two constraints. For example, the previous hypothesis is equivalent to $\beta_1 = \theta_i>0$ and $\beta_2 = 1 - \theta_i > 0$. However, adjusting the prior mean in the case of range constraints would require to center the prior on the middle of the range space \parencite{mulderBIEMSFortran902012}. Thus, in the previous example we would set $\mu_{\beta_1} = \mu_{\beta_2} = .5$. For a detailed description of prior specification for range constraints consider Appendix A in \textcite{guApproximatedAdjustedFractional2018}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Hypothesis Matrix Rank}\label{sec:rank-hypo}
In Equation~\ref{eq:normal-transf}, we presented how to obtain the distribution of the transformed parameters $\bm{\beta}$ from the distribution of the original parameters $\bm{\theta}$. This is simply a linear transformation, but, to obtain a proper covariance matrix $\Sigma_{\beta} = R\Sigma_{\theta}R^T$, the hypothesis matrix $R$ has to be \textit{full-row-rank} (i.e., all rows are linearly independent).

To overcome this limit, we can use the same solution adopted by \textcite{mulderBayesFactorsTesting2016, mulderSimpleBayesianTesting2019}. A matrix $R^*$ is defined selecting the maximum number of linearly independent rows of $R$ and it is used to compute the covariance matrix $\Sigma_{\beta} = R^*\Sigma_{\theta}R^{*T}$. This allows us to obtain the distribution of the parameters $\bm{\beta^*}$ that are linearly independent. The remaining constraints can be expressed as linear combinations of the parameters $\bm{\beta^*}$. Finally, to evaluate the probability that all constraints holds we can draw a large sample from the distribution of $\bm{\beta^*}$ and compute the proportion of draws satisfying all the constraints.

Let's see an example to clarify the procedure. Suppose we have the following hypothesis $H_i: 0 = \theta_1 < \{\theta_2, \theta_3\} < \theta_4$. Thus, the corresponding hypothesis matrix is
\begin{equation}
\begin{aligned}
R &= \begin{bmatrix}
 1 & 0 & 0 & 0\\
-1 & 1 & 0 & 0\\
-1 & 0 & 1 & 0\\
 0 &-1 & 0 & 1\\
 0 & 0 &-1 & 1\\
\end{bmatrix} \ ~ \ \text{  with  }
\end{aligned}
\qquad
\begin{aligned}
\beta_1 &= \theta_1\\
\beta_2 &= \theta_2 - \theta_1\\
\beta_3 &= \theta_3 - \theta_1\\
\beta_4 &= \theta_4 - \theta_2\\
\beta_5 &= \theta_4 - \theta_3\\
\end{aligned}
\end{equation}

Note that only the first four rows ($\beta_1,\beta_2,\beta_3,\beta_4$) are linearly independent. The fifth row, instead, can be obtained as a linear combination: $\beta_5 = \beta_4 - \beta_3 + \beta_2$. A this point, we can define $R^*$ as the first four rows of $R$ and use it to obtain the distribution of $\bm{\beta^*}$ given the original distribution of $\bm{\theta}$. To compute the probability of the inequality constraints, we need to draw a large sample from the multivariate normal distribution conditional to the equality constraints. To do that wee can use the \texttt{rcmvnorm()} function from the R-package \texttt{condMVNorm}.
\codespacing
<<example-rank, echo = TRUE, cache=TRUE>>=
# theta distribution
theta_mu <- c(0,0,0,0)
theta_sigma <- diag(4)*2

# Hypothesis matrix
R <- matrix(c(1, 0, 0, 0,
             -1, 1, 0, 0,
             -1, 0, 1, 0,
              0,-1, 0, 1,
              0, 0,-1, 1), ncol = 4, byrow = TRUE)
r <- c(0, 0, 0, 0, 0)

# beta distribution
beta_mu <- R[1:4,] %*% theta_mu - r[1:4]
beta_sigma <- R[1:4, ] %*% theta_sigma %*% t(R[1:4, ])

# Obtain draws
set.seed(2021)
obs <- condMVNorm::rcmvnorm(
  1e4, mean = beta_mu, sigma = beta_sigma,
  dependent.ind = 2:4,        # inequality variables
  given.ind = 1, X.given = 0) # equality variable and constraint

# Define linearly dependent constraints
colnames(obs) <- c("beta_2", "beta_3", "beta_4")
obs <- cbind(obs,
             "beta_5" = obs[, "beta_4"] - obs[, "beta_3"] + obs[, "beta_2"])

# Evaluate on each draw if all constraints hold
test <- rowSums(obs > 0) == 4

# Proportion
mean(test)
@
\doublespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Comparable Hypotheses}
To compute the Bayes Factor between different informative hypotheses, we can simply consider the ratio of the Bayes Factor of each hypotheses against the encompassing model:
\begin{equation}
BF_{ij} = \frac{BF_{iu}}{BF_{ju}}
\end{equation}
In order to do that, however, it is necessary for the informative hypotheses considered to be comparable \parencite{guApproximatedAdjustedFractional2018}. That is, the intersection of the constrained region of each hypothesis must be non-empty. Thus, a set of informative hypotheses $\{H_1,\ldots, H_i\}$ is comparable if exist at least one solution to
\begin{equation}
\begin{bmatrix}
R_{1E}\\
R_{1I}
\end{bmatrix} \theta = \begin{bmatrix}
r_{1E}\\
r_{1I}
\end{bmatrix}, \ldots,\begin{bmatrix}
R_{iE}\\
R_{iI}
\end{bmatrix} \theta = \begin{bmatrix}
r_{iE}\\
r_{iI}
\end{bmatrix}
\end{equation}
This will allow to select a common solution of $\bm{\theta}$  that can be used as the adjusted prior mean of the encompassing model. Note that to solve these equations, also inequality constraints are considered as equality constraints.

For example, $H_1: \theta_1 > \theta_2$ and $H_2: 0 = \theta_1 < \theta_2$ are comparable hypotheses because exists the common solution $\theta_1 = \theta_2 = 0$. On the country, $H_3: \theta_1 < \theta_2 +3 $ and $H_4: \theta_1 = \theta_2$ are not comparable hypotheses because there is no a common solution to $\theta_1 = \theta_2 +3 $ and $\theta_1 = \theta_2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Bounded Parameter Space}

The described approach using (multivariate) normal approximation can be straightforward applied in the case in which parameters of interest are unbounded. In case of bounded parameters (i.e., variances, correlations, or probabilities), however, appropriate prior distributions are required to take into account the parameters bounded nature.

Note that, if only inequality constraints are considered, the prior distribution is only used to measure the complexity of the inequality constrained hypotheses. In this case, the prior probability that a inequality constraint holds will be insensitive to the exact distributional form as long as the distribution is symmetrical and centered on the focal point. For example, the probability that a correlation coefficient $\rho_i$ is greater than zero is the same if we consider a  uniform distribution between -1 and 1 (taking into account parameter bounded nature) or if we consider a normal distribution centered on zero (without taking into account parameter bounded nature; see Figure~\ref{fig:plot-bounded-par}).

<<plot-bounded-par, fig.cap="The probability that $\\rho_i>0$ is the same under the uniform prior and the normal prior">>=
plot_bounded_par()
@

Thus, as suggested by \textcite{guBayesianEvaluationInequality2014}, as long as the complexity of the constrained hypotheses is the same regardless of a (multivariate) normal distribution or an appropriate bonded prior distribution being used, it is still possible to follow the described approach. This is true for hypotheses that belong to an equivalent set \parencite[for an appropriate definition of equivalent set see][]{guBayesianEvaluationInequality2014, hoijtinkInformativeHypothesesTheory2012}.

In the case of equality and inequality constraints, however, normal approximation is possible, based on the same rationale as before, only to compute the conditional prior probability. Instead, to estimate the marginal prior density, alternative solutions have to be adopted using appropriate prior distribution that takes into account the parameters bounded nature. See \textcite{mulderBayesFactorTesting2019} for an example of hypotheses evaluating correlation coefficients in generalized multivariate probit model.

Note that these issues are relevant only when considering the prior distribution. In the case of the posterior distribution, instead, we can safely approximate it to a (multivariate) normal distribution thanks to the large-sample theory. Moreover, prior specification is relevant only for the parameters of interest (i.e., the parameters involved in the constraints) as nuisance parameters are integrated out in the Bayes Factor. Thus, the results will be insensitive to the actual choice of the prior for the nuisance parameters as long as it is vague enough.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypotheses Testing in the Attachement Theory}

In this section, we propose a real case study example evaluating different informative hypotheses within the attachment theory. This will allow us to face the common issues found in real research applications. First, we provide required background information regarding the attachment theory and study aims and characteristics. Subsequently, all the steps involved in the evaluation of informative hypotheses using the Bayes Factor with encompassing prior approach are described. Finally, the results are briefly discussed.

All materials, data [?], and analysis code are available at TODO. The analyses report with further details is available online at TODO.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Background Information}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{The Attachamnt Theory}

The attachment theory originates from the pioneering work of \textcite{bowlbyAttachmentLoss1969} and \textcite{ainsworthAttachmentExplorationSeparation1970}. The main tenet of attachment theory is that the relationships with the caregivers that children develop in the early stages of their life (i.e., \textit{attachment bound}) will affect children social and emotional development \parencite{cassidyHandbookAttachmentTheory2016}. The attachment bound concerns particular aspects of a relationship that are related to emotional regulation, proximity and protection. Four main attachment styles have been recognized in the literature according to the different behavioral patterns and internal presentations:
\begin{itemize}
  \item{\textbf{Secure Attachment}} - children who are securely attached display optimal emotional regulation and they consider the caregiver as a secure base.
  \item{\textbf{Anxious Attachment}} - anxious children manifest high level of anxiety in stressful situations and their relationships with the caregivers is ambivalent displaying anger or helplessness.
  \item{\textbf{Avoidant Attachment}} - avoidant children mask distress in stressful situations displaying little emotions and their relationships with the caregivers is characterized by little involvement.
  \item{\textbf{Fearful Attachment}} - fearful children lack an adequate emotional regulation in stressful situation displaying disorganized behaviors.
\end{itemize}

The attachment theory is one of the main and most supported theory in psychology. In the literature, however, there is still an open debate on the relative role of mother and father attachment on children social-emotional development. Four different main theoretical perspectives have been identified \parencite{brethertonFathersAttachmentTheory2010}:
\begin{itemize}
  \item{\textbf{Monotropy Theory}} - only the principal attachment figure (usually the mother) has an impact on children development.
  \item{\textbf{Hierarchy Theory}} - the principal attachment figure (usually the mother) has a greater impact on children development than the subsidiary attachment figure (usually the father).
  \item{\textbf{Independence Theory}} - all attachment figures are equally important but they they affect differently the children development.
  \item{\textbf{Integration Theory}} - to understand the impact on children development is necessary to consider attachment relationships taken together.
\end{itemize}
Contrasting results have been reported by studies investigating which is the "\textit{correct}" theory. No study, however, have tried to properly evaluate the different theoretical by directly compare the different hypotheses.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Present Study}

The aim of the present study is to directly compare the four different theoretical perspectives regarding the role of father and mother attachment, using the Bayes Factor with the encompassing prior approach.

 ($n =\Sexpr{nrow(data_cluster)}$; \Sexpr{perc_females()}\% Females) Italian children between 8 and 12 years old (\textit{middle childhood}, third to sixth school grade) were included in the analysis,. More information about the sample and the cluster analysis can bee found at TODO



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{important notes}

[definition of prior]

Note that normal prior distribution can be used only for unbounded parameters. In case of bounded parameters (i.e., correlations or probabilities) adequate bounded prior are required. difference approx for prior and posterior and between density and conditional probability.

note on range constraints



note on standardize parameters

\section{Old Material}

[Problem of nested models?]
p-value do

statistical approach have been developed to evaluate hypothesis and by far one of the most applied tool is the Null Significance hypothesis

When conducting a study, researchers usually have expectetions about the phenomeno under investigation. They want to verify some kind of hypothesis.

translation of theoretical hypothesis into statistical models
these hypothesis can be moreover less conscious, verify if an effect exist but if interrogated in mor e depth evaluate
Informative hypothesis with equality or order constraints.

"Informative hypotheses are increasingly being used in psychological sciences because they adequately capture researchers’ theories and expectations; One of the objectives of psychological studies is to test hypotheses that represent scientific expectations. The main tool available for this purpose is null hypothesis significance testing where the goal is to falsify a null hypothesis of ‘no effect’. On the other hand, psychologists may expect; These expectations cannot be formulated by the traditional null hypothesis. Instead, such expectations can be translated to so-called informative hypotheses which assume a specific structure of the model parameters (Hoijtink, 2012)" \textcite{guApproximatedAdjustedFractional2018}

Traditional null hypothesis, statistical approaches has evolved to evaluate specific hypothesis,

"Many empirical researchers seek to evaluate and test hypothe- ses by comparing theoretical predictions to observed data. The dominant statistical vehicle for this activity is null hypothesis sig- nificance testing using p values. Despite their popularity, the liter- ature contains an intense and ongoing debate about the usefulness of p values for testing scientific expectations (e.g., Berger \& Sellke, 1987; Cohen, 1994; Edwards, Lindman, \& Savage, 1963; Hubbard \& Armstrong, 2006; Wagenmakers, 2007; Wainer, 1999, among many others).
One important critique of p values is that they cannot be used to quantify evidence in favor of the null hypothesis; a p value can only be used to falsify that null hypothesis; he p value does not allow one to discriminate absence of evidence (i.e., uninformative data) from evidence of absence (i.e., data supporting the null hypothesis; p values tend to overestimate the evidence against the null hypothesis; A final critique we mention here is that p values are limited regarding the types of hypotheses that can be tested. ; p values are of limited use for testing hypotheses with order constraints on the parameters of interest (Braeken, Mulder, \& Wood, 2015);
Bayes factors are computed by assessing the relative predictive adequacy of the hypotheses under consideration, as provided by the so-called marginal likelihood ; The goal of a null hypothesis significance test (NHST) on the other hand is to determine whether there is enough evidence in the data to reject the null hypothesis, while controlling the probability of incorrectly rejecting the null; In a Bayes factor test the outcome is the relative evidence in the data for H0 against H1, which lies on a continuous; the outcome of a NHST, as advocated by Neyman and Pearson, is a dichotomous decision; BF relative measure which balances between the plausibility of the null hypothesis and the alternative hypothesis where the prior under the alternative formalizes the anticipated effect if the null is not true.
" \parencite{mulderEditorsIntroductionSpecial2016}

"A core element of science is that data are used to argue for or against hypotheses or theories. ;these statistics are designed to make decisions, such as rejecting a hypothesis, rather than providing for a measure of ev- idence. ; In this paper, we explore a statistical notion that does allow for the desired interpretation as a measure of evidence: the Bayes factor (Good, 1979, 1985; Jeffreys, 1961; Kass \& Raftery, 1995).; appealing method for assessing the impact of data on the evaluation of hypotheses. Bayes factors present a useful and meaningful measure of evidence." \parencite{moreyPhilosophyBayesFactors2016}


different approaches model comparison: formalize models according to hypotheses or theoretical perspectives

- what are informative hypothesis
  - equal; grater; smaller; Inequality equality constraints
  - comes from theoretical prespective, previous studies, review
  - "An informative hypothesis consists of equality and/or inequality constraints among the parameters of interest in a statistical model." \textcite{guApproximatedAdjustedFractional2018}
  - "Informative hypotheses covers a much broader range of scientific expectations than the class of standard null hypotheses; In addition, by testing competing informative hypotheses directly against each other a researcher obtains a direct answer as to which scientific theory is most supported by the data." \textcite{guApproximatedAdjustedFractional2018}
  - evaluation of informative hypothesis frequentist vs bayesian  \textcite{guApproximatedAdjustedFractional2018}

- methods available
  - NHST - multiple testing (post hoc analyses)
  - Bayesian model comparison - information criteria
    - "in contrast to Bayesian model selection (BF) both the AIC and BIC are as yet unable to deal with hypotheses specified using inequality constraints." \parencite{vandeschootIntroductionBayesianModel2011}
  - BF encompassing prior
    - Gu, Mulder, Dekovic, and Hoijtink (2014) have shown how to evaluate inequality constrained hypothesis in general statistical models
    - the usefulness of the Bayes factor for testing hypotheses in psychological research has been highlighted in various studies in a special issue on the topic (Mulder \& Wagenmakers, 2016)
    - \textcite{guApproximatedAdjustedFractional2018} "This paper proposes an approximation of a fractional Bayes factor to extend its applicability to testing informative hypotheses for more general models. These models can be generalized linear (mixed) models (McCullogh \& Searle, 2001); Due to large-sample theory (Gelman, Carlin, Stern, \& Rubin, 2004, pp. 101–107), the posterior distribution of the parameters in each model can be approximated by a (multivariate) normal distribution. This paper also approximates the implicit fractional prior with a (multivariate) normal distribution as a general methodology to ensure a fast computation of the (adjusted) fractional Bayesian factor. "

"Of course, these hypotheses can be evaluated using classical null hypothesis testing, or one-sided hypothesis testing. However, when there are more groups, more variables, or more constraints, null hypothesis testing is not the appropriate tool see Van de Schoot, Hoijtink et al. (2011) for a detailed discussion. Moreover, p-values are fundamentally incompa- tible with measures of evidence."\parencite{vandeschootIntroductionBayesianModel2011}

"inequality constraints can provide a direct evaluation of the hypothesis of interest, in contrast to other, heuristic methods such as testing the equality of all three dosage condition parameters and then carrying out additional, post hoc analyses to determine directional differences." \parencite{heckMultinomialModelsLinear2019}

- formalization hypotheses:
  - declare expectation strong theories
  - "(a) translating a verbal theory into predicted patterns, (b) deriving algebraic implications of axioms or formal theories, and (c) brute force enumeration of all of the predictions made by the deterministic theory, typically under a set of theory-specific assumptions" \parencite{heckMultinomialModelsLinear2019}

- definition of informative hypothesis
  - \textcite{guApproximatedAdjustedFractional2018}

- Components Bayesian model selection\parencite{vandeschootIntroductionBayesianModel2011}:
  - Prior - Admissible parameter space
  - Likelihood
  - Marginal likelihood
  - Model fit / model complexity (size) "In sum, the marginal likelihood rewards a hypothesis with the correct (in)equality constraints. This is because the average likelihood value is higher when many small likelihood values are not taken into account. The smaller the parameter space, the less complex a model becomes. Therefore, the methodology combines model fit and model size of a hypothesis."
  - BF "Recall that Bayes factors provide a direct quantification of the support in the data for the constraints imposed on the means. With support we mean: the trade-off between model size and model fit."

A key difficulty in analyzing inequality-constrained models and theories is that it can quickly become difficult to characterize the resulting restricted parameter space (e.g., Davis- Stober, 2012; Fishburn, 1992) \parencite{heckMultinomialModelsLinear2019}

- previoous work
  - \textcite{vandeschootIntroductionBayesianModel2011} - simple general introduction Bayesian model selection for evaluating informative hypotheses. Easily overcomes problem of equality with region of equality. No code or clear step by step.
  - \textcite{heckMultinomialModelsLinear2019} - Multinomial Models with Linear Inequality Constraints. Complex mathematical psychology
  - \textcite{guApproximatedAdjustedFractional2018} - adjusted fractional BF to avoid prior specification; general approach for different model and eequality and inequalitty constraints.
  - \textcite{hoijtinkInformativeHypothesesTheory2012} - general book on testing informative hypothesis
  - \textcite{hoijtinkTutorialTestingHypotheses2019} - introduction-tutorial to Bayes factor, very descriptive no formulas, the part of informative hypothesis is restricted to ANOVA using bain package.
  -\textcite{mulderEditorsIntroductionSpecial2016} - introduction to issue about BF.
  \textcite{moreyPhilosophyBayesFactors2016} - phylosophycal fundation of the BF.
  -\textcite{katoBayesianApproachInequality2006} - inequality constraints in mixed effect model
  - not god for wide audience, complex notation athough valuable makes complex to understand.



- extra
  - combination with threshoolds $\theta_1 - \theta_2 < k$
  - different approach with range constraints $|\theta_1 - \theta_2| < k$
  - "the parameters of interest may need to be standardized in some situations. The need for standardization depends on the statistical model and informative hypothesis under evaluation.;  it is undesirable to standardize the parameters h if they represent means." \textcite{guApproximatedAdjustedFractional2018}
  - issue of consistency
  - range constrain issue mean prior see Gu 2018
  - multiverse analysis

\Textcite{andrewBayesianDataAnalysis} said this,
too \parencite{andrewBayesianDataAnalysis}.  Further evidence comes from
other sources \parencite{andrewBayesianDataAnalysis}.  \lipsum[3]

\section{Method}
\subsection{Participants}

\subsection{Materials}

\subsection{Design}

\subsection{Procedure}

\subsubsection{Instrument \#1}

\paragraph{Reliability}

\subparagraph{Inter-rater reliability}

\subparagraph{Test-retest reliability}

\paragraph{Validity}

\subparagraph{Face validity}

\subparagraph{Construct validity}

\section{Results}
Table~\ref{tab:BasicTable} summarizes the data. \lipsum[15]

<<>>=
plot(rnorm(19))
@

<<>>=
kable(data.frame(x=1:3, y = letters[1:3]), booktabs = T) %>%
  kable_styling(latex_options = "striped")
@

\begin{table}
  \caption{Sample Basic Table}
  \label{tab:BasicTable}
  \begin{tabular}{@{}llr@{}}         \toprule
  \multicolumn{2}{c}{Item}        \\ \cmidrule(r){1-2}
  Animal    & Description & Price \\ \midrule
  Gnat      & per gram    & 13.65 \\
            & each        &  0.01 \\
  Gnu       & stuffed     & 92.50 \\
  Emu       & stuffed     & 33.33 \\
  Armadillo & frozen      &  8.99 \\ \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
    \caption{This is my first figure caption.}
    \includegraphics[bb=0in 0in 2.5in 2.5in, height=2.5in, width=2.5in]{Figure1.pdf}
    \label{fig:Figure1}
\end{figure}

Figure~\ref{fig:Figure1} shows this trend. \lipsum[16]

\section{Discussion}

\printbibliography

\appendix

\section{Instrument}
\label{app:instrument}

As shown in Figure~\ref{fig:Figure2}, these results are impressive. \lipsum[20]

\begin{figure}
    \caption{This is my second figure caption.}
    \includegraphics[bb=0in 0in 2.5in 2.5in, height=2.5in, width=2.5in]{Figure1.pdf}
    \label{fig:Figure2}
\end{figure}

\lipsum[21]
\section{Pilot Data}
\label{app:surveydata}

The detailed results are shown in Table~\ref{tab:DeckedTable}. \lipsum[22]

\begin{table}
  \begin{threeparttable}
    \caption{A More Complex Decked Table}
    \label{tab:DeckedTable}
    \begin{tabular}{@{}lrrr@{}}         \toprule
    Distribution type  & \multicolumn{2}{l}{Percentage of} & Total number   \\
                       & \multicolumn{2}{l}{targets with}  & of trials per  \\
                       & \multicolumn{2}{l}{segment in}    & participant    \\ \cmidrule(r){2-3}
                                    &  Onset  &  Coda            &          \\ \midrule
    Categorical -- onset\tabfnm{a}  &    100  &     0            &  196     \\
    Probabilistic                   &     80  &    20\tabfnm{*}  &  200     \\
    Categorical -- coda\tabfnm{b}   &      0  &   100\tabfnm{*}  &  196     \\ \midrule
    \end{tabular}
    \begin{tablenotes}[para,flushleft]
        {\small
            \textit{Note.} All data are approximate.

            \tabfnt{a}Categorical may be onset.
            \tabfnt{b}Categorical may also be coda.

            \tabfnt{*}\textit{p} < .05.
            \tabfnt{**}\textit{p} < .01.
         }
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\lipsum[23]

\end{document}

%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% This work is "maintained" (as per LPPL maintenance status) by
%% Daniel A. Weiss.
%%
%% This work consists of the file  apa7.dtx
%% and the derived files           apa7.ins,
%%                                 apa7.cls,
%%                                 apa7.pdf,
%%                                 README,
%%                                 APA7american.txt,
%%                                 APA7british.txt,
%%                                 APA7dutch.txt,
%%                                 APA7english.txt,
%%                                 APA7german.txt,
%%                                 APA7ngerman.txt,
%%                                 APA7greek.txt,
%%                                 APA7czech.txt,
%%                                 APA7turkish.txt,
%%                                 APA7endfloat.cfg,
%%                                 Figure1.pdf,
%%                                 shortsample.tex,
%%                                 longsample.tex, and
%%                                 bibliography.bib.
%%
%%
%% End of file `./samples/longsample.tex'.

<<biber>>=
system(paste("biber", sub("\\.Rnw$", "", current_input())))
@

