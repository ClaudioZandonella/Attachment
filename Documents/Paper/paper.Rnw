%%
%% This is file `./samples/longsample.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% apa7.dtx  (with options: `longsample')
%% ----------------------------------------------------------------------
%%
%% apa7 - A LaTeX class for formatting documents in compliance with the
%% American Psychological Association's Publication Manual, 7th edition
%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% ----------------------------------------------------------------------
%%
\documentclass[man, floatsintext]{apa7}

\usepackage{lipsum}

\usepackage[american]{babel}

% KabelExtra packages
\usepackage{booktabs} %
\usepackage{longtable} %
\usepackage{array} %
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float} %
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\usepackage{amsmath}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{../Biblio-attachment.bib}

\title{Evaluating Informative Hypotheses With Equality and Inequality Constraint: The Bayes Factor Encompassing Prior Approach}
\shorttitle{Evaluating Informative Hypotheses With Equality and Inequality Constraint: The Bayes Factor Encompassing Prior Approach}

\author{Claudio Zandonella Callegher, Tatiana Marci, Pietro De Carli, Gianmarco Altoè}
\affiliation{UNIPD}

\leftheader{loook}

\abstract{\lipsum[1]}

\keywords{keyword-1, keyword-2}

\authornote{
   \addORCIDlink{Claudio Zandonella Callegher}{0000-0000-0000-0000}

  Correspondence concerning this article should be addressed to Claudio Zandonella Callegher, Department of Developmental Psychology and Socialisation, University of Padua, Via Venezia, 8 - 35131 Padova, Italy.  E-mail: claudiozandonella@gmail.com}

<<settings, echo=FALSE>>=
library(kableExtra)
@

\begin{document}
\maketitle

\section{Introduction}

One of the principal aspects of empirical research is the evaluation of research or theoretical hypothesis. When conducting a study, researcher usually have expectations based on hypotheses or theoretical perspectives they want to evaluate according to observed data.

For example researcher could be interested in evaluating whether the treatment group scores higher than the control group.

they want to evaluate them according to / if they are supported by the data; collecting evidence.

In psychology the most often use tool (the dominant statistical approach) to evaluate research hypotheses (if an hypothesis is supported by the data), is the Null hypothesis Significance Test (NHST). Debate in the literature about the utility of NHST (p-value) in hypothesis testing. this approach, however, present several limitation. First, data are evaluated against the catch all null-hypothesis that "nothing is going on" (Cohen, 1994; Royall, 1997, pp. 79–81). This is rarely the actual hypothesis of interest of the researcher who usually have some specific expectation about the phenomenon under investigation. Second, the NHST do not formalize the alternative hypothesis thus in the NHST is not possible to "accept" an hypothesis but only reject the Null-Hypothesi (the p-value do not quantify the evidence in favor of an hypothesis). This is inconvenient to the researcher as in case of not significant result is left in a state of indecision. Third limited type of hypothesis that can be evaluated, no include expectation in the form of parameter order constraints. [Expectation can not be formulated - you can use one-side test however, when there are more groups, more variables, or more constraints, null hypothesis testing is not the appropriate tool see Van de Schoot, Hoijtink et al. (2011) for a detailed discussion.\parencite{vandeschootIntroductionBayesianModel2011}]

[classical significance tests are only suitable for testing a null hypothesis against a single alternative, and unsuitable for testing multiple hypotheses with equality as well as order constraints (Silvapulle \& Sen, 2004). Second, traditional model comparison tools (e.g., the AIC, BIC, or CFI) are generally not suitable for evaluating models (or hypotheses) with order constraints on certain parameters (Mulder et al. 2009; Braeken, Mulder, \& Wood, 2015). \textcite{mulderSimpleBayesianTesting2019}]

[broad class of hypotheses]

In general, researchers are interested in comparing different hypothesis and understand which is the most supported by the data. Model comparison is a different approach that allows to compare different models. Models formalized according to researchers expectation or theoretical perspectives are compared against each other according to the data. In this way, it is possible to evaluate the relative plausibility of each model given the data and the set of model considered. Models can be compared using Information criteria (the most popular are the AIC and BIC) and the relative plausibility of each model can be computed. [Information criteria allow to evaluate the ability of the model to predict new data penalizing for model complexity.] Model comparison has several advantages and in particular it allows to directly compute the relative plausibility of each hypothesis. Model comparison, however, is still limited in the possibility to formalize hypothesis that include complex parameter order constraints.

An alternative approach to evaluate research hypotheses is based on the Bayes Factor. In the last 25 years, there has been an increasing interest / growth in popularity in the use of Bayes Factor and its adoption has been proposed (advocated) as the solution to the critical issues of the NHST. Although the Bayes Factor has its own limitations and  does not solve the fundamental issues of the misuse of statistical techniques \url{https://statmodeling.stat.columbia.edu/2019/09/10/i-hate-bayes-factors-when-theyre-used-for-null-hypothesis-significance-testing/}, it offers some clear advantages. In particular, the Bayes Factor allows to compare hypotheses obtaining a relative index of evidence, like in the model comparison approach, but it also allows to easily compare researchers' expectations. In fact, informative hypotheses can be formalized according to researchers expectation using equality and inequality constraints and than compared.

The evaluation of informative hypothesis based on the Bayes Factor has received increasing attention and several contribution can be found in the literature. \textcite{vandeschootIntroductionBayesianModel2011} presented a general introduction to informative hypotheses testing using the Bays Factor, whereas \textcite{hoijtinkInformativeHypothesesTheory2012} offered more detailed description on the development of this approach. Other studies, instead, considered the application of this method in specific models; for example: mixed effect models \parencite{katoBayesianApproachInequality2006}, evaluation of correlation coefficients \parencite{mulderBayesFactorsTesting2016} and multinomial models \parencite{heckMultinomialModelsLinear2019}. In addition, \textcite{guBayesianEvaluationInequality2014} proposed an approximate procedure to evaluate as long as  the structural parameters are unbounded.

Note, however, that all previously mentioned studies considered informative hypothesis only with inequality constraints. Equality constraints could not be defined together with inequality constraints in the same hypothesis but they had to be approximated as "about equality constraints" (i.e., a equality constraints of type $\theta_i = \theta_j$ is approximated as $|\theta_i - \theta_j| < \xi$ for a value of $\xi$ small enough {TODO more below?}).

It's only more recently that \textcite{guApproximatedAdjustedFractional2018} introduced an approximate procedure to evaluate both equality and inequality constraints using the Bayes Factor in the case of general statistical models (i.e., generalized linear mixed models and structural equation models). Subsequentley, this approach was exteended  by \textcite{mulderBayesFactorTesting2019} to the generalized multivariate probit models. Finally, \textcite{mulderSimpleBayesianTesting2019} proposed an accurate procedure (i.e., not based on an approximation) that allows to test informative hypothesis with equality and inequality constraints in linear regression models.

The development and implementation of this approach are of particular interest because, although with its limits (discussed latrer), it allows researchers to directly evaluate their expectations and hypotheses. Available literature, however, it is mainly based on articles published in specialize journals of mathematical psychology. The complexity of these articles makes it difficult for non-experts in this approach to clearly understand all the steps involved. On the other hand, articles offering a general introduction to the Bayes Factor do not provide enough details to allow readers to apply autonomously the method to their problems, but they usually relay on ad-hoc solutions implemented in some statistical software.

The aim of this paper is to offer a clear e detailed description of the entire procedure. In particular we refer to the approximated method proposed by \textcite{guApproximatedAdjustedFractional2018} as it is applicable to a wider range of conditions than the more accurate approach proposed by \textcite{mulderSimpleBayesianTesting2019}. The paper is organized as follow. First the method is introduced considering a toy example. This allow to provide a  detailed description of all steps and elements involved in the formalization of informative hypothesis and Bayes Factor computation. Subsequently an application of the method to real data is presented to offer the opportunity to discuss the typical issues encountered in real complex scenario.

[Code provided in the SM]

\section{Bays Factor for Informative Hypothesis Testing}

The evaluation of informative hypothesis with equality and/or inequality constraints involves several steps and elements. In this section, first we describe how to formulate informative hypothesis. Subsequently, we introduce the Bayes Factor considering the encompassing prior approach based on the approximated method proposed by \textcite{guApproximatedAdjustedFractional2018}.

[In order to facilitate the understanding, a toy example is considered.]

\subsection{Formulation of Informative Hypothesis}

Informative hypotheses can be defined according to researchers expectation, evidence form the literature or theoretical perspectives and they are formed by equality and/or inequality constraints on certain model parameters. These constraints are obtain as a linear combination of certain parameters and eventual constant values. For example, it is possible to state that two parameters are equal ($\theta_i = \theta_j$), one parameter is grater than another ($\theta_i > \theta_j$), the difference between two parameter is less than a given value ($\theta_i - \theta_j < .5$) or express other complex conditions ($2\times\theta_i - \theta_j < .1 \times \sigma$).

Thus, an informative hypothesis $H_i$ with equality and inequality constraints can be expressed in the form
\begin{equation}
H_i:\ \ R_E\theta = r_E \ ~ \ \& \ ~ \ R_I\theta > r_I,
\end{equation}
where $R_E$ is a matrix expressing the equality constraints and $r_E$ is a vector containing the constants of the equality constraints. Whereas, $R_I$ is a matrix expressing the inequality constraints and $r_E$ is a vector containing the constants of the inequality constraints. Finally, $\theta$ is a vector of the model parameters involved in the constraints.

As an example, consider a study evaluating the efficacy of a new psychological treatment that is supposed to improve a given cognitive ability. In the study, a control group receiving no treatment and another group receiving the traditional treatment were included as comparison. Moreover, imagine that the new psychological treatment was administrated in three different modalities to different groups: individually, in group, online. Researchers expect no differences between the three modalities but they assume that the new treatment to perform better than the traditional one that in turn is better than the control condition. We can express this hypothesis as
\begin{equation}
H_i:\ \ \theta_{control}< \theta_{traditional} < \theta_{individual} = \theta_{group} = \theta_{online},
\end{equation}
were $\theta_{control}$ is the average score of the control group, $\theta_{traditional}$ is the average score of the group receiving the traditional treatment, and $\theta_{individual}$, $\theta_{group}$, and $\theta_{online}$ are the average scores of the groups receiving the new treatment individually, in group, and on-line, respectively. The correspondig formulation of the hypothesis using matrix notation introduced before is
\begin{equation}
\begin{aligned}
H_i: &\\
& R_E\theta =
\begin{bmatrix}
0 & 0 & 1 & -1 & 0\\
0 & 0 & 0 & 1 & -1
\end{bmatrix}	\begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} = \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_E,\\~\\
& R_I\theta =
\begin{bmatrix}
-1 & 1 & 0 & 0 & 0\\
0 & -1 & 1 & 0 & 0
\end{bmatrix}	\begin{bmatrix} \theta_{control} \\ \theta_{traditional} \\ \theta_{individual} \\ \theta_{group} \\ \theta_{online}\end{bmatrix} > \begin{bmatrix} 0 \\ 0\end{bmatrix} = r_I.
\end{aligned}
\end{equation}
Note how each row of $R_E$ and $R_I$ matrices expresses an equality or an inequality constraint, respectively. For example, in the first row of $R_E$ we have $\beta_{individual} - \beta_{group} = 0$ (i.e., $\beta_{individual} = \beta_{group}$) and in the first row of $R_I$ we have $\beta_{traditional} - \beta_{control} > 0$ (i.e., $\beta_{traditional} > \beta_{control}$).

Now that we have understood how to define an informative hypothesis with equality and inequality constraints introducing an appropriate notation, lets see how to evaluate an informative hypothesis using the Bayes Factor.

\subsection{Bayes Factor}

The Bayes Factor of hypothesis $H_1$ against a competing hypothesis $H_2$ is defined as the ration between the marginal likelihoods of the two hypothesis:
\begin{equation}
BF_{12} = \frac{Pr(Y|H_1)}{Pr(Y|H_2)} = \frac{\int l(Y|\theta_1, H_1) \pi(\theta_1| H_1)\,d\theta_1}{\int l(Y|\theta_2, H_2) \pi(\theta_2| H_2)\,d\theta_2},
\end{equation}
where $Y$ indicates the data, $\theta_i$ is the vector of parameters, $l(Y|\theta_i, , H_i)$ is the likelihood function, and $\pi{\theta_i| H_i}$ is the prior of the parameters under the hypothesis $H_i$. The marginal likelihood $Pr(Y|H_i)$ is a measure of the plausibility of the data under $H_i$.

Therefore, the Bayes Factor $BF_{12}$ quantifies the relative support of the data for the two competing hypotheses (and not the ratio between the probability of the two hypotheses). For values close to 1, the Bayes Factor indicates that $H_1$ and $H_2$ have similar support from the data. Larger values indicate evidence in favor of $H_1$, whereas values close to 0 indicate evidence in favor of $H_2$. The ratio between the posterior probabilities of the two hypotheses can be computed as
\begin{equation}
\frac{Pr(H_1|Y)}{Pr(H_2|Y)} = \overbrace{\frac{Pr(Y|H_1)}{Pr(Y|H_2)}}^{BF_{12}} \times \frac{Pr(H_1)}{Pr(H_2)},
\end{equation}
where $Pr(H_i)$ is th prior probability of $H_i$ and $Pr(H_i|Y)$ is the posterior probability of $H_i$.For a detailed description of the Bayes Factor considering also its interpretation and application in different context see \textcite{wagenmakersBayesianHypothesisTesting2010, mulderEditorsIntroductionSpecial2016, heckReviewApplicationsBayes2020}.

Note that to compute the marginal likelihood $Pr(Y|H_i)$ of an hypothesis $H_i$ it is necessary to integrate the product between the likelihood and the prior. These integrals, however, are usually difficult to compute [in particular in the case of hypothesis with order constraints]. In the case of hypothesis with equality and inequality constraints, however, it is possible to adopt the encompassing prior approach.

\subsubsection{Encompassing Prior Approach}

The basic idea of the encompassing prior approach is to consider an informative hypothesis as a subset of the parameter space of an unconstrained model. Thus, to evaluate the plausibility of an hypothesis we can consider the proportion of parameters of the unconstrained model that satisfy the constraints. More specifically, given an informative hypothesis $H_i$ and an unconstrained model $H_u$ (or \textit{encompassing model}) that does not contains any constraints on the parameters, if the prior under $H_i$ is defined as a truncation of the proper prior under $H_u$ (\textit{encompassing prior}) according to the constraints, then the Bayes Factor between $H_i$ and $H_u$ can be written as
\begin{equation}
\begin{aligned}
BF_{iu} &= \frac{Pr(\text{Inequality Const|Equality Const, Data, } H_u)}{\pi(\text{Inequality Const|Equality Const, } H_u)} \times \frac{Pr(\text{Equality Const|Data, } H_u)}{\pi(\text{Equality Const}|H_u)} \\ ~\\
&= \frac{Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)}{\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)} \times \frac{Pr(R_E\theta =r_E| Y, H_u)}{\pi(R_E\theta =r_E| H_u)}.
\end{aligned}
\end{equation}

The first term is the ratio between the conditional posterior probability and conditional prior probability that the inequality constraints hold under the unconstrained model $H_u$ given the equality constraints. The second term is the ratio between marginal posterior density and the marginal prior density of the equality constraints under $H_u$, the well-known Savage–Dickey density ratio \parencite{dickeyWeightedLikelihoodRatio1971, wetzelsEncompassingPriorGeneralization2010}. In particular, the four elements can be interpreted as:
\begin{itemize}
  \item{The \textbf{conditional posterior probability} $Pr(R_I\theta > r_I | R_E\theta =r_E, Y, H_u)$ is a measure of the fit of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{conditional prior probability} $\pi(R_I\theta > r_I | R_E\theta =r_E, H_u)$ is a measure of the complexity of the inequality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal posterior density} $Pr(R_E\theta =r_E| Y, H_u)$ is a measure of the fit of the equality constraints of $H_i$ under $H_u$.}
  \item{The \textbf{marginal prior density} $\pi(R_E\theta =r_E| H_u)$ is a measure of the complexity of the equality constraints of $H_i$ under $H_u$.}
\end{itemize}
Summarizing at the denominators we have measures of the \textbf{complexity} of the inequality and equality constraints of the informative hypothesis $H_i$; at the numerators, instead, we have measures of th \textbf{fit} of the data to the inequality and equality constraints of the informative hypothesis $H_i$. Thus, if the hypothesis $H_i$, although applying constraints to the parametric space (less complexity), is still able to provide a good description of the data, the Bayes Factor will favor $H_i$. On the contrary, if the support of by the data is poor, the Bayes Factor will favor the unconstrained model $H_u$.

The proof of the formulation of the Bayes Factor following the encompassing prior approach and the evaluation of its consistency (i.e., the probability of selecting the correct hypothesis goes to 1 for the sample size going to infinity) is provided in \textcite{guApproximatedAdjustedFractional2018, mulderBayesFactorTesting2019}. Note that slightly different notation is used here to enhance comprehension and underline that at the numerators we have values computed from the posterior of $H_u$, whereas at the denominators we have values computed from the prior of $H_u$.

To compute the Bayes Factor following the encompassing prior approach, only the prior and the posterior of the unconstrained model are required. In order to do that, we first need to define the encompassing prior for the unconstrained model.

\subsubsection{Definition of Encompassing Prior}

Prior specification is an important element as the resulting value of the Bayes Factor is affected by the prior choice. This aspects is particularly relevant in the case of equality constraints \parencite{lindleyStatisticalParadox1957, bartlettCommentLindleyStatistical1957}, whereas inequality constraints are not affected as long as the prior is symmetric and centered to the focal point of interest (this aspect will be further discussed in the next section).

In order to avoid arbitrary specification of the prior, different methods have been proposed in the literature. For example: Jeffreys-Zellner-Siow (JZS) objective priors do not require subjective specification \parencite{jeffreysTheoryProbability1961,zellnerPosteriorOddsRatios1980, bayarriExtendingConventionalPriors2007}; partial Bayes Factor \parencite{desantisMethodsDefaultRobust1999} uses part of the data to define the prior wheres the other part is used to compute the Bayes Factor; intrinsic Bayes Factor \parencite{bergerIntrinsicBayesFactor1996} and fractional Bayes Factor \parencite{ohaganFractionalBayesFactors1995} are a variation of the partial Bayes Factor where priors are defined according to the average of all possible minimal subsets of the data or to a given small fraction of the data, respectively.

Both, \textcite{guApproximatedAdjustedFractional2018, mulderSimpleBayesianTesting2019} based their approach on the fractional Bayes Factor. Starting from non-informative prior, a minimal fraction of the data is used to obtain a proper updated prior (have you ever heard Lindley's \citeyear{lindleyBayesianStatisticsReview1972} famous quote \textit{“Today’s posterior is tomorrow’s prior”}?) and the remaining part of the data is used to compute the Bayes Factor.

\paragraph{Prior Adjustment}

[definition of prior]

\section{Old Material}

[Problem of nested models?]
p-value do

statistical approach have been developed to evaluate hypothesis and by far one of the most applied tool is the Null Significance hypothesis

When conducting a study, researchers usually have expectetions about the phenomeno under investigation. They want to verify some kind of hypothesis.

translation of theoretical hypothesis into statistical models
these hypothesis can be moreover less conscious, verify if an effect exist but if interrogated in mor e depth evaluate
Informative hypothesis with equality or order constraints.

"Informative hypotheses are increasingly being used in psychological sciences because they adequately capture researchers’ theories and expectations; One of the objectives of psychological studies is to test hypotheses that represent scientific expectations. The main tool available for this purpose is null hypothesis significance testing where the goal is to falsify a null hypothesis of ‘no effect’. On the other hand, psychologists may expect; These expectations cannot be formulated by the traditional null hypothesis. Instead, such expectations can be translated to so-called informative hypotheses which assume a specific structure of the model parameters (Hoijtink, 2012)" \textcite{guApproximatedAdjustedFractional2018}

Traditional null hypothesis, statistical approaches has evolved to evaluate specific hypothesis,

"Many empirical researchers seek to evaluate and test hypothe- ses by comparing theoretical predictions to observed data. The dominant statistical vehicle for this activity is null hypothesis sig- nificance testing using p values. Despite their popularity, the liter- ature contains an intense and ongoing debate about the usefulness of p values for testing scientific expectations (e.g., Berger \& Sellke, 1987; Cohen, 1994; Edwards, Lindman, \& Savage, 1963; Hubbard \& Armstrong, 2006; Wagenmakers, 2007; Wainer, 1999, among many others).
One important critique of p values is that they cannot be used to quantify evidence in favor of the null hypothesis; a p value can only be used to falsify that null hypothesis; he p value does not allow one to discriminate absence of evidence (i.e., uninformative data) from evidence of absence (i.e., data supporting the null hypothesis; p values tend to overestimate the evidence against the null hypothesis; A final critique we mention here is that p values are limited regarding the types of hypotheses that can be tested. ; p values are of limited use for testing hypotheses with order constraints on the parameters of interest (Braeken, Mulder, \& Wood, 2015);
Bayes factors are computed by assessing the relative predictive adequacy of the hypotheses under consideration, as provided by the so-called marginal likelihood ; The goal of a null hypothesis significance test (NHST) on the other hand is to determine whether there is enough evidence in the data to reject the null hypothesis, while controlling the probability of incorrectly rejecting the null; In a Bayes factor test the outcome is the relative evidence in the data for H0 against H1, which lies on a continuous; the outcome of a NHST, as advocated by Neyman and Pearson, is a dichotomous decision; BF relative measure which balances between the plausibility of the null hypothesis and the alternative hypothesis where the prior under the alternative formalizes the anticipated effect if the null is not true.
" \parencite{mulderEditorsIntroductionSpecial2016}

"A core element of science is that data are used to argue for or against hypotheses or theories. ;these statistics are designed to make decisions, such as rejecting a hypothesis, rather than providing for a measure of ev- idence. ; In this paper, we explore a statistical notion that does allow for the desired interpretation as a measure of evidence: the Bayes factor (Good, 1979, 1985; Jeffreys, 1961; Kass \& Raftery, 1995).; appealing method for assessing the impact of data on the evaluation of hypotheses. Bayes factors present a useful and meaningful measure of evidence." \parencite{moreyPhilosophyBayesFactors2016}


different approaches model comparison: formalize models according to hypotheses or theoretical perspectives

- what are informative hypothesis
  - equal; grater; smaller; Inequality equality constraints
  - comes from theoretical prespective, previous studies, review
  - "An informative hypothesis consists of equality and/or inequality constraints among the parameters of interest in a statistical model." \textcite{guApproximatedAdjustedFractional2018}
  - "Informative hypotheses covers a much broader range of scientific expectations than the class of standard null hypotheses; In addition, by testing competing informative hypotheses directly against each other a researcher obtains a direct answer as to which scientific theory is most supported by the data." \textcite{guApproximatedAdjustedFractional2018}
  - evaluation of informative hypothesis frequentist vs bayesian  \textcite{guApproximatedAdjustedFractional2018}

- methods available
  - NHST - multiple testing (post hoc analyses)
  - Bayesian model comparison - information criteria
    - "in contrast to Bayesian model selection (BF) both the AIC and BIC are as yet unable to deal with hypotheses specified using inequality constraints." \parencite{vandeschootIntroductionBayesianModel2011}
  - BF encompassing prior
    - Gu, Mulder, Dekovic, and Hoijtink (2014) have shown how to evaluate inequality constrained hypothesis in general statistical models
    - the usefulness of the Bayes factor for testing hypotheses in psychological research has been highlighted in various studies in a special issue on the topic (Mulder \& Wagenmakers, 2016)
    - \textcite{guApproximatedAdjustedFractional2018} "This paper proposes an approximation of a fractional Bayes factor to extend its applicability to testing informative hypotheses for more general models. These models can be generalized linear (mixed) models (McCullogh \& Searle, 2001); Due to large-sample theory (Gelman, Carlin, Stern, \& Rubin, 2004, pp. 101–107), the posterior distribution of the parameters in each model can be approximated by a (multivariate) normal distribution. This paper also approximates the implicit fractional prior with a (multivariate) normal distribution as a general methodology to ensure a fast computation of the (adjusted) fractional Bayesian factor. "

"Of course, these hypotheses can be evaluated using classical null hypothesis testing, or one-sided hypothesis testing. However, when there are more groups, more variables, or more constraints, null hypothesis testing is not the appropriate tool see Van de Schoot, Hoijtink et al. (2011) for a detailed discussion. Moreover, p-values are fundamentally incompa- tible with measures of evidence."\parencite{vandeschootIntroductionBayesianModel2011}

"inequality constraints can provide a direct evaluation of the hypothesis of interest, in contrast to other, heuristic methods such as testing the equality of all three dosage condition parameters and then carrying out additional, post hoc analyses to determine directional differences." \parencite{heckMultinomialModelsLinear2019}

- formalization hypotheses:
  - declare expectation strong theories
  - "(a) translating a verbal theory into predicted patterns, (b) deriving algebraic implications of axioms or formal theories, and (c) brute force enumeration of all of the predictions made by the deterministic theory, typically under a set of theory-specific assumptions" \parencite{heckMultinomialModelsLinear2019}

- definition of informative hypothesis
  - \textcite{guApproximatedAdjustedFractional2018}

- Components Bayesian model selection\parencite{vandeschootIntroductionBayesianModel2011}:
  - Prior - Admissible parameter space
  - Likelihood
  - Marginal likelihood
  - Model fit / model complexity (size) "In sum, the marginal likelihood rewards a hypothesis with the correct (in)equality constraints. This is because the average likelihood value is higher when many small likelihood values are not taken into account. The smaller the parameter space, the less complex a model becomes. Therefore, the methodology combines model fit and model size of a hypothesis."
  - BF "Recall that Bayes factors provide a direct quantification of the support in the data for the constraints imposed on the means. With support we mean: the trade-off between model size and model fit."

A key difficulty in analyzing inequality-constrained models and theories is that it can quickly become difficult to characterize the resulting restricted parameter space (e.g., Davis- Stober, 2012; Fishburn, 1992) \parencite{heckMultinomialModelsLinear2019}

- previoous work
  - \textcite{vandeschootIntroductionBayesianModel2011} - simple general introduction Bayesian model selection for evaluating informative hypotheses. Easily overcomes problem of equality with region of equality. No code or clear step by step.
  - \textcite{heckMultinomialModelsLinear2019} - Multinomial Models with Linear Inequality Constraints. Complex mathematical psychology
  - \textcite{guApproximatedAdjustedFractional2018} - adjusted fractional BF to avoid prior specification; general approach for different model and eequality and inequalitty constraints.
  - \textcite{hoijtinkInformativeHypothesesTheory2012} - general book on testing informative hypothesis
  - \textcite{hoijtinkTutorialTestingHypotheses2019} - introduction-tutorial to Bayes factor, very descriptive no formulas, the part of informative hypothesis is restricted to ANOVA using bain package.
  -\textcite{mulderEditorsIntroductionSpecial2016} - introduction to issue about BF.
  \textcite{moreyPhilosophyBayesFactors2016} - phylosophycal fundation of the BF.
  -\textcite{katoBayesianApproachInequality2006} - inequality constraints in mixed effect model
  - not god for wide audience, complex notation athough valuable makes complex to understand.



- extra
  - combination with threshoolds $\theta_1 - \theta_2 < k$
  - different approach with range constraints $|\theta_1 - \theta_2| < k$
  - "the parameters of interest may need to be standardized in some situations. The need for standardization depends on the statistical model and informative hypothesis under evaluation.;  it is undesirable to standardize the parameters h if they represent means." \textcite{guApproximatedAdjustedFractional2018}
  - issue of consistency
  - range constrain issue mean prior see Gu 2018
  - multiverse analysis

\Textcite{andrewBayesianDataAnalysis} said this,
too \parencite{andrewBayesianDataAnalysis}.  Further evidence comes from
other sources \parencite{andrewBayesianDataAnalysis}.  \lipsum[3]

\section{Method}
\subsection{Participants}

\subsection{Materials}

\subsection{Design}

\subsection{Procedure}

\subsubsection{Instrument \#1}

\paragraph{Reliability}

\subparagraph{Inter-rater reliability}

\subparagraph{Test-retest reliability}

\paragraph{Validity}

\subparagraph{Face validity}

\subparagraph{Construct validity}

\section{Results}
Table~\ref{tab:BasicTable} summarizes the data. \lipsum[15]

<<>>=
plot(rnorm(19))
@

<<>>=
kable(data.frame(x=1:3, y = letters[1:3]), booktabs = T) %>%
  kable_styling(latex_options = "striped")
@

\begin{table}
  \caption{Sample Basic Table}
  \label{tab:BasicTable}
  \begin{tabular}{@{}llr@{}}         \toprule
  \multicolumn{2}{c}{Item}        \\ \cmidrule(r){1-2}
  Animal    & Description & Price \\ \midrule
  Gnat      & per gram    & 13.65 \\
            & each        &  0.01 \\
  Gnu       & stuffed     & 92.50 \\
  Emu       & stuffed     & 33.33 \\
  Armadillo & frozen      &  8.99 \\ \bottomrule
  \end{tabular}
\end{table}

\begin{figure}
    \caption{This is my first figure caption.}
    \includegraphics[bb=0in 0in 2.5in 2.5in, height=2.5in, width=2.5in]{Figure1.pdf}
    \label{fig:Figure1}
\end{figure}

Figure~\ref{fig:Figure1} shows this trend. \lipsum[16]

\section{Discussion}

\printbibliography

\appendix

\section{Instrument}
\label{app:instrument}

As shown in Figure~\ref{fig:Figure2}, these results are impressive. \lipsum[20]

\begin{figure}
    \caption{This is my second figure caption.}
    \includegraphics[bb=0in 0in 2.5in 2.5in, height=2.5in, width=2.5in]{Figure1.pdf}
    \label{fig:Figure2}
\end{figure}

\lipsum[21]
\section{Pilot Data}
\label{app:surveydata}

The detailed results are shown in Table~\ref{tab:DeckedTable}. \lipsum[22]

\begin{table}
  \begin{threeparttable}
    \caption{A More Complex Decked Table}
    \label{tab:DeckedTable}
    \begin{tabular}{@{}lrrr@{}}         \toprule
    Distribution type  & \multicolumn{2}{l}{Percentage of} & Total number   \\
                       & \multicolumn{2}{l}{targets with}  & of trials per  \\
                       & \multicolumn{2}{l}{segment in}    & participant    \\ \cmidrule(r){2-3}
                                    &  Onset  &  Coda            &          \\ \midrule
    Categorical -- onset\tabfnm{a}  &    100  &     0            &  196     \\
    Probabilistic                   &     80  &    20\tabfnm{*}  &  200     \\
    Categorical -- coda\tabfnm{b}   &      0  &   100\tabfnm{*}  &  196     \\ \midrule
    \end{tabular}
    \begin{tablenotes}[para,flushleft]
        {\small
            \textit{Note.} All data are approximate.

            \tabfnt{a}Categorical may be onset.
            \tabfnt{b}Categorical may also be coda.

            \tabfnt{*}\textit{p} < .05.
            \tabfnt{**}\textit{p} < .01.
         }
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\lipsum[23]

\end{document}

%%
%% Copyright (C) 2019 by Daniel A. Weiss <daniel.weiss.led at gmail.com>
%%
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License (LPPL), either
%% version 1.3c of this license or (at your option) any later
%% version.  The latest version of this license is in the file:
%%
%% http://www.latex-project.org/lppl.txt
%%
%% Users may freely modify these files without permission, as long as the
%% copyright line and this statement are maintained intact.
%%
%% This work is not endorsed by, affiliated with, or probably even known
%% by, the American Psychological Association.
%%
%% This work is "maintained" (as per LPPL maintenance status) by
%% Daniel A. Weiss.
%%
%% This work consists of the file  apa7.dtx
%% and the derived files           apa7.ins,
%%                                 apa7.cls,
%%                                 apa7.pdf,
%%                                 README,
%%                                 APA7american.txt,
%%                                 APA7british.txt,
%%                                 APA7dutch.txt,
%%                                 APA7english.txt,
%%                                 APA7german.txt,
%%                                 APA7ngerman.txt,
%%                                 APA7greek.txt,
%%                                 APA7czech.txt,
%%                                 APA7turkish.txt,
%%                                 APA7endfloat.cfg,
%%                                 Figure1.pdf,
%%                                 shortsample.tex,
%%                                 longsample.tex, and
%%                                 bibliography.bib.
%%
%%
%% End of file `./samples/longsample.tex'.

<<biber>>=
system(paste("biber", sub("\\.Rnw$", "", current_input())))
@

