---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Externalizing Problems {#ext}

In this chapter, we present the analyses to evaluate the different role of mother attachment and father attachment on children's externalizing problems. First, we discuss the appropriate models family to to take into account data characteristics. Next we conduct inference following three different approaches: 1) traditional Null Hypothesis Significance Testing (NHST) 2) Model Comparison using the AIC and BIC criteria 3) Bayes Factor with encompassing prior approach.

## Models Family Choiche {#model-choice}

Externalizing problems are computed as the sum of 10 items of the SDQ, obtaining discrete scores that range from 0 to 20. Thus, we should use appropriate discrete distribution such as the *Poisson* distribution or the *Negative Binomial*. In the Poisson distribution mean and variance are defined according to the same parameter $\lambda$. On the contrary, Negative Binomial has an extra parameter to adjust the variance allowing more flexibility. Considering data distribution (see Figure~\@ref(fig:plot-externalizing-dist)), we can observe that data have high dispersion with a long right tail. In this case, Poisson distribution would be a poor choice and we prefer Negative Binomial instead.

Again, considering data distribution (see Figure~\@ref(fig:plot-externalizing-dist)), we can observe an high peak of values at zero. Remember that this is not a clinical sample, thus it is expected that children majority has no problems or really few problem. We could question ourselves, however, whether a *Zero-Inflated* model may be appropriate

### Zero Inflated Negative Binomial

To evaluate the presence of zero inflation in our data, we compare the number of observed zeros and expected zeros in a Negative Binomial mixed effects model. We consider in the model the role of gender and the interaction between mother attachment and father attachment. Moreover, we consider the children's classroom ID as a random effect to account for teachers' different ability to evaluate children's problems. Using R formula syntax, we have
```{r echo = TRUE, eval=FALSE}
# model formula
externalizing_sum ~ gender + mother * father + (1|ID_class)
```

The model is fitted using the function `glmmTMB()` from the `glmmTMB` R-package [@brooksGlmmTMBBalancesSpeed2017]. Next, we compare the number of observed zero and expected zeros using an adapted version the function `check_zeroinflation()` from the R-package `performance` [@ludeckePerformancePackageAssessment2021] that solves a small bug (see issue https://github.com/easystats/performance/issues/367).
```{r echo = TRUE, cache=TRUE}
my_check_zeroinflation(fit_ext_nb)
```

Results indicate that the model is slightly under-fitting the number of zeros. Now, we can try to fit a *Zero Inflated Negative Binomial* (ZINB) model and compare the performance of the two models. ZINB models are defined as
$$
y_{i} \sim ZINB(p_{i}, \mu_{i}, \phi),
$$
where $p_i$ is the probability of an observation $y_{i}$ being an extra zero (i.e., a zero not coming from the Negative Binomial distribution) and $1-p_i$ indicates the probability of a given observation $y_{i}$ being generated form a Negative Binomial distribution with mean $\mu_{i}$ and variance $\sigma_{i}^2 = \mu_{i} + \frac{\mu_{i}^2}{\phi}$. Moreover, we have that
$$
p_{i} = \text{logit}^{-1}(X_i^T\beta_p),\\
\mu_{i} = \text{exp}(X_i^T\beta_{\mu}).
$$
That is, both $p$ and $\mu$ are modelled separately according to (possibly) different variables. In our case, we consider only the role of gender for $p$ (i.e., the probability of having externalizing problems depends on gender), whereas for $\mu$ the consider also the interaction between mother attachment and father attachment. In both cases, we consider the children's classroom ID as a random effect (teachers may differ in they ability to detect children's problem and quantify them). Using R formula syntax, we have
```{r echo = TRUE, eval=FALSE}
# formula for p
p ~ gender + (1|ID_class)

# formula for mu
mu ~ gender + mother * father + (1|ID_class)
```

The ZINB model is fitted using the function `zeroinfl()` from the `pscl` R-package [@zeileisRegressionModelsCount2008]. To compare the ZINB model and the Negative Binomial model we conduct an analysis of *Deviance*. Note that, in the case of generalized linear models (GLM), the deviance is the corresponding of the residual variance used in the traditional ANOVA in the case of linear models.
```{r echo=TRUE, cache=TRUE}
anova(fit_ext_nb, fit_ext_zinb)
```

Overall, results indicate that the ZINB model performs better than the Negative Binomial model. Thus, in the following analyses we decide to use ZINB models.

## NHST 

Following traditional NHST approach, we consider the model previously defined that includes all effects of interest. That is the gender effect and the interaction between mother attachment and father attachment. Subsequently, we can run an analysis of deviance to evaluate the significance of the predictors using the function `Anova()` from the R-package `car` [@foxCompanionAppliedRegression2019].
```{r, echo =TRUE, cache=TRUE}
car::Anova(fit_ext_zinb)
```
Results indicate a statistically significant effect of gender and mother attachment. On the contrary, the interaction and father attachment are no significant. Model summary is reported below.
```{r, echo =TRUE, cache=TRUE}
summary(fit_ext_zinb)
```
To evaluate the effect of gender and mother attachment, the marginal predicted values according to gender and mother attachment are presented separately in Figure~\@ref(fig:plot-nhst-effects). Not that the marginal predicted values for gender are averaged over mother and father attachment effects. Whereas, the marginal predicted values for mother attachment are averaged over father attachment and gender effect.
```{r plot-nhst-effects, cache=TRUE, fig.asp=.65, message=FALSE, fig.cap="Marginal predicted values according to gender and mother attachment. Values are averaged over the other effects ($n_{subj} = 847$)."}
get_plot_zinb(model = fit_ext_zinb, attachment = "mother")
```

Post-hoc test are run to evaluate differences between mother attachment styles. To do that we use the `contrast()` function from the `emmeans` R-package, considering pairwise comparisons and adjusting *p*-values according to multivariate *t*-distribution. This approach is less restrictive than traditional *“Bonferroni”* method, as it determines the adjustment according to a multivariate *t*-distribution with the same covariance structure as the estimates. Results are reported below,
```{r, cache=TRUE, echo = TRUE}
emmeans::contrast(emmeans::emmeans(fit_ext_zinb, specs = ~ mother ),
                  "pairwise", adjust = "mvt")
```

Overall, results indicate that Males have more externalizing problems than Females and, regarding mother attachment, Fearful children have more problems than Secure children.

To evaluate the fit of model to the data, we used $R^2$. In the case of generalized mixed-effects models, however, there are several definitions of $R^2$. We computed the *Marginal* $R^2$ and the *Conditional* $R^2$ as suggested by @nakagawaCoefficientDeterminationR22017. *Marginal* $R^2$ is concerned with the variance explained by fixed factors of the model, and *Conditional* $R^2$ is concerned with the variance explained by both fixed and random factors of the model. To do that we use the function `performance::r2()`.

```{r, echo=TRUE, message=FALSE, cache=TRUE}
performance::r2(fit_ext_zinb)
```

We can see that the actual variance explained by fixed effects is almost 10\%, not bad for psychology.

#### Conclusions {-}

Considering attachment theoretical perspectives, results indicate only the role of mother attachment. Note, however, that traditional NHST does not allow us to evaluate evidence in favour of an hypotheses. Moreover, we actually have not tested our hypotheses but only the catch-all null hypothesis that *“nothing is going on”*.

## Model Comparison

Model comparison allows us to compare multiple hypotheses and identify which is the most supported by the data [@mcelreathStatisticalRethinkingBayesian2020]. First, we need to formalize models according to our hypotheses. Subsequently, we can evaluate which is the most supported model among those considered according to the data using the AIC and BIC [@wagenmakersAICModelSelection2004; @akaike1973a; @schwarzEstimatingDimensionModel1978].

### Formalize Models

Following the same reasons as before (see Section~\@ref(model-choice)), we consider Zero Inflated Negative Binomial Mixed-Effects models. Again, we consider only the role of gender as fixed effect and children's classroom ID as random effect for $p$. Whereas, considering $\mu$, we define four different models to take into account the different theoretical perspectives:

- `fit_ext_zero`: we consider only the effect of gender. This model assumes that attachment plays no role.
- `fit_ext_mother`: we consider the additive effects of gender and mother attachment. This model supports the idea that only mother attachment is important (**Monotropy Theory**).
- `fit_ext_additive`: we consider the additive effects of gender,  mother attachment, and father attachment. This model supports the idea that both mother attachment and father attachment are important, but not their interaction (**Hierarchy Theory** or **Independence Theory**).
- `fit_ext_inter`: we consider the additive effects of gender and the interaction between mother attachment and father attachment. This model supports the idea that the interaction between mother attachment and father attachment is important (**Integration Theory**).

Moreover, in all models we include children's classroom ID as random effect to take into  account teachers' different ability to evaluate children's problems. Using R formula syntax, we have
```{r, echo = TRUE, eval=FALSE}
# formula for p (same for all models)
p ~ gender + (1|ID_class)

# formula for mu

# fit_ext_zero
mu ~ gender + (1|ID_class)

# fit_ext_mother
mu ~ gender + mother + (1|ID_class)

# fit_ext_additive
mu ~ gender + mother + father + (1|ID_class)

# fit_ext_inter
mu ~ gender + mother * father + (1|ID_class)
```

### AIC and BIC Results

After estimating the models, the AIC and BIC values together with their relative weights are computed. Results are reported in Table~\@ref(tab:table-AIC-BIC-weights).

```{r table-AIC-BIC-weights, cache=TRUE}
get_table_AIC_BIC(AIC_weights = AIC_weights_ext,
                  BIC_weights = BIC_weights_ext,
                  problem = "ext",
                  format = params$format,
                  path_img = "images/")
```


Accordinng to AIC, the most likely model is `fit_ext_mother` (`r perc_model_ext("fit_ext_mother", "AIC")`\%) and the second most likely model is `fit_ext_additive` (`r perc_model_ext("fit_ext_additive", "AIC")`\%) given the data and the set of models considered. According to BIC, instead,  the most likely model is `fit_ext_zero` (`r perc_model_ext("fit_ext_zero", "BIC")`\%) and the second most likely model is `fit_ext_mother` (`r perc_model_ext("fit_ext_mother", "BIC")`\%) given the data and the set of models considered.

To interpret these results note that, AIC tends to select more complex models that can better explain the data, on the contrary, BIC penalizes complex models to a greater extent.  As pointed by @kuhaAICBICComparisons2004, using the two criteria together is always advocated as agreement provides reassurance on the robustness of the results and disagreement still provides useful information for the discussion. We can say that there is evidence in favour of the role of mother attachment but probably this effect is small.

### Selected Model

Considering model `fit_ext_mother`, we can run an analysis of deviance to evaluate the significance of the predictors.
```{r, echo =TRUE, cache=TRUE}
car::Anova(fit_ext_mother)
```
Results confirm a statistically significant effect of gender and mother attachment. Model summary is reported below.
```{r, echo =TRUE, cache=TRUE}
summary(fit_ext_mother)
```
To evaluate the effect of gender and mother attachment, the marginal predicted values according to gender and mother attachment are presented separately in Figure~\@ref(fig:plot-comparison-effects). Not that the marginal predicted values for gender are averaged over mother attachment. Whereas, the marginal predicted values for mother attachment are averaged over gender.
```{r plot-comparison-effects, cache=TRUE, fig.asp=.65, message=FALSE, fig.cap="Marginal predicted values according to gender and mother attachment ($n_{subj} = 847$)."}
get_plot_zinb(model = fit_ext_mother, attachment = "mother")
```

Post-hoc test are run to evaluate differences between mother attachment styles, considering pairwise comparisons and adjusting *p*-values according to multivariate *t*-distribution. Results are reported below,
```{r, cache=TRUE, echo = TRUE}
emmeans::contrast(emmeans::emmeans(fit_ext_mother, specs = ~ mother ),
                  "pairwise", adjust = "mvt")
```

Overall, results indicate that Males have more externalizing problems than Females. Regarding mother attachment, Fearful and Avoidant children have more problems than Secure children. Moreover, also the difference between Anxious and Secure children and the difference between Anxious and Fearful children have a low (but not statistically significant) *p*-value.

To evaluate the fit of model to the data, we computed the *Marginal* $R^2$ and the *Conditional* $R^2$.

```{r, echo=TRUE, message=FALSE, cache=TRUE}
performance::r2(fit_ext_mother)
```

We can see that the actual variance explained by fixed effects is around 7\%, not bad for psychology.

#### Conclusions {-}

Considering attachment theoretical perspectives, results indicate only the role of mother attachment so we can support the **Monotropy Theory**. Note, however, that the compared models contain no information regarding the expected direction of the effects but we only include/exclude predictors.

## Bayes Factor

To properly evaluate hypotheses with information regarding the expected direction of the effects, we use the Bayes Factor with encompassing prior approach. See the main article for a detailed introduction to this approach [TODO: add link article].

First, we define the encompassing model. Subsequently, we obtain the hypotheses matrices according to the informative hypotheses. Next, we compute the Bayes Factor and, finally, we describe the selected model.

### Encompassing Model 

We define a Zero-Inflated Negative Binomial (ZINB) mixed-effects model to take into account the characteristics of the dependent variable and its distribution (see Section~\@ref(model-choice)).  Again, we consider only the role of gender as fixed effect and children’s classroom ID as random effect for $p$. Whereas, regarding $\mu$, we consider the interaction between mother and father attachment together with gender as fixed effects and children’s classroom ID as random effect. In the R formula syntax, we have
```{r, echo=TRUE, eval=FALSE}
# formula for p
p ~ gender + (1|ID_class)

# formula for mu
mu ~ gender + mother * father + (1|ID_class)
```

The encompassing model is estimated using 6 independent chains with 10,000 iterations (warm-up 2,000). To do that we use the `brm()` fucntion from the `brms` R-package [@burknerBrmsPackageBayesian2017; burknerAdvancedBayesianMultilevel2018a], which is based on STAN [@standevelopmentteamRStanInterfaceStan2020].

#### Prior Choiche {-}

Prior choice is important for the parameters involved in the equality and inequality constraints. In our case, the parameters of interest (i.e., those related to mother and father attachment interaction) are unbounded. Thus, we can simply specify as prior a normal distribution with mean 0 and a given standard deviation. Considering the standard deviation, however, we have to choose a value so that the resulting prior is non-informative but without being excessively diffuse. 

We can evaluate the consequences of different values choice considering the resulting prior predictions. To facilitate this step, we compute prior prediction considering only the intercept and a single parameter of interest. Remembering that the inverse link function (i.e., function that in a GLM transform the model linear prediction into the value on the original response scale) is the exponential function, we consider as intercept the value 1, because $exp(1) \approx 2.7$ that is close to the externalizing problems sample mean  `r ext_mean()`. In Table~\@ref(table-prior-predict), summary information about prior predictions for different standard deviation values are reported.
```{r table-prior-predict}
get_table_prior_predict(format = params$format)
```

Considering that externalizing problems are bounded between 0 and 20, a reasonable prior is  $\mathcal{N}(0,3)$. as prior predicted values cover all possible values without including excessively large values. More diffuse priors would result in values with a higher order of magnitude and tighter priors would exclude plausible values. The influence of prior specification will be subsequently evaluated in a prior sensitivity analysis. 

Regarding the other nuisance parameters (i.e., intercepts, random effects and shapes parameters) `brms` default priors are maintained. The resulting prior settings are
```{r prior-seettings}
options(width = 300)
brms::prior_summary(encompassing_model_ext)
```

#### Hypotheses Matrices {-}

Finally, note that in this case, we do not need to standardize our parameters as they represent mean groups differences (see main article [TODO: add link article]).




