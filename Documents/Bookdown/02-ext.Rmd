---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Externalizing Problems {#ext}

In this chapter, we present the analyses to evaluate the different role of mother attachment and father attachment on children's externalizing problems. First, we discuss the appropriate models family to to take into account data characteristics. Next we conduct inference following three different approaches: 1) traditional Null Hypothesis Significance Testing (NHST) 2) Model Comparison using the AIC and BIC criteria 3) Bayes Factor with encompassing prior approach.

## Models Family Choiche {#model-choice}

Externalizing problems are computed as the sum of 10 items of the SDQ, obtaining discrete scores that range from 0 to 20. Thus, we should use appropriate discrete distribution such as the *Poisson* distribution or the *Negative Binomial*. In the Poisson distribution mean and variance are defined according to the same parameter $\lambda$. On the contrary, Negative Binomial has an extra parameter to adjust the variance allowing more flexibility. Considering data distribution (see Figure~\@ref(fig:plot-externalizing-dist)), we can observe that data have high dispersion with a long right tail. In this case, Poisson distribution would be a poor choice and we prefer Negative Binomial instead.

Again, considering data distribution (see Figure~\@ref(fig:plot-externalizing-dist)), we can observe an high peak of values at zero. Remember that this is not a clinical sample, thus it is expected that children majority has no problems or really few problem. We could question ourselves, however, whether a *Zero-Inflated* model may be appropriate

### Zero Inflated Negative Binomial

To evaluate the presence of zero inflation in our data, we compare the number of observed zeros and expected zeros in a Negative Binomial mixed effects model. We consider in the model the role of gender and the interaction between mother attachment and father attachment. Moreover, we consider the children's classroom ID as a random effect to account for teachers' different ability to evaluate children's problems. Using R formula syntax, we have
```{r echo = TRUE, eval=FALSE}
# model formula
externalizing_sum ~ gender + mother * father + (1|ID_class)
```

The model is fitted using the function `glmmTMB()` from the `glmmTMB` R-package [@brooksGlmmTMBBalancesSpeed2017]. Next, we compare the number of observed zero and expected zeros using an adapted version the function `check_zeroinflation()` from the R-package `performance` [@ludeckePerformancePackageAssessment2021] that solves a small bug (see issue https://github.com/easystats/performance/issues/367).
```{r echo = TRUE, cache=TRUE}
my_check_zeroinflation(fit_ext_nb)
```

Results indicate that the model is slightly under-fitting the number of zeros. Now, we can try to fit a *Zero Inflated Negative Binomial* (ZINB) model and compare the performance of the two models. ZINB models are defined as
$$
y_{i} \sim ZINB(p_{i}, \mu_{i}, \phi),
$$
where $p_i$ is the probability of an observation $y_{i}$ being an extra zero (i.e., a zero not coming from the Negative Binomial distribution) and $1-p_i$ indicates the probability of a given observation $y_{i}$ being generated form a Negative Binomial distribution with mean $\mu_{i}$ and variance $\sigma_{i}^2 = \mu_{i} + \frac{\mu_{i}^2}{\phi}$. Moreover, we have that
$$
p_{i} = \text{logit}^{-1}(X_i^T\beta_p),\\
\mu_{i} = \text{exp}(X_i^T\beta_{\mu}).
$$
That is, both $p$ and $\mu$ are modelled separately according to (possibly) different variables. In our case, we consider only the role of gender for $p$ (i.e., the probability of having externalizing problems depends on gender), whereas for $\mu$ the consider also the interaction between mother attachment and father attachment. In both cases, we consider the children's classroom ID as a random effect (teachers may differ in they ability to detect children's problem and quantify them). Using R formula syntax, we have
```{r echo = TRUE, eval=FALSE}
# formula for p
p ~ gender + (1|ID_class)

# formula for mu
mu ~ gender + mother * father + (1|ID_class)
```

The ZINB model is fitted using the function `zeroinfl()` from the `pscl` R-package [@zeileisRegressionModelsCount2008]. To compare the ZINB model and the Negative Binomial model we conduct an analysis of *Deviance*. Note that, in the case of generalized linear models (GLM), the deviance is the corresponding of the residual variance used in the traditional ANOVA in the case of linear models.
```{r echo=TRUE, cache=TRUE}
anova(fit_ext_nb, fit_ext_zinb)
```

Overall, results indicate that the ZINB model performs better than the Negative Binomial model. Thus, in the following analyses we decide to use ZINB models.

## NHST 

Following traditional NHST approach, we consider the model previously defined that includes all effects of interest. That is the gender effect and the interaction between mother attachment and father attachment. Subsequently, we can run an analysis of deviance to evaluate the significance of the predictors using the function `Anova()` from the R-package `car` [@foxCompanionAppliedRegression2019].
```{r, echo =TRUE, cache=TRUE}
car::Anova(fit_ext_zinb)
```
Results indicate a statistically significant effect of gender and mother attachment. On the contrary, the interaction and father attachment are no significant. Model summary is reported below.
```{r, echo =TRUE, cache=TRUE}
summary(fit_ext_zinb)
```
To evaluate the effect of gender and mother attachment, the marginal predicted values according to gender and mother attachment are presented separately in Figure~\@ref(fig:plot-nhst-effects). Not that the marginal predicted values for gender are averaged over mother and father attachment effects. Whereas, the marginal predicted values for mother attachment are averaged over father attachment and gender effect.
```{r plot-nhst-effects, cache=TRUE, fig.asp=.65, message=FALSE, fig.cap="Marginal predicted values according to gender and mother attachment. Values are averaged over the other effects ($n_{subj} = 847$)."}
get_plot_zinb(model = fit_ext_zinb, attachment = "mother")
```

Post-hoc test are run to evaluate differences between mother attachment styles. To do that we use the `contrast()` function from the `emmeans` R-package, considering pairwise comparisons and adjusting *p*-values according to multivariate *t*-distribution. This approach is less restrictive than traditional *“Bonferroni”* method, as it determines the adjustment according to a multivariate *t*-distribution with the same covariance structure as the estimates. Results are reported below,
```{r, cache=TRUE, echo = TRUE}
emmeans::contrast(emmeans::emmeans(fit_ext_zinb, specs = ~ mother ),
                  "pairwise", adjust = "mvt")
```

Overall, results indicate that Males have more externalizing problems than Females and, regarding mother attachment, Fearful children have more problems than Secure children.

To evaluate the fit of model to the data, we used $R^2$. In the case of generalized mixed-effects models, however, there are several definitions of $R^2$. We computed the *Marginal* $R^2$ and the *Conditional* $R^2$ as suggested by @nakagawaCoefficientDeterminationR22017. *Marginal* $R^2$ is concerned with the variance explained by fixed factors of the model, and *Conditional* $R^2$ is concerned with the variance explained by both fixed and random factors of the model. To do that we use the function `performance::r2()`.

```{r, echo=TRUE, message=FALSE, cache=TRUE}
performance::r2(fit_ext_zinb)
```

We can see that the actual variance explained by fixed factors is really small.

#### Conclusions {-}

Considering attachment theoretical perspectives, results indicate only the role of mother attachment. Note, however, that traditional NHST does not allow us to evaluate evidence in favour of an hypotheses. Moreover, we actually have not tested our hypotheses but only the catch-all null hypothesis that *“nothing is going on”*.

## Model Comparison

Model comparison allows us to compare multiple hypotheses and identify which is the most supported by the data [@mcelreathStatisticalRethinkingBayesian2020]. First, we need to formalize models according to our hypotheses. Subsequently,we can evaluate which is the most supported model among those considered according to the data using the AIC and BIC [@wagenmakersAICModelSelection2004; @akaike1973a; @schwarzEstimatingDimensionModel1978].

### Formalize Models

Following the same reasons as before (see Section~\@ref(model-choice)), we consider Zero Inflated Negative Binomial Mixed-Effects models. Again, we consider only the role of gender as fixed effect and children's classroom ID as random effect for $p$. Whereas, considering $\mu$, we define four different models to take into account the different theoretical perspectives:

- `fit_ext_zero`: we consider only the effect of gender. This model assumes that attachment plays no role.
- `fit_ext_mother`: we consider the additive effects of gender and mother attachment. This model supports the idea that only mother attachment is important (**Monotropy Theory**).
- `fit_ext_additive`: we consider the additive effects of gender,  mother attachment, and father attachment. This model supports the idea that both mother attachment and father attachment are important, but not their interaction (**Hierarchy Theory** or **Independence Theory**).
- `fit_ext_inter`: we consider the additive effects of gender and the interaction between mother attachment and father attachment. This model supports the idea that the interaction between mother attachment and father attachment is important (**Integration Theory**).

Moreover, in all models we include children's classroom ID as random effect to take into  account teachers' different ability to evaluate children's problems. Using R formula syntax, we have
```{r, echo = TRUE, eval=FALSE}
# formula for p (same for all models)
p ~ gender + (1|ID_class)

# formula for mu

# fit_ext_zero
mu ~ gender + (1|ID_class)

# fit_ext_mother
mu ~ gender + mother + (1|ID_class)

# fit_ext_additive
mu ~ gender + mother + father + (1|ID_class)

# fit_ext_inter
mu ~ gender + mother * father + (1|ID_class)
```

### AIC and BIC Results

After estimating the models, the AIC and BIC values together with their relative weights are computed. Results are reported in Table~\@ref(tab:table-AIC-BIC-weights).

```{r table-AIC-BIC-weights}
get_table_AIC_BIC(AIC_weights = AIC_weights_ext,
                  BIC_weights = BIC_weights_ext,
                  problem = "ext",
                  format = params$format,
                  path_img = "images/")
```


Accordinng to AIC, the most likely model is `fit\_ext\_mother` (92\%) and the second most likely model is `fit\_ext\_additive` (8\%) given the data and the set of models considered. According to BIC, instead,  the most likely model is `fit\_ext\_zero` (78\%) and the second most likely model is `fit\_ext\_mother` (22\%) given the data and the set of models considered.

To interpret these results note that, AIC tends to select more complex models that can better explain the data, on the contrary, BIC penalizes complex models to a greater extent.  As pointed by @kuhaAICBICComparisons2004, using the two criteria together is always advocated as agreement provides reassurance on the robustness of the results and disagreement still provides useful information for the discussion. We can say that there is evidence in favour of the role of mother attachment but probably this effect is small.

### Selected Model









